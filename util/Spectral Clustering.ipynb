{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path \n",
    "import snap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import bernoulli\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_G1():\n",
    "    G = snap.TUNGraph.New()\n",
    "    for i in xrange(0,8):\n",
    "        G.AddNode(i)\n",
    "    G.AddEdge(0,1)\n",
    "    G.AddEdge(0,2)\n",
    "    G.AddEdge(0,3)\n",
    "    G.AddEdge(1,2)\n",
    "    G.AddEdge(1,5)\n",
    "    G.AddEdge(2,3)\n",
    "    G.AddEdge(2,4)\n",
    "    G.AddEdge(3,4)\n",
    "    G.AddEdge(4,7)\n",
    "    G.AddEdge(5,6)\n",
    "    G.AddEdge(5,7)\n",
    "    G.AddEdge(6,7)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_G2():\n",
    "    G = snap.TUNGraph.New()\n",
    "    for i in xrange(0,4):\n",
    "        G.AddNode(i)\n",
    "    G.AddEdge(0,1)\n",
    "    G.AddEdge(1,2)\n",
    "    G.AddEdge(2,3)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_G3():\n",
    "    G = snap.TUNGraph.New()\n",
    "    for i in xrange(0,10):\n",
    "        G.AddNode(i)\n",
    "    G.AddEdge(0,1)\n",
    "    G.AddEdge(0,2)\n",
    "    G.AddEdge(0,3)\n",
    "    G.AddEdge(0,9)\n",
    "    G.AddEdge(1,2)\n",
    "    G.AddEdge(1,3)\n",
    "    G.AddEdge(2,3)\n",
    "    G.AddEdge(3,4)\n",
    "    G.AddEdge(4,5)\n",
    "    G.AddEdge(4,6)\n",
    "    G.AddEdge(5,6)\n",
    "    G.AddEdge(6,7)\n",
    "    G.AddEdge(7,8)\n",
    "    G.AddEdge(7,9)\n",
    "    G.AddEdge(8,9)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_G4():\n",
    "    G = snap.TUNGraph.New()\n",
    "    G.AddNode(47)\n",
    "    G.AddNode(82)\n",
    "    G.AddNode(103)\n",
    "    G.AddNode(300)\n",
    "    G.AddEdge(47,82)\n",
    "    G.AddEdge(47,300)\n",
    "    G.AddEdge(82,103)\n",
    "    G.AddEdge(82,300)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency_matrix(G):\n",
    "    '''\n",
    "    This function might be useful for you to build the adjacency matrix of a\n",
    "    given graph and return it as a numpy array.\n",
    "    \n",
    "    Returns: adjacency matrix A.\n",
    "    '''\n",
    "    ##########################################################################\n",
    "    n = G.GetNodes()\n",
    "    A = np.zeros((n, n))\n",
    "    \n",
    "    for N1 in G.Nodes(): # Iterate over each node.\n",
    "        for nbr_idx in xrange(N1.GetDeg()): # Iterate over each neighbor of the node.\n",
    "            N2 = G.GetNI(N1.GetNbrNId(nbr_idx))\n",
    "            A[N1.GetId(), N2.GetId()] = 1\n",
    "            A[N2.GetId(), N1.GetId()] = 1\n",
    "\n",
    "    return A\n",
    "    ##########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passes :)\n"
     ]
    }
   ],
   "source": [
    "def test_get_adjacency_matrix():\n",
    "    G1 = make_test_G1()\n",
    "    A = get_adjacency_matrix(G1)\n",
    "    \n",
    "    # Check that A is what is expected.\n",
    "    expected_A = np.zeros((8,8))\n",
    "    expected_A[0,1] = 1\n",
    "    expected_A[1,0] = 1\n",
    "    expected_A[0,2] = 1\n",
    "    expected_A[2,0] = 1\n",
    "    expected_A[0,3] = 1\n",
    "    expected_A[3,0] = 1\n",
    "    expected_A[1,2] = 1\n",
    "    expected_A[2,1] = 1\n",
    "    expected_A[1,5] = 1\n",
    "    expected_A[5,1] = 1\n",
    "    expected_A[2,3] = 1\n",
    "    expected_A[3,2] = 1\n",
    "    expected_A[2,4] = 1\n",
    "    expected_A[4,2] = 1\n",
    "    expected_A[3,4] = 1\n",
    "    expected_A[4,3] = 1\n",
    "    expected_A[4,7] = 1\n",
    "    expected_A[7,4] = 1\n",
    "    expected_A[5,6] = 1\n",
    "    expected_A[6,5] = 1\n",
    "    expected_A[5,7] = 1\n",
    "    expected_A[7,5] = 1\n",
    "    expected_A[6,7] = 1\n",
    "    expected_A[7,6] = 1   \n",
    "    assert(np.array_equal(A, expected_A))\n",
    "    print(\"Test passes :)\")\n",
    "    \n",
    "test_get_adjacency_matrix() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sparse_degree_matrix(G):\n",
    "    '''\n",
    "    This function might be useful for you to build the degree matrix of a\n",
    "    given graph and return it as a numpy array. The sparse degree matrix is\n",
    "    matrix D.\n",
    "    '''\n",
    "    ##########################################################################\n",
    "    n = G.GetNodes()\n",
    "    D = np.zeros((n, n))\n",
    "    \n",
    "    for NI in G.Nodes(): # Iterate over each node.\n",
    "        D[NI.GetId(), NI.GetId()] = NI.GetDeg()\n",
    "\n",
    "    return D\n",
    "    ##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passes :)\n"
     ]
    }
   ],
   "source": [
    "def test_get_adjacency_matrix():\n",
    "    G1 = make_test_G1()\n",
    "    D = get_sparse_degree_matrix(G1)\n",
    "    \n",
    "    # Check that D is what is expected.\n",
    "    expected_D = np.zeros((8,8))\n",
    "    expected_D[0,0] = 3\n",
    "    expected_D[1,1] = 3\n",
    "    expected_D[2,2] = 4\n",
    "    expected_D[3,3] = 3\n",
    "    expected_D[4,4] = 3\n",
    "    expected_D[5,5] = 3\n",
    "    expected_D[6,6] = 2\n",
    "    expected_D[7,7] = 3\n",
    "    assert(np.array_equal(D, expected_D))\n",
    "    print(\"Test passes :)\")\n",
    "    \n",
    "test_get_adjacency_matrix() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for $\\texttt{normalized_cut_minimization}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_D_neg_half(D):\n",
    "    \"\"\"\n",
    "    Given diagonal matrix of degrees D, computes D^{-1/2} matrix.\n",
    "    \"\"\"\n",
    "    diag_D = np.diag(D)\n",
    "    D_neg_half = np.diag(diag_D**(-0.5))\n",
    "\n",
    "    return D_neg_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passes :)\n"
     ]
    }
   ],
   "source": [
    "def test_compute_D_neg_half():\n",
    "    G1 = make_test_G1()\n",
    "    D = get_sparse_degree_matrix(G1)\n",
    "    D_neg_half = compute_D_neg_half(D)\n",
    "    \n",
    "    # Check that computed D_neg_half is what is expected.\n",
    "    expected_D_neg_half = np.zeros((G1.GetNodes(), G1.GetNodes()))\n",
    "    expected_D_neg_half[0,0] = 1.0 / (3**0.5)\n",
    "    expected_D_neg_half[1,1] = 1.0 / (3**0.5)\n",
    "    expected_D_neg_half[2,2] = 1.0 / (4**0.5)\n",
    "    expected_D_neg_half[3,3] = 1.0 / (3**0.5)\n",
    "    expected_D_neg_half[4,4] = 1.0 / (3**0.5)    \n",
    "    expected_D_neg_half[5,5] = 1.0 / (3**0.5)\n",
    "    expected_D_neg_half[6,6] = 1.0 / (2**0.5)\n",
    "    expected_D_neg_half[7,7] = 1.0 / (3**0.5)\n",
    "    assert(np.allclose(D_neg_half, expected_D_neg_half))\n",
    "    print(\"Test passes :)\")\n",
    "\n",
    "test_compute_D_neg_half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_normalized_graph_Laplacian(A, D, D_neg_half):\n",
    "    \"\"\"\n",
    "    Returns normalized graph Laplacian, D^{-1/2} L D^{1/2}.\n",
    "    \"\"\"\n",
    "    L = D - A # Graph Laplacian.\n",
    "    normalized_L = np.dot(np.dot(D_neg_half, L), D_neg_half)\n",
    "    return normalized_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passes :)\n"
     ]
    }
   ],
   "source": [
    "def test_compute_normalized_graph_Laplacian():\n",
    "    G2 = make_test_G2()\n",
    "    A = get_adjacency_matrix(G2)\n",
    "    D = get_sparse_degree_matrix(G2)\n",
    "    D_neg_half = compute_D_neg_half(D)\n",
    "    normalized_L = compute_normalized_graph_Laplacian(A, D, D_neg_half)\n",
    "    \n",
    "    # Check that normalized_L is what is expected.\n",
    "    expected_normalized_L = np.array([[1, -0.70711, 0, 0],\n",
    "                                      [-0.70711, 1, -0.5, 0],\n",
    "                                      [0, -0.5, 1, -0.70711],\n",
    "                                      [0, 0, -0.70711, 1]])  \n",
    "    assert(np.allclose(normalized_L, expected_normalized_L))\n",
    "    print(\"Test passes :)\")\n",
    "\n",
    "test_compute_normalized_graph_Laplacian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_second_smallest_eigvec(normalized_L):\n",
    "    \"\"\"\n",
    "    Returns the eigenvector associated with the second smallest eigenvalue of the given noramlized\n",
    "    graph Laplacian.\n",
    "    \"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eigh(normalized_L) # Eigenvalues in ascending order.\n",
    "    v = eigvecs[:, 1] # Get the eigenvector associated with the second smallest eigenvalue. \n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def test_get_second_smallest_eigvec():\n",
    "    G2 = make_test_G2()\n",
    "    A = get_adjacency_matrix(G2)\n",
    "    D = get_sparse_degree_matrix(G2)\n",
    "    D_neg_half = compute_D_neg_half(D)\n",
    "    normalized_L = compute_normalized_graph_Laplacian(A, D, D_neg_half)\n",
    "    v = get_second_smallest_eigvec(normalized_L)\n",
    "    \n",
    "    # Check that v is what we expect.\n",
    "    expected_v = np.array([-0.57735, -0.40825, 0.40824, 0.57735])\n",
    "    assert(np.allclose(v, expected_v, atol=10**-5))   \n",
    "    \n",
    "test_get_second_smallest_eigvec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalized_cut_minimization(G):\n",
    "    '''\n",
    "    Implement the normalized cut minimizaton algorithm we derived in the last\n",
    "    homework here.\n",
    "    \n",
    "    Returns set S and set bar_S containing the two found communities.\n",
    "    '''\n",
    "    A = get_adjacency_matrix(G)\n",
    "    D = get_sparse_degree_matrix(G)\n",
    "    ##########################################################################\n",
    "    D_neg_half = compute_D_neg_half(D)\n",
    "    normalized_L = compute_normalized_graph_Laplacian(A, D, D_neg_half)\n",
    "    v = get_second_smallest_eigvec(normalized_L)\n",
    "    minimizer_x = np.dot(D_neg_half, v)\n",
    "    \n",
    "    # Get cluster assignments.\n",
    "    S = list(np.where(minimizer_x >= 0)[0])\n",
    "    neg_S = list(np.where(minimizer_x < 0)[0])\n",
    "    return S, neg_S\n",
    "\n",
    "    ##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests pass!!! :):):)\n"
     ]
    }
   ],
   "source": [
    "def test_normalized_cut_minimization():\n",
    "    # ========================= Use G2 to test math is correct. ==============================\n",
    "    G2 = make_test_G2()\n",
    "    S, neg_S = normalized_cut_minimization(G2)\n",
    "    \n",
    "    # Check that communities are what mathematically we expect.\n",
    "    expected_S = [2,3]\n",
    "    expected_neg_S = [0,1]\n",
    "    assert(S == expected_S)\n",
    "    assert(neg_S == expected_neg_S)\n",
    "    \n",
    "    # =============== Use G1 to check that the communities found make sense. =================\n",
    "    G1 = make_test_G1()\n",
    "    S, neg_S = normalized_cut_minimization(G1)\n",
    "    \n",
    "    # Check that communites are intuitively what we expect.\n",
    "    expected_S = [0,1,2,3,4]\n",
    "    expected_neg_S = [5,6,7]\n",
    "    assert(S == expected_S)\n",
    "    assert(neg_S == expected_neg_S)\n",
    "    \n",
    "    print(\"All tests pass!!! :):):)\")\n",
    "\n",
    "test_normalized_cut_minimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modularity(G, community_dict):\n",
    "    '''\n",
    "    This function might be useful to compute the modularity of a given cut\n",
    "    defined by two sets S and neg_S. We would normally require sets S and neg_S\n",
    "    to be disjoint and to include all nodes in Graph.\n",
    "    \n",
    "    - community_dict: maps node id to community\n",
    "    '''\n",
    "    ##########################################################################\n",
    "    A = get_adjacency_matrix(G)\n",
    "    two_M = float(np.sum(A))\n",
    "    mod_sum = 0\n",
    "    for NI in G.Nodes():\n",
    "        NI_id = NI.GetId()\n",
    "        for NJ in G.Nodes():\n",
    "            NJ_id = NJ.GetId()\n",
    "            if (community_dict[NI_id] == community_dict[NJ_id]):\n",
    "                mod_sum += A[NI_id, NJ_id] - ((NI.GetDeg() * NJ.GetDeg()) / two_M)\n",
    "    modularity = mod_sum / two_M\n",
    "    return modularity\n",
    "    ##########################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passes :)\n"
     ]
    }
   ],
   "source": [
    "def test_modularity():\n",
    "    sample_G = snap.LoadEdgeList(snap.PUNGraph, \"q1-sample.txt\", 0, 1)\n",
    "    S, neg_S = normalized_cut_minimization(sample_G)\n",
    "    community_dict = {node_id:0 for node_id in S}\n",
    "    community_dict.update({node_id:1 for node_id in neg_S})\n",
    "    modularity = get_modularity(sample_G, community_dict)\n",
    "    \n",
    "    # Check that modularity has expected value.\n",
    "    expected_modularity = 0.09\n",
    "    epsilon = 10**-3\n",
    "    assert(abs(modularity - expected_modularity) < epsilon)\n",
    "    print(\"Test passes :)\")\n",
    "\n",
    "test_modularity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_by_eig_space(G, k):\n",
    "    \"\"\"\n",
    "    Build reduced space from k smallest eigenvectors so that each node is now represented on\n",
    "    k numbers, from the k eigenvectors. Then use this reduced representation go cluster nodes.\n",
    "    Returns dictionary mapping each node id to. Note that it uses the adjacency matrix index\n",
    "    as the node_id. You have to map this index to the true node_id later.\n",
    "    \"\"\"\n",
    "    print \"Cluster with k:\", k\n",
    "    A = get_adjacency_matrix(G)\n",
    "    D = get_sparse_degree_matrix(G)\n",
    "    D_neg_half = compute_D_neg_half(D)\n",
    "    normalized_L = compute_normalized_graph_Laplacian(A, D, D_neg_half)\n",
    "\n",
    "    # Find eigenvalues.\n",
    "    eigvals, eigvecs = np.linalg.eigh(normalized_L) # Eigevanlues in ascending ordering.\n",
    "    v = eigvecs[:, 1:k+1] # The eigenvalues stored in columns. So each row represents a node.\n",
    "    \n",
    "    # Cluster with k-means.\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(v)\n",
    "    return {node:cluster for (node, cluster) in enumerate(kmeans.labels_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster with k: 1\n",
      "Cluster with k: 2\n",
      "Cluster with k: 3\n",
      "Cluster with k: 4\n",
      "Cluster with k: 5\n",
      "Cluster with k: 6\n",
      "Cluster with k: 7\n",
      "Test passes :)\n"
     ]
    }
   ],
   "source": [
    "def test_cluster_by_eig_space():\n",
    "    G3 = make_test_G3()\n",
    "    \n",
    "    highest_mod = 0\n",
    "    highest_mod_k = 0\n",
    "    highest_mod_comm_dict = None\n",
    "    for k in xrange(1,8):\n",
    "        comm_dict = cluster_by_eig_space(G3, k)\n",
    "        mod = get_modularity(G3, comm_dict)\n",
    "        if (mod > highest_mod):\n",
    "            highest_mod = mod\n",
    "            highest_mod_k = k\n",
    "            highest_mod_comm_dict = comm_dict\n",
    "            \n",
    "    expected_highest_mod_comm_dict = {0:2,1:2,2:2,3:2,4:0,5:0,6:0,7:1,8:1,9:1}\n",
    "    assert(highest_mod_k == 3)\n",
    "    assert(highest_mod_comm_dict == expected_highest_mod_comm_dict)\n",
    "    print \"Test passes :)\"\n",
    "\n",
    "test_cluster_by_eig_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Postids Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_postids(G):\n",
    "    \"\"\"\n",
    "    Returns a Graph containing all the remapped post_ids so that they go from 0 to n. \n",
    "    Also returns the dictionary that maps the ids to their new value.\n",
    "    \"\"\"\n",
    "    postid_map = dict()\n",
    "    new_G = snap.TUNGraph.New()\n",
    "    index = 0\n",
    "    \n",
    "    # Remap all nodes. Only keep ones with degree > 0.\n",
    "    for N in G.Nodes():\n",
    "        if (N.GetDeg() < 1): continue \n",
    "        postid_map[N.GetId()] = index\n",
    "        new_G.AddNode(index)\n",
    "        index += 1\n",
    "                \n",
    "    # Remap all edges.\n",
    "    for E in G.Edges(): # Edge traversal\n",
    "        new_G.AddEdge(postid_map[E.GetSrcNId()], postid_map[E.GetDstNId()])\n",
    "        \n",
    "    return new_G, postid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def test_remap_postids():\n",
    "    G4 = make_test_G4()\n",
    "    new_G, postid_map = remap_postids(G4)\n",
    "    \n",
    "    # Get nodes in graph. \n",
    "    edges = set()\n",
    "    for E in new_G.Edges():\n",
    "        edges.add((E.GetSrcNId(), E.GetDstNId()))\n",
    "    \n",
    "    expected_postid_map = {47:0,82:1,103:2,300:3}\n",
    "    expected_edges = {(0,1),(0,3),(1,2),(1,3)}\n",
    "    assert(postid_map == expected_postid_map)\n",
    "    assert(edges == expected_edges)\n",
    "\n",
    "test_remap_postids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Actually Cluster the Postids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../data/stats.stackexchange.com/Mixed\"\n",
    "POST_ID_FOLDED_GRAPH_PATH = path.join(BASE_PATH, \"Postid_Folded_Graph.graph\")\n",
    "NGRAMID_DICT_PICKLE_PATH = path.join(BASE_PATH, \"Bigramid_Dict.pickle\")\n",
    "POSTID_SET_PICKLE_PATH = path.join(BASE_PATH, \"Postid_set.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post_graph.\n",
    "FIn = snap.TFIn(POST_ID_FOLDED_GRAPH_PATH)\n",
    "post_graph = snap.TUNGraph.Load(FIn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of nodes in graph: 19725\n"
     ]
    }
   ],
   "source": [
    "print \"Original number of nodes in graph:\", post_graph.GetNodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create remapped post_graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eig_post_graph, postid_dict = remap_postids(post_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes with degree greater that 0: 17139\n"
     ]
    }
   ],
   "source": [
    "print \"Nodes with degree greater that 0:\", eig_post_graph.GetNodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster for each k and compute modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "community_maps = []\n",
    "mod_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster with k: 10\n",
      "Modularity 7.397733253897822e-05\n",
      "Cluster with k: 50\n",
      "Modularity 0.43976184876628965\n",
      "Cluster with k: 100\n",
      "Modularity 0.5412240931643605\n",
      "Cluster with k: 150\n",
      "Modularity 0.5499615019989026\n",
      "Cluster with k: 200\n",
      "Modularity 0.542894315254493\n"
     ]
    }
   ],
   "source": [
    "for k in xrange(0,250,50):\n",
    "    if (k == 0): k = 10\n",
    "    comm_map = cluster_by_eig_space(eig_post_graph, k)\n",
    "    mod = get_modularity(eig_post_graph, comm_map)\n",
    "    print \"Modularity\", mod\n",
    "    community_maps.append(comm_map)\n",
    "    mod_scores.append(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster with k: 250\n",
      "Modularity 0.537163366470572\n",
      "Cluster with k: 300\n",
      "Modularity 0.5266861109597994\n",
      "Cluster with k: 350\n",
      "Modularity 0.526417187856714\n",
      "Cluster with k: 400\n",
      "Modularity 0.5257398231970715\n",
      "Cluster with k: 450\n",
      "Modularity 0.5217433611742635\n"
     ]
    }
   ],
   "source": [
    "for k in xrange(250,500,50):\n",
    "    if (k == 0): k = 10\n",
    "    comm_map = cluster_by_eig_space(eig_post_graph, k)\n",
    "    mod = get_modularity(eig_post_graph, comm_map)\n",
    "    print \"Modularity\", mod\n",
    "    community_maps.append(comm_map)\n",
    "    mod_scores.append(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster with k: 500\n",
      "Modularity 0.5203227014171681\n",
      "Cluster with k: 550\n",
      "Modularity 0.5171775749165793\n",
      "Cluster with k: 600\n",
      "Modularity 0.5138722025387538\n",
      "Cluster with k: 650\n",
      "Modularity 0.5101391796789309\n",
      "Cluster with k: 700\n",
      "Modularity 0.49851099362008344\n"
     ]
    }
   ],
   "source": [
    "for k in xrange(500,750,50):\n",
    "    if (k == 0): k = 10\n",
    "    comm_map = cluster_by_eig_space(eig_post_graph, k)\n",
    "    mod = get_modularity(eig_post_graph, comm_map)\n",
    "    print \"Modularity\", mod\n",
    "    community_maps.append(comm_map)\n",
    "    mod_scores.append(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster with k: 750\n",
      "Modularity 0.49600519840901175\n",
      "Cluster with k: 800\n",
      "Modularity 0.5015773629178529\n",
      "Cluster with k: 850\n",
      "Modularity 0.5018306438572164\n",
      "Cluster with k: 900\n",
      "Modularity 0.48268253658044274\n",
      "Cluster with k: 950\n",
      "Modularity 0.49793110922234624\n",
      "Cluster with k: 1000\n",
      "Modularity 0.49777639389193645\n"
     ]
    }
   ],
   "source": [
    "for k in xrange(750,1001,50):\n",
    "    if (k == 0): k = 10\n",
    "    comm_map = cluster_by_eig_space(eig_post_graph, k)\n",
    "    mod = get_modularity(eig_post_graph, comm_map)\n",
    "    print \"Modularity\", mod\n",
    "    community_maps.append(comm_map)\n",
    "    mod_scores.append(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the Communities for Clustering with the Best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 150\n",
    "best_k_index = 3\n",
    "best_comm_map = community_maps[best_k_index]\n",
    "best_mod_score = mod_scores[best_k_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the post top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"../data/stats.stackexchange.com/Mixed\"\n",
    "POST_TOP_NGRAM_PATH = path.join(BASE_PATH, \"STATS_20k-Posts_11-top_uni&bigrams_nostem.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_top_ngram_df(topwords_path):\n",
    "    # Load csv containing the top words.\n",
    "    posts_df = pd.read_csv(topwords_path, sep = \"\\t\", usecols = \n",
    "                           [\"Id\", \"OwnerUserId\", \"TopWord1\", \"TopWord2\", \"TopWord3\", \"TopWord4\", \"TopWord5\"])\n",
    "    \n",
    "    # Clean dataframe.\n",
    "    posts_df = posts_df.dropna()\n",
    "    posts_df = posts_df.rename(columns={\n",
    "            \"Id\": \"post_id\", \"OwnerUserId\": \"user_id\"})\n",
    "    posts_df[\"user_id\"] = posts_df[\"user_id\"].astype(np.int64)\n",
    "    posts_df[\"post_id\"] = posts_df[\"post_id\"].astype(np.int64)\n",
    "    posts_df = posts_df[posts_df[\"user_id\"] > 0]\n",
    "    posts_df = posts_df[posts_df[\"post_id\"] > 0]\n",
    "    \n",
    "\n",
    "    return posts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Add communities column to posts_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_communities_post_df(post_df, best_comm_map, postid_dict):\n",
    "    \n",
    "    # Iterate over rows and add each community to each row.\n",
    "    community_array = []\n",
    "    for index, row in post_df.iterrows():\n",
    "        user_id = row[\"post_id\"]\n",
    "        A_id = postid_dict.get(user_id, -1)\n",
    "        if (A_id < 0):\n",
    "            community_array.append(-1)\n",
    "        else:\n",
    "            community_array.append(best_comm_map[A_id])\n",
    "    post_df.loc[:,'Community'] = community_array\n",
    "    \n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df_w_comm = add_communities_post_df(post_df, best_comm_map, postid_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print community sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community: 0 Size: 87\n",
      "Community: 1 Size: 8343\n",
      "Community: 2 Size: 2\n",
      "Community: 3 Size: 2\n",
      "Community: 4 Size: 2\n",
      "Community: 5 Size: 2\n",
      "Community: 6 Size: 2\n",
      "Community: 7 Size: 2\n",
      "Community: 8 Size: 2\n",
      "Community: 9 Size: 2\n",
      "Community: 10 Size: 2\n",
      "Community: 11 Size: 2\n",
      "Community: 12 Size: 2\n",
      "Community: 13 Size: 2\n",
      "Community: 14 Size: 2\n",
      "Community: 15 Size: 3\n",
      "Community: 16 Size: 169\n",
      "Community: 17 Size: 200\n",
      "Community: 18 Size: 8\n",
      "Community: 19 Size: 7\n",
      "Community: 20 Size: 2\n",
      "Community: 21 Size: 2\n",
      "Community: 22 Size: 2\n",
      "Community: 23 Size: 4\n",
      "Community: 24 Size: 2\n",
      "Community: 25 Size: 2\n",
      "Community: 26 Size: 5\n",
      "Community: 27 Size: 2\n",
      "Community: 28 Size: 2\n",
      "Community: 29 Size: 2\n",
      "Community: 30 Size: 29\n",
      "Community: 31 Size: 2\n",
      "Community: 32 Size: 4\n",
      "Community: 33 Size: 5\n",
      "Community: 34 Size: 2\n",
      "Community: 35 Size: 3\n",
      "Community: 36 Size: 2\n",
      "Community: 37 Size: 2\n",
      "Community: 38 Size: 6\n",
      "Community: 39 Size: 2\n",
      "Community: 40 Size: 2\n",
      "Community: 41 Size: 2\n",
      "Community: 42 Size: 109\n",
      "Community: 43 Size: 65\n",
      "Community: 44 Size: 2\n",
      "Community: 45 Size: 2\n",
      "Community: 46 Size: 2\n",
      "Community: 47 Size: 2\n",
      "Community: 48 Size: 2\n",
      "Community: 49 Size: 2\n",
      "Community: 50 Size: 222\n",
      "Community: 51 Size: 6\n",
      "Community: 52 Size: 4\n",
      "Community: 53 Size: 2\n",
      "Community: 54 Size: 145\n",
      "Community: 55 Size: 6\n",
      "Community: 56 Size: 67\n",
      "Community: 57 Size: 2\n",
      "Community: 58 Size: 114\n",
      "Community: 59 Size: 89\n",
      "Community: 60 Size: 50\n",
      "Community: 61 Size: 26\n",
      "Community: 62 Size: 2\n",
      "Community: 63 Size: 3\n",
      "Community: 64 Size: 73\n",
      "Community: 65 Size: 18\n",
      "Community: 66 Size: 206\n",
      "Community: 67 Size: 114\n",
      "Community: 68 Size: 84\n",
      "Community: 69 Size: 49\n",
      "Community: 70 Size: 42\n",
      "Community: 71 Size: 3\n",
      "Community: 72 Size: 84\n",
      "Community: 73 Size: 87\n",
      "Community: 74 Size: 21\n",
      "Community: 75 Size: 5\n",
      "Community: 76 Size: 58\n",
      "Community: 77 Size: 102\n",
      "Community: 78 Size: 104\n",
      "Community: 79 Size: 3\n",
      "Community: 80 Size: 6\n",
      "Community: 81 Size: 3\n",
      "Community: 82 Size: 53\n",
      "Community: 83 Size: 38\n",
      "Community: 84 Size: 120\n",
      "Community: 85 Size: 2\n",
      "Community: 86 Size: 50\n",
      "Community: 87 Size: 91\n",
      "Community: 88 Size: 79\n",
      "Community: 89 Size: 72\n",
      "Community: 90 Size: 13\n",
      "Community: 91 Size: 55\n",
      "Community: 92 Size: 19\n",
      "Community: 93 Size: 83\n",
      "Community: 94 Size: 8\n",
      "Community: 95 Size: 110\n",
      "Community: 96 Size: 72\n",
      "Community: 97 Size: 60\n",
      "Community: 98 Size: 2\n",
      "Community: 99 Size: 45\n",
      "Community: 100 Size: 128\n",
      "Community: 101 Size: 70\n",
      "Community: 102 Size: 246\n",
      "Community: 103 Size: 68\n",
      "Community: 104 Size: 16\n",
      "Community: 105 Size: 116\n",
      "Community: 106 Size: 27\n",
      "Community: 107 Size: 121\n",
      "Community: 108 Size: 112\n",
      "Community: 109 Size: 132\n",
      "Community: 110 Size: 80\n",
      "Community: 111 Size: 92\n",
      "Community: 112 Size: 6\n",
      "Community: 113 Size: 26\n",
      "Community: 114 Size: 82\n",
      "Community: 115 Size: 27\n",
      "Community: 116 Size: 73\n",
      "Community: 117 Size: 130\n",
      "Community: 118 Size: 1037\n",
      "Community: 119 Size: 106\n",
      "Community: 120 Size: 35\n",
      "Community: 121 Size: 55\n",
      "Community: 122 Size: 130\n",
      "Community: 123 Size: 184\n",
      "Community: 124 Size: 90\n",
      "Community: 125 Size: 48\n",
      "Community: 126 Size: 42\n",
      "Community: 127 Size: 77\n",
      "Community: 128 Size: 25\n",
      "Community: 129 Size: 42\n",
      "Community: 130 Size: 90\n",
      "Community: 131 Size: 88\n",
      "Community: 132 Size: 35\n",
      "Community: 133 Size: 44\n",
      "Community: 134 Size: 127\n",
      "Community: 135 Size: 180\n",
      "Community: 136 Size: 27\n",
      "Community: 137 Size: 175\n",
      "Community: 138 Size: 48\n",
      "Community: 139 Size: 5\n",
      "Community: 140 Size: 210\n",
      "Community: 141 Size: 34\n",
      "Community: 142 Size: 2\n",
      "Community: 143 Size: 18\n",
      "Community: 144 Size: 81\n",
      "Community: 145 Size: 301\n",
      "Community: 146 Size: 122\n",
      "Community: 147 Size: 156\n",
      "Community: 148 Size: 48\n",
      "Community: 149 Size: 38\n",
      "Community: -1 Size: 2586\n"
     ]
    }
   ],
   "source": [
    "communities = set(post_df_w_comm[\"Community\"])\n",
    "for comm in communities:\n",
    "    print \"Community:\", comm, \"Size:\", len(post_df[post_df_w_comm[\"Community\"] == comm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>TopWord1</th>\n",
       "      <th>TopWord2</th>\n",
       "      <th>TopWord3</th>\n",
       "      <th>TopWord4</th>\n",
       "      <th>TopWord5</th>\n",
       "      <th>Community</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>317788</td>\n",
       "      <td>187855</td>\n",
       "      <td>th level</td>\n",
       "      <td>effect th</td>\n",
       "      <td>factor</td>\n",
       "      <td>level factor</td>\n",
       "      <td>levene</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>318375</td>\n",
       "      <td>122192</td>\n",
       "      <td>factor</td>\n",
       "      <td>variables clustering</td>\n",
       "      <td>clustering</td>\n",
       "      <td>dataframe</td>\n",
       "      <td>numerical</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>318976</td>\n",
       "      <td>164061</td>\n",
       "      <td>alias</td>\n",
       "      <td>column</td>\n",
       "      <td>factors</td>\n",
       "      <td>factor</td>\n",
       "      <td>dummy</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>319526</td>\n",
       "      <td>85650</td>\n",
       "      <td>loadings</td>\n",
       "      <td>sign</td>\n",
       "      <td>varimax</td>\n",
       "      <td>factor</td>\n",
       "      <td>sign factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>320895</td>\n",
       "      <td>26456</td>\n",
       "      <td>subjects subjects</td>\n",
       "      <td>dfs</td>\n",
       "      <td>subjects</td>\n",
       "      <td>factor</td>\n",
       "      <td>returns</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>321472</td>\n",
       "      <td>129390</td>\n",
       "      <td>communalities</td>\n",
       "      <td>efa</td>\n",
       "      <td>factor</td>\n",
       "      <td>eigenvalue</td>\n",
       "      <td>accounted variance</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>321886</td>\n",
       "      <td>187640</td>\n",
       "      <td>irt</td>\n",
       "      <td>cfa</td>\n",
       "      <td>factor cfa</td>\n",
       "      <td>factor</td>\n",
       "      <td>single factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3672</th>\n",
       "      <td>322430</td>\n",
       "      <td>8013</td>\n",
       "      <td>analysis</td>\n",
       "      <td>complete factor</td>\n",
       "      <td>complete</td>\n",
       "      <td>factor</td>\n",
       "      <td>consider complete</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5036</th>\n",
       "      <td>324173</td>\n",
       "      <td>192279</td>\n",
       "      <td>dk</td>\n",
       "      <td>wrong dk</td>\n",
       "      <td>factor analysis</td>\n",
       "      <td>scale wrong</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>324282</td>\n",
       "      <td>26569</td>\n",
       "      <td>portfolio</td>\n",
       "      <td>portfolio factor</td>\n",
       "      <td>averaged time</td>\n",
       "      <td>fama</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5355</th>\n",
       "      <td>324588</td>\n",
       "      <td>40634</td>\n",
       "      <td>factor</td>\n",
       "      <td>factor factor</td>\n",
       "      <td>factor combinations</td>\n",
       "      <td>studentized</td>\n",
       "      <td>measurements</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5664</th>\n",
       "      <td>324969</td>\n",
       "      <td>192795</td>\n",
       "      <td>factors</td>\n",
       "      <td>problem</td>\n",
       "      <td>data item</td>\n",
       "      <td>factor</td>\n",
       "      <td>2n</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7126</th>\n",
       "      <td>326772</td>\n",
       "      <td>123700</td>\n",
       "      <td>load</td>\n",
       "      <td>factors</td>\n",
       "      <td>factor</td>\n",
       "      <td>subgeneral</td>\n",
       "      <td>trifactor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7592</th>\n",
       "      <td>327337</td>\n",
       "      <td>194464</td>\n",
       "      <td>factor</td>\n",
       "      <td>factor factor</td>\n",
       "      <td>interaction factor</td>\n",
       "      <td>covariate interaction</td>\n",
       "      <td>factor significant</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8026</th>\n",
       "      <td>327894</td>\n",
       "      <td>59951</td>\n",
       "      <td>ancova</td>\n",
       "      <td>ancova used</td>\n",
       "      <td>covariate</td>\n",
       "      <td>adjusted</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8116</th>\n",
       "      <td>328013</td>\n",
       "      <td>81743</td>\n",
       "      <td>ignored</td>\n",
       "      <td>factor</td>\n",
       "      <td>numeric</td>\n",
       "      <td>column factor</td>\n",
       "      <td>column happens</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8125</th>\n",
       "      <td>328024</td>\n",
       "      <td>194945</td>\n",
       "      <td>plot factor</td>\n",
       "      <td>site factor</td>\n",
       "      <td>split plot</td>\n",
       "      <td>factor</td>\n",
       "      <td>site</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8603</th>\n",
       "      <td>328655</td>\n",
       "      <td>139121</td>\n",
       "      <td>items</td>\n",
       "      <td>factor</td>\n",
       "      <td>th</td>\n",
       "      <td>according weights</td>\n",
       "      <td>believe distribution</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9752</th>\n",
       "      <td>330177</td>\n",
       "      <td>162705</td>\n",
       "      <td>factor analysis</td>\n",
       "      <td>psych package</td>\n",
       "      <td>loadings</td>\n",
       "      <td>common variance</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9788</th>\n",
       "      <td>330229</td>\n",
       "      <td>148743</td>\n",
       "      <td>factor different</td>\n",
       "      <td>factor</td>\n",
       "      <td>control condition</td>\n",
       "      <td>condition</td>\n",
       "      <td>linear mixed</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10823</th>\n",
       "      <td>331571</td>\n",
       "      <td>143094</td>\n",
       "      <td>insignificant variance</td>\n",
       "      <td>insignificant</td>\n",
       "      <td>items</td>\n",
       "      <td>items loading</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10965</th>\n",
       "      <td>331752</td>\n",
       "      <td>17072</td>\n",
       "      <td>cfa</td>\n",
       "      <td>confirmatory factor</td>\n",
       "      <td>confirmatory</td>\n",
       "      <td>factor analysis</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>331790</td>\n",
       "      <td>76762</td>\n",
       "      <td>factor</td>\n",
       "      <td>factor analysis</td>\n",
       "      <td>personality</td>\n",
       "      <td>personality dimension</td>\n",
       "      <td>principal</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11733</th>\n",
       "      <td>332769</td>\n",
       "      <td>198276</td>\n",
       "      <td>factors</td>\n",
       "      <td>level</td>\n",
       "      <td>levels</td>\n",
       "      <td>factor</td>\n",
       "      <td>3rd factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12139</th>\n",
       "      <td>333297</td>\n",
       "      <td>183208</td>\n",
       "      <td>chain</td>\n",
       "      <td>factor</td>\n",
       "      <td>store</td>\n",
       "      <td>shop</td>\n",
       "      <td>store chains</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12861</th>\n",
       "      <td>335242</td>\n",
       "      <td>188154</td>\n",
       "      <td>called factor</td>\n",
       "      <td>factor</td>\n",
       "      <td>coefficients</td>\n",
       "      <td>constituent variables</td>\n",
       "      <td>factor constituent</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12905</th>\n",
       "      <td>335293</td>\n",
       "      <td>200417</td>\n",
       "      <td>factor</td>\n",
       "      <td>factor level</td>\n",
       "      <td>level</td>\n",
       "      <td>appears thinking</td>\n",
       "      <td>combinations removes</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13527</th>\n",
       "      <td>336105</td>\n",
       "      <td>53456</td>\n",
       "      <td>latent</td>\n",
       "      <td>loadings</td>\n",
       "      <td>factor</td>\n",
       "      <td>factor loadings</td>\n",
       "      <td>latent variables</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13745</th>\n",
       "      <td>336384</td>\n",
       "      <td>199063</td>\n",
       "      <td>latent factor</td>\n",
       "      <td>factor</td>\n",
       "      <td>solutions</td>\n",
       "      <td>latent</td>\n",
       "      <td>efa</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14371</th>\n",
       "      <td>337228</td>\n",
       "      <td>102192</td>\n",
       "      <td>factors</td>\n",
       "      <td>factor analysis</td>\n",
       "      <td>number factors</td>\n",
       "      <td>factor</td>\n",
       "      <td>number</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14804</th>\n",
       "      <td>337791</td>\n",
       "      <td>163114</td>\n",
       "      <td>factor</td>\n",
       "      <td>indicators</td>\n",
       "      <td>factors displayed</td>\n",
       "      <td>set indicators</td>\n",
       "      <td>factors</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14922</th>\n",
       "      <td>337949</td>\n",
       "      <td>202507</td>\n",
       "      <td>factor</td>\n",
       "      <td>factor model</td>\n",
       "      <td>subgroup</td>\n",
       "      <td>factor explains</td>\n",
       "      <td>percent variance</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15128</th>\n",
       "      <td>338218</td>\n",
       "      <td>53456</td>\n",
       "      <td>multilevel</td>\n",
       "      <td>multilevel factor</td>\n",
       "      <td>factor analysis</td>\n",
       "      <td>muthen</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15399</th>\n",
       "      <td>338576</td>\n",
       "      <td>149096</td>\n",
       "      <td>factor solution</td>\n",
       "      <td>generated</td>\n",
       "      <td>factor</td>\n",
       "      <td>generated data</td>\n",
       "      <td>solution</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15557</th>\n",
       "      <td>338780</td>\n",
       "      <td>199063</td>\n",
       "      <td>uvi</td>\n",
       "      <td>loadings</td>\n",
       "      <td>latent</td>\n",
       "      <td>latent variables</td>\n",
       "      <td>run uvi</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16138</th>\n",
       "      <td>339555</td>\n",
       "      <td>158856</td>\n",
       "      <td>factor</td>\n",
       "      <td>trays</td>\n",
       "      <td>blocking factor</td>\n",
       "      <td>factor treatment</td>\n",
       "      <td>blocking</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16140</th>\n",
       "      <td>339557</td>\n",
       "      <td>188851</td>\n",
       "      <td>aic</td>\n",
       "      <td>factor</td>\n",
       "      <td>aic value</td>\n",
       "      <td>change aic</td>\n",
       "      <td>factor change</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16386</th>\n",
       "      <td>339868</td>\n",
       "      <td>198635</td>\n",
       "      <td>factor</td>\n",
       "      <td>latent factor</td>\n",
       "      <td>load</td>\n",
       "      <td>factor analysis</td>\n",
       "      <td>remember</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16902</th>\n",
       "      <td>340542</td>\n",
       "      <td>202968</td>\n",
       "      <td>rfe</td>\n",
       "      <td>factor</td>\n",
       "      <td>argument vector</td>\n",
       "      <td>classes object</td>\n",
       "      <td>error task</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17087</th>\n",
       "      <td>340793</td>\n",
       "      <td>204577</td>\n",
       "      <td>factor rotation</td>\n",
       "      <td>rotation</td>\n",
       "      <td>factor</td>\n",
       "      <td>arbitrary procrustean</td>\n",
       "      <td>coefficient suitable</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17241</th>\n",
       "      <td>340995</td>\n",
       "      <td>198195</td>\n",
       "      <td>factor</td>\n",
       "      <td>factor scores</td>\n",
       "      <td>scores</td>\n",
       "      <td>loyalty</td>\n",
       "      <td>satisfaction</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17555</th>\n",
       "      <td>341407</td>\n",
       "      <td>199063</td>\n",
       "      <td>factor</td>\n",
       "      <td>factors</td>\n",
       "      <td>cloud</td>\n",
       "      <td>latent</td>\n",
       "      <td>cloud points</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17818</th>\n",
       "      <td>341743</td>\n",
       "      <td>205238</td>\n",
       "      <td>emotion</td>\n",
       "      <td>emotion differentiation</td>\n",
       "      <td>differentiation</td>\n",
       "      <td>loadings</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17828</th>\n",
       "      <td>341755</td>\n",
       "      <td>52554</td>\n",
       "      <td>factor</td>\n",
       "      <td>comparisons</td>\n",
       "      <td>comparisons pairwise</td>\n",
       "      <td>explicitly obtain</td>\n",
       "      <td>factor combination</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18460</th>\n",
       "      <td>342623</td>\n",
       "      <td>205826</td>\n",
       "      <td>factor extraction</td>\n",
       "      <td>factor</td>\n",
       "      <td>extraction</td>\n",
       "      <td>cortable</td>\n",
       "      <td>extraction methods</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18675</th>\n",
       "      <td>342897</td>\n",
       "      <td>206002</td>\n",
       "      <td>factor scores</td>\n",
       "      <td>paf</td>\n",
       "      <td>structure matrix</td>\n",
       "      <td>tetrachoric</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>343463</td>\n",
       "      <td>178588</td>\n",
       "      <td>minitab</td>\n",
       "      <td>block</td>\n",
       "      <td>block factor</td>\n",
       "      <td>factor covariates</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19147</th>\n",
       "      <td>343470</td>\n",
       "      <td>206373</td>\n",
       "      <td>items correlated</td>\n",
       "      <td>factor model</td>\n",
       "      <td>correlated residuals</td>\n",
       "      <td>factor</td>\n",
       "      <td>items</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19267</th>\n",
       "      <td>343626</td>\n",
       "      <td>159259</td>\n",
       "      <td>bias</td>\n",
       "      <td>factor scores</td>\n",
       "      <td>factor</td>\n",
       "      <td>avoiding method</td>\n",
       "      <td>bias avoiding</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19752</th>\n",
       "      <td>344279</td>\n",
       "      <td>206936</td>\n",
       "      <td>fixed factor</td>\n",
       "      <td>know fixed</td>\n",
       "      <td>disease</td>\n",
       "      <td>affect</td>\n",
       "      <td>factor</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id  user_id                TopWord1                 TopWord2  \\\n",
       "67      317788   187855                th level                effect th   \n",
       "526     318375   122192                  factor     variables clustering   \n",
       "990     318976   164061                   alias                   column   \n",
       "1417    319526    85650                loadings                     sign   \n",
       "2466    320895    26456       subjects subjects                      dfs   \n",
       "2930    321472   129390           communalities                      efa   \n",
       "3252    321886   187640                     irt                      cfa   \n",
       "3672    322430     8013                analysis          complete factor   \n",
       "5036    324173   192279                      dk                 wrong dk   \n",
       "5116    324282    26569               portfolio         portfolio factor   \n",
       "5355    324588    40634                  factor            factor factor   \n",
       "5664    324969   192795                 factors                  problem   \n",
       "7126    326772   123700                    load                  factors   \n",
       "7592    327337   194464                  factor            factor factor   \n",
       "8026    327894    59951                  ancova              ancova used   \n",
       "8116    328013    81743                 ignored                   factor   \n",
       "8125    328024   194945             plot factor              site factor   \n",
       "8603    328655   139121                   items                   factor   \n",
       "9752    330177   162705         factor analysis            psych package   \n",
       "9788    330229   148743        factor different                   factor   \n",
       "10823   331571   143094  insignificant variance            insignificant   \n",
       "10965   331752    17072                     cfa      confirmatory factor   \n",
       "10995   331790    76762                  factor          factor analysis   \n",
       "11733   332769   198276                 factors                    level   \n",
       "12139   333297   183208                   chain                   factor   \n",
       "12861   335242   188154           called factor                   factor   \n",
       "12905   335293   200417                  factor             factor level   \n",
       "13527   336105    53456                  latent                 loadings   \n",
       "13745   336384   199063           latent factor                   factor   \n",
       "14371   337228   102192                 factors          factor analysis   \n",
       "14804   337791   163114                  factor               indicators   \n",
       "14922   337949   202507                  factor             factor model   \n",
       "15128   338218    53456              multilevel        multilevel factor   \n",
       "15399   338576   149096         factor solution                generated   \n",
       "15557   338780   199063                     uvi                 loadings   \n",
       "16138   339555   158856                  factor                    trays   \n",
       "16140   339557   188851                     aic                   factor   \n",
       "16386   339868   198635                  factor            latent factor   \n",
       "16902   340542   202968                     rfe                   factor   \n",
       "17087   340793   204577         factor rotation                 rotation   \n",
       "17241   340995   198195                  factor            factor scores   \n",
       "17555   341407   199063                  factor                  factors   \n",
       "17818   341743   205238                 emotion  emotion differentiation   \n",
       "17828   341755    52554                  factor              comparisons   \n",
       "18460   342623   205826       factor extraction                   factor   \n",
       "18675   342897   206002           factor scores                      paf   \n",
       "19142   343463   178588                 minitab                    block   \n",
       "19147   343470   206373        items correlated             factor model   \n",
       "19267   343626   159259                    bias            factor scores   \n",
       "19752   344279   206936            fixed factor               know fixed   \n",
       "\n",
       "                   TopWord3               TopWord4              TopWord5  \\\n",
       "67                   factor           level factor                levene   \n",
       "526              clustering              dataframe             numerical   \n",
       "990                 factors                 factor                 dummy   \n",
       "1417                varimax                 factor           sign factor   \n",
       "2466               subjects                 factor               returns   \n",
       "2930                 factor             eigenvalue    accounted variance   \n",
       "3252             factor cfa                 factor         single factor   \n",
       "3672               complete                 factor     consider complete   \n",
       "5036        factor analysis            scale wrong                factor   \n",
       "5116          averaged time                   fama                factor   \n",
       "5355    factor combinations            studentized          measurements   \n",
       "5664              data item                 factor                    2n   \n",
       "7126                 factor             subgeneral             trifactor   \n",
       "7592     interaction factor  covariate interaction    factor significant   \n",
       "8026              covariate               adjusted                factor   \n",
       "8116                numeric          column factor        column happens   \n",
       "8125             split plot                 factor                  site   \n",
       "8603                     th      according weights  believe distribution   \n",
       "9752               loadings        common variance                factor   \n",
       "9788      control condition              condition          linear mixed   \n",
       "10823                 items          items loading                factor   \n",
       "10965          confirmatory        factor analysis                factor   \n",
       "10995           personality  personality dimension             principal   \n",
       "11733                levels                 factor            3rd factor   \n",
       "12139                 store                   shop          store chains   \n",
       "12861          coefficients  constituent variables    factor constituent   \n",
       "12905                 level       appears thinking  combinations removes   \n",
       "13527                factor        factor loadings      latent variables   \n",
       "13745             solutions                 latent                   efa   \n",
       "14371        number factors                 factor                number   \n",
       "14804     factors displayed         set indicators               factors   \n",
       "14922              subgroup        factor explains      percent variance   \n",
       "15128       factor analysis                 muthen                factor   \n",
       "15399                factor         generated data              solution   \n",
       "15557                latent       latent variables               run uvi   \n",
       "16138       blocking factor       factor treatment              blocking   \n",
       "16140             aic value             change aic         factor change   \n",
       "16386                  load        factor analysis              remember   \n",
       "16902       argument vector         classes object            error task   \n",
       "17087                factor  arbitrary procrustean  coefficient suitable   \n",
       "17241                scores                loyalty          satisfaction   \n",
       "17555                 cloud                 latent          cloud points   \n",
       "17818       differentiation               loadings                factor   \n",
       "17828  comparisons pairwise      explicitly obtain    factor combination   \n",
       "18460            extraction               cortable    extraction methods   \n",
       "18675      structure matrix            tetrachoric                factor   \n",
       "19142          block factor      factor covariates                factor   \n",
       "19147  correlated residuals                 factor                 items   \n",
       "19267                factor        avoiding method         bias avoiding   \n",
       "19752               disease                 affect                factor   \n",
       "\n",
       "       Community  \n",
       "67            86  \n",
       "526           86  \n",
       "990           86  \n",
       "1417          86  \n",
       "2466          86  \n",
       "2930          86  \n",
       "3252          86  \n",
       "3672          86  \n",
       "5036          86  \n",
       "5116          86  \n",
       "5355          86  \n",
       "5664          86  \n",
       "7126          86  \n",
       "7592          86  \n",
       "8026          86  \n",
       "8116          86  \n",
       "8125          86  \n",
       "8603          86  \n",
       "9752          86  \n",
       "9788          86  \n",
       "10823         86  \n",
       "10965         86  \n",
       "10995         86  \n",
       "11733         86  \n",
       "12139         86  \n",
       "12861         86  \n",
       "12905         86  \n",
       "13527         86  \n",
       "13745         86  \n",
       "14371         86  \n",
       "14804         86  \n",
       "14922         86  \n",
       "15128         86  \n",
       "15399         86  \n",
       "15557         86  \n",
       "16138         86  \n",
       "16140         86  \n",
       "16386         86  \n",
       "16902         86  \n",
       "17087         86  \n",
       "17241         86  \n",
       "17555         86  \n",
       "17818         86  \n",
       "17828         86  \n",
       "18460         86  \n",
       "18675         86  \n",
       "19142         86  \n",
       "19147         86  \n",
       "19267         86  \n",
       "19752         86  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df[post_df_w_comm[\"Community\"] == 86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>TopWord1</th>\n",
       "      <th>TopWord2</th>\n",
       "      <th>TopWord3</th>\n",
       "      <th>TopWord4</th>\n",
       "      <th>TopWord5</th>\n",
       "      <th>Community</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>317930</td>\n",
       "      <td>2392</td>\n",
       "      <td>model</td>\n",
       "      <td>mis specified</td>\n",
       "      <td>mis</td>\n",
       "      <td>model mis</td>\n",
       "      <td>adding parameters</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>318012</td>\n",
       "      <td>188026</td>\n",
       "      <td>model residuals</td>\n",
       "      <td>model</td>\n",
       "      <td>plots</td>\n",
       "      <td>appear bit</td>\n",
       "      <td>appears outliers</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>318256</td>\n",
       "      <td>188195</td>\n",
       "      <td>error variable</td>\n",
       "      <td>predict outcome</td>\n",
       "      <td>model</td>\n",
       "      <td>outcome</td>\n",
       "      <td>model diagram</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>318282</td>\n",
       "      <td>1390</td>\n",
       "      <td>smooth</td>\n",
       "      <td>effect</td>\n",
       "      <td>smooth effect</td>\n",
       "      <td>smooth interaction</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>318340</td>\n",
       "      <td>31363</td>\n",
       "      <td>deviance</td>\n",
       "      <td>model</td>\n",
       "      <td>deviance decrease</td>\n",
       "      <td>expect deviance</td>\n",
       "      <td>added model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>318341</td>\n",
       "      <td>138414</td>\n",
       "      <td>crimes</td>\n",
       "      <td>house prices</td>\n",
       "      <td>house</td>\n",
       "      <td>prices</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>318537</td>\n",
       "      <td>188357</td>\n",
       "      <td>chemotherapy</td>\n",
       "      <td>chemotherapy variable</td>\n",
       "      <td>variable equals</td>\n",
       "      <td>cox model</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>318576</td>\n",
       "      <td>86652</td>\n",
       "      <td>parameter</td>\n",
       "      <td>model selection</td>\n",
       "      <td>model</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>maximum likelihood</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>318664</td>\n",
       "      <td>178215</td>\n",
       "      <td>replicated</td>\n",
       "      <td>performance model</td>\n",
       "      <td>model</td>\n",
       "      <td>able train</td>\n",
       "      <td>performance</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>318796</td>\n",
       "      <td>113090</td>\n",
       "      <td>statistic</td>\n",
       "      <td>define conditional</td>\n",
       "      <td>conditional model</td>\n",
       "      <td>model</td>\n",
       "      <td>measurable</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>319289</td>\n",
       "      <td>188903</td>\n",
       "      <td>bnlearn</td>\n",
       "      <td>second model</td>\n",
       "      <td>naive bayes</td>\n",
       "      <td>naive</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>319453</td>\n",
       "      <td>130693</td>\n",
       "      <td>combination model</td>\n",
       "      <td>noint</td>\n",
       "      <td>point intervention</td>\n",
       "      <td>forecast combination</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>319455</td>\n",
       "      <td>136768</td>\n",
       "      <td>pixels</td>\n",
       "      <td>year</td>\n",
       "      <td>observations</td>\n",
       "      <td>replicated</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>319463</td>\n",
       "      <td>116440</td>\n",
       "      <td>validation set</td>\n",
       "      <td>validation</td>\n",
       "      <td>set</td>\n",
       "      <td>generalization performance</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>319656</td>\n",
       "      <td>140300</td>\n",
       "      <td>prior</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>theil</td>\n",
       "      <td>model</td>\n",
       "      <td>cauchy</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>319903</td>\n",
       "      <td>35989</td>\n",
       "      <td>attending</td>\n",
       "      <td>closer certain</td>\n",
       "      <td>model</td>\n",
       "      <td>probability attending</td>\n",
       "      <td>priors</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>319960</td>\n",
       "      <td>189357</td>\n",
       "      <td>smaller model</td>\n",
       "      <td>model</td>\n",
       "      <td>anova turn</td>\n",
       "      <td>claim severity</td>\n",
       "      <td>finally reduced</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>319964</td>\n",
       "      <td>2310</td>\n",
       "      <td>smaller model</td>\n",
       "      <td>smaller</td>\n",
       "      <td>model</td>\n",
       "      <td>bic</td>\n",
       "      <td>agree looks</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>320080</td>\n",
       "      <td>178864</td>\n",
       "      <td>causalimpact</td>\n",
       "      <td>model</td>\n",
       "      <td>strong time</td>\n",
       "      <td>yada</td>\n",
       "      <td>secondary dataset</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>320108</td>\n",
       "      <td>189427</td>\n",
       "      <td>regsubsets</td>\n",
       "      <td>model</td>\n",
       "      <td>far predict</td>\n",
       "      <td>function regsubsets</td>\n",
       "      <td>pb1 pb2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>320207</td>\n",
       "      <td>180168</td>\n",
       "      <td>dk</td>\n",
       "      <td>agree midly</td>\n",
       "      <td>midly</td>\n",
       "      <td>response</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>320281</td>\n",
       "      <td>140574</td>\n",
       "      <td>exam</td>\n",
       "      <td>exam1</td>\n",
       "      <td>cheating</td>\n",
       "      <td>second exam</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>320519</td>\n",
       "      <td>123571</td>\n",
       "      <td>plot</td>\n",
       "      <td>check model</td>\n",
       "      <td>model variance</td>\n",
       "      <td>model</td>\n",
       "      <td>affect consider</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>320723</td>\n",
       "      <td>18459</td>\n",
       "      <td>utility model</td>\n",
       "      <td>utility</td>\n",
       "      <td>model</td>\n",
       "      <td>entity</td>\n",
       "      <td>entity making</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>320732</td>\n",
       "      <td>28500</td>\n",
       "      <td>predictors</td>\n",
       "      <td>response</td>\n",
       "      <td>response types</td>\n",
       "      <td>types</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>320928</td>\n",
       "      <td>190034</td>\n",
       "      <td>regression model</td>\n",
       "      <td>model</td>\n",
       "      <td>assume normal</td>\n",
       "      <td>normal model</td>\n",
       "      <td>regression</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>320965</td>\n",
       "      <td>60261</td>\n",
       "      <td>hurdle</td>\n",
       "      <td>model</td>\n",
       "      <td>poisson</td>\n",
       "      <td>zeros</td>\n",
       "      <td>zero</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>321042</td>\n",
       "      <td>99274</td>\n",
       "      <td>statistical model</td>\n",
       "      <td>statistical</td>\n",
       "      <td>model</td>\n",
       "      <td>distributions suppose</td>\n",
       "      <td>family</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>321208</td>\n",
       "      <td>8013</td>\n",
       "      <td>multinomial model</td>\n",
       "      <td>multinomial</td>\n",
       "      <td>considerably</td>\n",
       "      <td>model</td>\n",
       "      <td>candidate predictors</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>321267</td>\n",
       "      <td>27961</td>\n",
       "      <td>items</td>\n",
       "      <td>user</td>\n",
       "      <td>sampling</td>\n",
       "      <td>model</td>\n",
       "      <td>random sampling</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16171</th>\n",
       "      <td>339600</td>\n",
       "      <td>102117</td>\n",
       "      <td>optimization</td>\n",
       "      <td>predictive model</td>\n",
       "      <td>decisions</td>\n",
       "      <td>data</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16379</th>\n",
       "      <td>339857</td>\n",
       "      <td>191904</td>\n",
       "      <td>regression model</td>\n",
       "      <td>model</td>\n",
       "      <td>considering linnear</td>\n",
       "      <td>lasso penalization</td>\n",
       "      <td>like quantile</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16422</th>\n",
       "      <td>339918</td>\n",
       "      <td>203939</td>\n",
       "      <td>model runs</td>\n",
       "      <td>runs</td>\n",
       "      <td>model</td>\n",
       "      <td>adapted model</td>\n",
       "      <td>answer subset</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16460</th>\n",
       "      <td>339971</td>\n",
       "      <td>203961</td>\n",
       "      <td>rmse mape</td>\n",
       "      <td>rmse</td>\n",
       "      <td>mape</td>\n",
       "      <td>accurate</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16484</th>\n",
       "      <td>340005</td>\n",
       "      <td>199063</td>\n",
       "      <td>difference</td>\n",
       "      <td>fit1</td>\n",
       "      <td>optimal difference</td>\n",
       "      <td>constrained difference</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16486</th>\n",
       "      <td>340007</td>\n",
       "      <td>203998</td>\n",
       "      <td>degrees freedom</td>\n",
       "      <td>degrees</td>\n",
       "      <td>freedom</td>\n",
       "      <td>model</td>\n",
       "      <td>bias risk</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16518</th>\n",
       "      <td>340047</td>\n",
       "      <td>30351</td>\n",
       "      <td>formulas divide</td>\n",
       "      <td>model</td>\n",
       "      <td>average model</td>\n",
       "      <td>numeric</td>\n",
       "      <td>add formulas</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16520</th>\n",
       "      <td>340051</td>\n",
       "      <td>27608</td>\n",
       "      <td>significance</td>\n",
       "      <td>interactions dichotomous</td>\n",
       "      <td>significance remaining</td>\n",
       "      <td>model</td>\n",
       "      <td>significant</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16633</th>\n",
       "      <td>340192</td>\n",
       "      <td>36229</td>\n",
       "      <td>binned</td>\n",
       "      <td>household</td>\n",
       "      <td>bin</td>\n",
       "      <td>model</td>\n",
       "      <td>replace binned</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16792</th>\n",
       "      <td>340400</td>\n",
       "      <td>204327</td>\n",
       "      <td>unknown data</td>\n",
       "      <td>model</td>\n",
       "      <td>unknown</td>\n",
       "      <td>rmse</td>\n",
       "      <td>prediction</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16864</th>\n",
       "      <td>340489</td>\n",
       "      <td>178816</td>\n",
       "      <td>deviance</td>\n",
       "      <td>model</td>\n",
       "      <td>data residual</td>\n",
       "      <td>deviance small</td>\n",
       "      <td>df say</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17027</th>\n",
       "      <td>340701</td>\n",
       "      <td>60261</td>\n",
       "      <td>censored truncated</td>\n",
       "      <td>censored</td>\n",
       "      <td>truncated</td>\n",
       "      <td>model</td>\n",
       "      <td>hurdle</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17124</th>\n",
       "      <td>340839</td>\n",
       "      <td>193315</td>\n",
       "      <td>forecasts</td>\n",
       "      <td>model</td>\n",
       "      <td>determined variance</td>\n",
       "      <td>fairly naive</td>\n",
       "      <td>forecasts distribution</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17235</th>\n",
       "      <td>340986</td>\n",
       "      <td>117573</td>\n",
       "      <td>selection</td>\n",
       "      <td>mean equation</td>\n",
       "      <td>probit model</td>\n",
       "      <td>model</td>\n",
       "      <td>stages</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17238</th>\n",
       "      <td>340990</td>\n",
       "      <td>204725</td>\n",
       "      <td>transformation</td>\n",
       "      <td>regression model</td>\n",
       "      <td>using linear</td>\n",
       "      <td>regression</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17406</th>\n",
       "      <td>341209</td>\n",
       "      <td>204885</td>\n",
       "      <td>revenue</td>\n",
       "      <td>model better</td>\n",
       "      <td>model</td>\n",
       "      <td>better</td>\n",
       "      <td>comparing posteriors</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17644</th>\n",
       "      <td>341534</td>\n",
       "      <td>75357</td>\n",
       "      <td>testing</td>\n",
       "      <td>backtesting</td>\n",
       "      <td>testing period</td>\n",
       "      <td>model</td>\n",
       "      <td>model built</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17675</th>\n",
       "      <td>341570</td>\n",
       "      <td>4598</td>\n",
       "      <td>cross validation</td>\n",
       "      <td>validation</td>\n",
       "      <td>cross</td>\n",
       "      <td>performance</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18507</th>\n",
       "      <td>342684</td>\n",
       "      <td>7290</td>\n",
       "      <td>saturated model</td>\n",
       "      <td>saturated</td>\n",
       "      <td>separate parameter</td>\n",
       "      <td>degenerate distribution</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18536</th>\n",
       "      <td>342719</td>\n",
       "      <td>39770</td>\n",
       "      <td>aic</td>\n",
       "      <td>noise variables</td>\n",
       "      <td>higher higher</td>\n",
       "      <td>non mathematical</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18546</th>\n",
       "      <td>342732</td>\n",
       "      <td>205892</td>\n",
       "      <td>inserted</td>\n",
       "      <td>low</td>\n",
       "      <td>predictors</td>\n",
       "      <td>predictor</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18551</th>\n",
       "      <td>342737</td>\n",
       "      <td>89649</td>\n",
       "      <td>underlying model</td>\n",
       "      <td>true underlying</td>\n",
       "      <td>true</td>\n",
       "      <td>model</td>\n",
       "      <td>true model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18957</th>\n",
       "      <td>343243</td>\n",
       "      <td>200312</td>\n",
       "      <td>model</td>\n",
       "      <td>errors</td>\n",
       "      <td>model errors</td>\n",
       "      <td>simple linear</td>\n",
       "      <td>outcome variable</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19066</th>\n",
       "      <td>343371</td>\n",
       "      <td>63739</td>\n",
       "      <td>model</td>\n",
       "      <td>let</td>\n",
       "      <td>model parameters</td>\n",
       "      <td>parameters</td>\n",
       "      <td>according straight</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19103</th>\n",
       "      <td>343420</td>\n",
       "      <td>4253</td>\n",
       "      <td>bayesian</td>\n",
       "      <td>penalization</td>\n",
       "      <td>model</td>\n",
       "      <td>predictive</td>\n",
       "      <td>posterior</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19175</th>\n",
       "      <td>343517</td>\n",
       "      <td>206398</td>\n",
       "      <td>pls</td>\n",
       "      <td>h2o</td>\n",
       "      <td>h2o model</td>\n",
       "      <td>pls model</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19395</th>\n",
       "      <td>343810</td>\n",
       "      <td>78268</td>\n",
       "      <td>say day</td>\n",
       "      <td>new variables</td>\n",
       "      <td>day</td>\n",
       "      <td>model</td>\n",
       "      <td>success</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19550</th>\n",
       "      <td>344005</td>\n",
       "      <td>35393</td>\n",
       "      <td>model</td>\n",
       "      <td>recreational</td>\n",
       "      <td>seeking help</td>\n",
       "      <td>different land</td>\n",
       "      <td>land use</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19860</th>\n",
       "      <td>344435</td>\n",
       "      <td>4253</td>\n",
       "      <td>event model</td>\n",
       "      <td>event</td>\n",
       "      <td>peaks</td>\n",
       "      <td>time event</td>\n",
       "      <td>model</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>344580</td>\n",
       "      <td>195935</td>\n",
       "      <td>model</td>\n",
       "      <td>mean</td>\n",
       "      <td>application model</td>\n",
       "      <td>baseline renders</td>\n",
       "      <td>building important</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id  user_id            TopWord1                  TopWord2  \\\n",
       "180     317930     2392               model             mis specified   \n",
       "249     318012   188026     model residuals                     model   \n",
       "429     318256   188195      error variable           predict outcome   \n",
       "450     318282     1390              smooth                    effect   \n",
       "498     318340    31363            deviance                     model   \n",
       "499     318341   138414              crimes              house prices   \n",
       "653     318537   188357        chemotherapy     chemotherapy variable   \n",
       "683     318576    86652           parameter           model selection   \n",
       "749     318664   178215          replicated         performance model   \n",
       "856     318796   113090           statistic        define conditional   \n",
       "1234    319289   188903             bnlearn              second model   \n",
       "1369    319453   130693   combination model                     noint   \n",
       "1371    319455   136768              pixels                      year   \n",
       "1377    319463   116440      validation set                validation   \n",
       "1519    319656   140300               prior                likelihood   \n",
       "1714    319903    35989           attending            closer certain   \n",
       "1761    319960   189357       smaller model                     model   \n",
       "1765    319964     2310       smaller model                   smaller   \n",
       "1852    320080   178864        causalimpact                     model   \n",
       "1873    320108   189427          regsubsets                     model   \n",
       "1948    320207   180168                  dk               agree midly   \n",
       "2009    320281   140574                exam                     exam1   \n",
       "2182    320519   123571                plot               check model   \n",
       "2348    320723    18459       utility model                   utility   \n",
       "2356    320732    28500          predictors                  response   \n",
       "2488    320928   190034    regression model                     model   \n",
       "2519    320965    60261              hurdle                     model   \n",
       "2588    321042    99274   statistical model               statistical   \n",
       "2717    321208     8013   multinomial model               multinomial   \n",
       "2765    321267    27961               items                      user   \n",
       "...        ...      ...                 ...                       ...   \n",
       "16171   339600   102117        optimization          predictive model   \n",
       "16379   339857   191904    regression model                     model   \n",
       "16422   339918   203939          model runs                      runs   \n",
       "16460   339971   203961           rmse mape                      rmse   \n",
       "16484   340005   199063          difference                      fit1   \n",
       "16486   340007   203998     degrees freedom                   degrees   \n",
       "16518   340047    30351     formulas divide                     model   \n",
       "16520   340051    27608        significance  interactions dichotomous   \n",
       "16633   340192    36229              binned                 household   \n",
       "16792   340400   204327        unknown data                     model   \n",
       "16864   340489   178816            deviance                     model   \n",
       "17027   340701    60261  censored truncated                  censored   \n",
       "17124   340839   193315           forecasts                     model   \n",
       "17235   340986   117573           selection             mean equation   \n",
       "17238   340990   204725      transformation          regression model   \n",
       "17406   341209   204885             revenue              model better   \n",
       "17644   341534    75357             testing               backtesting   \n",
       "17675   341570     4598    cross validation                validation   \n",
       "18507   342684     7290     saturated model                 saturated   \n",
       "18536   342719    39770                 aic           noise variables   \n",
       "18546   342732   205892            inserted                       low   \n",
       "18551   342737    89649    underlying model           true underlying   \n",
       "18957   343243   200312               model                    errors   \n",
       "19066   343371    63739               model                       let   \n",
       "19103   343420     4253            bayesian              penalization   \n",
       "19175   343517   206398                 pls                       h2o   \n",
       "19395   343810    78268             say day             new variables   \n",
       "19550   344005    35393               model              recreational   \n",
       "19860   344435     4253         event model                     event   \n",
       "19975   344580   195935               model                      mean   \n",
       "\n",
       "                     TopWord3                    TopWord4  \\\n",
       "180                       mis                   model mis   \n",
       "249                     plots                  appear bit   \n",
       "429                     model                     outcome   \n",
       "450             smooth effect          smooth interaction   \n",
       "498         deviance decrease             expect deviance   \n",
       "499                     house                      prices   \n",
       "653           variable equals                   cox model   \n",
       "683                     model                 uncertainty   \n",
       "749                     model                  able train   \n",
       "856         conditional model                       model   \n",
       "1234              naive bayes                       naive   \n",
       "1369       point intervention        forecast combination   \n",
       "1371             observations                  replicated   \n",
       "1377                      set  generalization performance   \n",
       "1519                    theil                       model   \n",
       "1714                    model       probability attending   \n",
       "1761               anova turn              claim severity   \n",
       "1765                    model                         bic   \n",
       "1852              strong time                        yada   \n",
       "1873              far predict         function regsubsets   \n",
       "1948                    midly                    response   \n",
       "2009                 cheating                 second exam   \n",
       "2182           model variance                       model   \n",
       "2348                    model                      entity   \n",
       "2356           response types                       types   \n",
       "2488            assume normal                normal model   \n",
       "2519                  poisson                       zeros   \n",
       "2588                    model       distributions suppose   \n",
       "2717             considerably                       model   \n",
       "2765                 sampling                       model   \n",
       "...                       ...                         ...   \n",
       "16171               decisions                        data   \n",
       "16379     considering linnear          lasso penalization   \n",
       "16422                   model               adapted model   \n",
       "16460                    mape                    accurate   \n",
       "16484      optimal difference      constrained difference   \n",
       "16486                 freedom                       model   \n",
       "16518           average model                     numeric   \n",
       "16520  significance remaining                       model   \n",
       "16633                     bin                       model   \n",
       "16792                 unknown                        rmse   \n",
       "16864           data residual              deviance small   \n",
       "17027               truncated                       model   \n",
       "17124     determined variance                fairly naive   \n",
       "17235            probit model                       model   \n",
       "17238            using linear                  regression   \n",
       "17406                   model                      better   \n",
       "17644          testing period                       model   \n",
       "17675                   cross                 performance   \n",
       "18507      separate parameter     degenerate distribution   \n",
       "18536           higher higher            non mathematical   \n",
       "18546              predictors                   predictor   \n",
       "18551                    true                       model   \n",
       "18957            model errors               simple linear   \n",
       "19066        model parameters                  parameters   \n",
       "19103                   model                  predictive   \n",
       "19175               h2o model                   pls model   \n",
       "19395                     day                       model   \n",
       "19550            seeking help              different land   \n",
       "19860                   peaks                  time event   \n",
       "19975       application model            baseline renders   \n",
       "\n",
       "                     TopWord5  Community  \n",
       "180         adding parameters         16  \n",
       "249          appears outliers         16  \n",
       "429             model diagram         16  \n",
       "450                     model         16  \n",
       "498               added model         16  \n",
       "499                     model         16  \n",
       "653                     model         16  \n",
       "683        maximum likelihood         16  \n",
       "749               performance         16  \n",
       "856                measurable         16  \n",
       "1234                    model         16  \n",
       "1369                    model         16  \n",
       "1371                    model         16  \n",
       "1377                    model         16  \n",
       "1519                   cauchy         16  \n",
       "1714                   priors         16  \n",
       "1761          finally reduced         16  \n",
       "1765              agree looks         16  \n",
       "1852        secondary dataset         16  \n",
       "1873                  pb1 pb2         16  \n",
       "1948                    model         16  \n",
       "2009                    model         16  \n",
       "2182          affect consider         16  \n",
       "2348            entity making         16  \n",
       "2356                    model         16  \n",
       "2488               regression         16  \n",
       "2519                     zero         16  \n",
       "2588                   family         16  \n",
       "2717     candidate predictors         16  \n",
       "2765          random sampling         16  \n",
       "...                       ...        ...  \n",
       "16171                   model         16  \n",
       "16379           like quantile         16  \n",
       "16422           answer subset         16  \n",
       "16460                   model         16  \n",
       "16484                   model         16  \n",
       "16486               bias risk         16  \n",
       "16518            add formulas         16  \n",
       "16520             significant         16  \n",
       "16633          replace binned         16  \n",
       "16792              prediction         16  \n",
       "16864                  df say         16  \n",
       "17027                  hurdle         16  \n",
       "17124  forecasts distribution         16  \n",
       "17235                  stages         16  \n",
       "17238                   model         16  \n",
       "17406    comparing posteriors         16  \n",
       "17644             model built         16  \n",
       "17675                   model         16  \n",
       "18507                   model         16  \n",
       "18536                   model         16  \n",
       "18546                   model         16  \n",
       "18551              true model         16  \n",
       "18957        outcome variable         16  \n",
       "19066      according straight         16  \n",
       "19103               posterior         16  \n",
       "19175                   model         16  \n",
       "19395                 success         16  \n",
       "19550                land use         16  \n",
       "19860                   model         16  \n",
       "19975      building important         16  \n",
       "\n",
       "[169 rows x 8 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df[post_df_w_comm[\"Community\"] == 16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- word to vec\n",
    "- average top five words in post, compute similarity in cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
