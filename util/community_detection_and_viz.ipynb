{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snap\n",
    "from os import path\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"../data/stats.stackexchange.com/Mixed\"\n",
    "FOLDED_NGRAM_GRAPH_PATH = path.join(BASE_PATH, \"Userid_Ngram_Folded_Graph.graph\")\n",
    "FOLDED_POSTID_GRAPH_PATH = path.join(BASE_PATH, \"Postid_Folded_Graph.graph\")\n",
    "NGRAM_DICT_PICKLE = path.join(BASE_PATH, \"Bigramid_Dict\")\n",
    "POSTID_PICKLE = path.join(BASE_PATH, \"STATS_20k-Posts_11-top_uni&bigrams_nostem.pickle\")\n",
    "POST_TOP_NGRAM_PATH = path.join(BASE_PATH, \"STATS_20k-Posts_11-top_uni&bigrams_nostem.tsv\")\n",
    "COMMUNITIES_PATH = path.join(BASE_PATH, 'postid-communities-with-postbodies.txt')\n",
    "COMMUNITIES_VEC_PATH = path.join(BASE_PATH, 'postid-communities.vector')\n",
    "EDGELIST_PATH = path.join(BASE_PATH, 'postid_edges.txt')\n",
    "COMMUNITIES_VIZ_PATH = path.join(BASE_PATH, 'postid-communities-viz.csv')\n",
    "COMMUNITIES_VIZ_PATH2 = path.join(BASE_PATH, 'postid-spectral-communities-viz.csv')\n",
    "SPEC_COMMUNITIES_PICKLE = path.join(BASE_PATH, 'Spectral_Node_to_Community_dict.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modularity(G, community_dict):\n",
    "    two_M = G.GetEdges() * 2.0\n",
    "    mod_sum = 0.0\n",
    "    for NI in G.Nodes():\n",
    "        NI_id = NI.GetId()\n",
    "        for NJ in G.Nodes():\n",
    "            NJ_id = NJ.GetId()\n",
    "            if (community_dict[NI_id] == community_dict[NJ_id]):\n",
    "                mod_sum += G.IsEdge(NI_id, NJ_id) - ((NI.GetDeg() * NJ.GetDeg()) / two_M)\n",
    "    modularity = mod_sum / two_M\n",
    "    return modularity\n",
    "\n",
    "def load_top_ngram_df(topwords_path):\n",
    "    # Load csv containing the top words.\n",
    "    posts_df = pd.read_csv(topwords_path, sep = \"\\t\", usecols =\n",
    "                           [\"Id\", \"OwnerUserId\", \"TopWord1\", \"TopWord2\", \"TopWord3\", \"TopWord4\", \"TopWord5\"])\n",
    "\n",
    "    # Clean dataframe.\n",
    "    posts_df = posts_df.dropna()\n",
    "    posts_df = posts_df.rename(columns={\n",
    "            \"Id\": \"post_id\", \"OwnerUserId\": \"user_id\"})\n",
    "    posts_df[\"user_id\"] = posts_df[\"user_id\"].astype(np.int64)\n",
    "    posts_df[\"post_id\"] = posts_df[\"post_id\"].astype(np.int64)\n",
    "    posts_df = posts_df[posts_df[\"user_id\"] > 0]\n",
    "    posts_df = posts_df[posts_df[\"post_id\"] > 0]\n",
    "\n",
    "    return posts_df\n",
    "\n",
    "def add_communities_post_df(post_df, best_comm_map, postid_dict):\n",
    "    # Iterate over rows and add each community to each row.\n",
    "    community_array = []\n",
    "    for index, row in post_df.iterrows():\n",
    "        user_id = row[\"post_id\"]\n",
    "        A_id = postid_dict.get(user_id, -1)\n",
    "        if (A_id < 0):\n",
    "            community_array.append(-1)\n",
    "        else:\n",
    "            community_array.append(best_comm_map[A_id])\n",
    "    post_df.loc[:,'Community'] = community_array\n",
    "\n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original nodes 19725\n",
      "edges 567729\n",
      "new nodes (no degree-0) 17139\n"
     ]
    }
   ],
   "source": [
    "f_in = snap.TFIn(FOLDED_POSTID_GRAPH_PATH)\n",
    "post_graph = snap.TUNGraph.Load(f_in)\n",
    "print \"original nodes\", post_graph.GetNodes()\n",
    "print \"edges\", post_graph.GetEdges()\n",
    "assert snap.CntSelfEdges(post_graph) == 0\n",
    "snap.DelZeroDegNodes(post_graph)\n",
    "print \"new nodes (no degree-0)\", post_graph.GetNodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1335it [02:54, 27.28it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "comm_vec = snap.TCnComV()\n",
    "modularity = snap.CommunityCNM(post_graph, comm_vec)\n",
    "\n",
    "f_out = snap.TFOut(COMMUNITIES_VEC_PATH)\n",
    "comm_vec.Save(f_out)\n",
    "f_out.Flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "communities 66\n"
     ]
    }
   ],
   "source": [
    "f_in = snap.TFIn(COMMUNITIES_VEC_PATH)\n",
    "comm_vec = snap.TCnComV()\n",
    "comm_vec.Load(f_in)\n",
    "\n",
    "print \"communities\", len(comm_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = open(POSTID_PICKLE, 'rb')\n",
    "postid_dict = pickle.load(pickle_file)\n",
    "\n",
    "community_dict = collections.defaultdict(int)\n",
    "\n",
    "with open(COMMUNITIES_PATH, 'w') as f:\n",
    "    for i, comm in enumerate(comm_vec):\n",
    "        f.write(\"#####Community {}#####\\n\".format(i))\n",
    "        community = snap.TIntV()\n",
    "        for node in comm:\n",
    "            community.Add(node)\n",
    "#             f.write(\"Node {}: {}\\n\".format(node, postid_dict[node]))\n",
    "            community_dict[node] = i\n",
    "        f.write('Community {}, nodes: {} modularity: {}\\n'.format(i, len(comm), snap.GetModularity(post_graph, community, post_graph.GetEdges())))\n",
    "    f.write(\"The modularity of the network is {}\\n\".format(modularity))\n",
    "    alt_modularity = get_modularity(post_graph, community_dict)\n",
    "    f.write(\"Alternate modularity of the network (sanity check) is {}\".format(alt_modularity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "postid_dict2 = collections.defaultdict(int)\n",
    "for node in community_dict:\n",
    "    postid_dict2[node] = node\n",
    "\n",
    "post_df = load_top_ngram_df(POST_TOP_NGRAM_PATH)\n",
    "post_df_w_comm = add_communities_post_df(post_df, community_dict, postid_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community: 0 Size: 4030\n",
      "Community: 1 Size: 2303\n",
      "Community: 2 Size: 24\n",
      "Community: 3 Size: 4711\n",
      "Community: 4 Size: 5343\n",
      "Community: 5 Size: 97\n",
      "Community: 6 Size: 4\n",
      "Community: 7 Size: 5\n",
      "Community: 8 Size: 159\n",
      "Community: 9 Size: 127\n",
      "Community: 10 Size: 2\n",
      "Community: 11 Size: 4\n",
      "Community: 12 Size: 9\n",
      "Community: 13 Size: 36\n",
      "Community: 14 Size: 4\n",
      "Community: 15 Size: 9\n",
      "Community: 16 Size: 2\n",
      "Community: 17 Size: 46\n",
      "Community: 18 Size: 3\n",
      "Community: 19 Size: 8\n",
      "Community: 20 Size: 2\n",
      "Community: 21 Size: 7\n",
      "Community: 22 Size: 55\n",
      "Community: 23 Size: 4\n",
      "Community: 24 Size: 4\n",
      "Community: 25 Size: 3\n",
      "Community: 26 Size: 4\n",
      "Community: 27 Size: 5\n",
      "Community: 28 Size: 10\n",
      "Community: 29 Size: 17\n",
      "Community: 30 Size: 4\n",
      "Community: 31 Size: 8\n",
      "Community: 32 Size: 12\n",
      "Community: 33 Size: 2\n",
      "Community: 34 Size: 2\n",
      "Community: 35 Size: 2\n",
      "Community: 36 Size: 4\n",
      "Community: 37 Size: 4\n",
      "Community: 38 Size: 2\n",
      "Community: 39 Size: 5\n",
      "Community: 40 Size: 2\n",
      "Community: 41 Size: 2\n",
      "Community: 42 Size: 3\n",
      "Community: 43 Size: 2\n",
      "Community: 44 Size: 2\n",
      "Community: 45 Size: 2\n",
      "Community: 46 Size: 2\n",
      "Community: 47 Size: 2\n",
      "Community: 48 Size: 5\n",
      "Community: 49 Size: 2\n",
      "Community: 50 Size: 2\n",
      "Community: 51 Size: 2\n",
      "Community: 52 Size: 2\n",
      "Community: 53 Size: 2\n",
      "Community: 54 Size: 2\n",
      "Community: 55 Size: 2\n",
      "Community: 56 Size: 2\n",
      "Community: 57 Size: 2\n",
      "Community: 58 Size: 2\n",
      "Community: 59 Size: 2\n",
      "Community: 60 Size: 2\n",
      "Community: 61 Size: 3\n",
      "Community: 62 Size: 2\n",
      "Community: 63 Size: 2\n",
      "Community: 64 Size: 2\n",
      "Community: 65 Size: 2\n",
      "Community: -1 Size: 2586\n"
     ]
    }
   ],
   "source": [
    "communities = set(post_df_w_comm[\"Community\"])\n",
    "for comm in communities:\n",
    "    print \"Community:\", comm, \"Size:\", len(post_df[post_df_w_comm[\"Community\"] == comm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>TopWord1</th>\n",
       "      <th>TopWord2</th>\n",
       "      <th>TopWord3</th>\n",
       "      <th>TopWord4</th>\n",
       "      <th>TopWord5</th>\n",
       "      <th>Community</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>318158</td>\n",
       "      <td>175125</td>\n",
       "      <td>risky</td>\n",
       "      <td>labelled</td>\n",
       "      <td>deemed risky</td>\n",
       "      <td>risky risky</td>\n",
       "      <td>labelled data</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>318379</td>\n",
       "      <td>164061</td>\n",
       "      <td>dice</td>\n",
       "      <td>roll</td>\n",
       "      <td>limit summation</td>\n",
       "      <td>rolls dice</td>\n",
       "      <td>roll dice</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>319047</td>\n",
       "      <td>8013</td>\n",
       "      <td>cut</td>\n",
       "      <td>sensitivity</td>\n",
       "      <td>harms</td>\n",
       "      <td>specificity</td>\n",
       "      <td>sensitivity specificity</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>319730</td>\n",
       "      <td>121522</td>\n",
       "      <td>existing model</td>\n",
       "      <td>existing</td>\n",
       "      <td>risky</td>\n",
       "      <td>deems risky</td>\n",
       "      <td>deems</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>320994</td>\n",
       "      <td>142829</td>\n",
       "      <td>wire</td>\n",
       "      <td>probability wire</td>\n",
       "      <td>wire cut</td>\n",
       "      <td>wire length</td>\n",
       "      <td>cut</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>321181</td>\n",
       "      <td>190212</td>\n",
       "      <td>existing cohort</td>\n",
       "      <td>cohort</td>\n",
       "      <td>power study</td>\n",
       "      <td>prospective</td>\n",
       "      <td>existing</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>323039</td>\n",
       "      <td>23732</td>\n",
       "      <td>vis</td>\n",
       "      <td>second order</td>\n",
       "      <td>second</td>\n",
       "      <td>better fits</td>\n",
       "      <td>check complex</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4883</th>\n",
       "      <td>323970</td>\n",
       "      <td>60065</td>\n",
       "      <td>corr</td>\n",
       "      <td>corr corr</td>\n",
       "      <td>cutoff</td>\n",
       "      <td>add edge</td>\n",
       "      <td>add edges</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5035</th>\n",
       "      <td>324172</td>\n",
       "      <td>115061</td>\n",
       "      <td>hand cut</td>\n",
       "      <td>cut point</td>\n",
       "      <td>cut</td>\n",
       "      <td>age understand</td>\n",
       "      <td>coefficient lower</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6764</th>\n",
       "      <td>326316</td>\n",
       "      <td>28740</td>\n",
       "      <td>tpr</td>\n",
       "      <td>fpr</td>\n",
       "      <td>assessing probabilistic</td>\n",
       "      <td>specificity</td>\n",
       "      <td>sensitivity</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8528</th>\n",
       "      <td>328550</td>\n",
       "      <td>133901</td>\n",
       "      <td>prevalence</td>\n",
       "      <td>sensitivity</td>\n",
       "      <td>specificity</td>\n",
       "      <td>properties test</td>\n",
       "      <td>outpatients</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8984</th>\n",
       "      <td>329150</td>\n",
       "      <td>78964</td>\n",
       "      <td>cut essence</td>\n",
       "      <td>essence matter</td>\n",
       "      <td>like cut</td>\n",
       "      <td>essence</td>\n",
       "      <td>cut</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>329605</td>\n",
       "      <td>1352</td>\n",
       "      <td>baskets</td>\n",
       "      <td>cutoff</td>\n",
       "      <td>baskets contain</td>\n",
       "      <td>subset baskets</td>\n",
       "      <td>contain cutoff</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9829</th>\n",
       "      <td>330278</td>\n",
       "      <td>130206</td>\n",
       "      <td>sensitivity specificity</td>\n",
       "      <td>mausner</td>\n",
       "      <td>specificity</td>\n",
       "      <td>ppv npv</td>\n",
       "      <td>sensitivity</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9922</th>\n",
       "      <td>330409</td>\n",
       "      <td>7290</td>\n",
       "      <td>alt</td>\n",
       "      <td>sensitivity</td>\n",
       "      <td>liver</td>\n",
       "      <td>specificity</td>\n",
       "      <td>levels alt</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>330508</td>\n",
       "      <td>189008</td>\n",
       "      <td>dice</td>\n",
       "      <td>dice faces</td>\n",
       "      <td>faces numbers</td>\n",
       "      <td>like clt</td>\n",
       "      <td>numbers dice</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10071</th>\n",
       "      <td>330619</td>\n",
       "      <td>196757</td>\n",
       "      <td>essence</td>\n",
       "      <td>bonus implement</td>\n",
       "      <td>capture essence</td>\n",
       "      <td>classical error</td>\n",
       "      <td>endogeneity classical</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10103</th>\n",
       "      <td>330666</td>\n",
       "      <td>114446</td>\n",
       "      <td>specificity</td>\n",
       "      <td>sensitivity</td>\n",
       "      <td>sensitivity specificity</td>\n",
       "      <td>prevalence</td>\n",
       "      <td>dc</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11271</th>\n",
       "      <td>332149</td>\n",
       "      <td>162303</td>\n",
       "      <td>sanitation</td>\n",
       "      <td>malaria</td>\n",
       "      <td>prevalence</td>\n",
       "      <td>community sanitation</td>\n",
       "      <td>dhs</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11764</th>\n",
       "      <td>332806</td>\n",
       "      <td>198339</td>\n",
       "      <td>dice</td>\n",
       "      <td>value dice</td>\n",
       "      <td>dice having</td>\n",
       "      <td>dice probability</td>\n",
       "      <td>having value</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11789</th>\n",
       "      <td>332834</td>\n",
       "      <td>164061</td>\n",
       "      <td>dice</td>\n",
       "      <td>dices</td>\n",
       "      <td>dice roll</td>\n",
       "      <td>roll</td>\n",
       "      <td>probability sided</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12084</th>\n",
       "      <td>333227</td>\n",
       "      <td>198641</td>\n",
       "      <td>bracket</td>\n",
       "      <td>safe</td>\n",
       "      <td>risky</td>\n",
       "      <td>roll</td>\n",
       "      <td>head</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12178</th>\n",
       "      <td>333353</td>\n",
       "      <td>198724</td>\n",
       "      <td>die</td>\n",
       "      <td>roll</td>\n",
       "      <td>second die</td>\n",
       "      <td>dice simultaneously</td>\n",
       "      <td>second</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>335260</td>\n",
       "      <td>200333</td>\n",
       "      <td>unstandarized</td>\n",
       "      <td>unstandarized value</td>\n",
       "      <td>value rate</td>\n",
       "      <td>loan</td>\n",
       "      <td>research</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13578</th>\n",
       "      <td>336178</td>\n",
       "      <td>201105</td>\n",
       "      <td>existing predictors</td>\n",
       "      <td>existing</td>\n",
       "      <td>additivity instead</td>\n",
       "      <td>changes existing</td>\n",
       "      <td>coefficients existing</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13911</th>\n",
       "      <td>336599</td>\n",
       "      <td>11887</td>\n",
       "      <td>dice</td>\n",
       "      <td>times</td>\n",
       "      <td>answered completely</td>\n",
       "      <td>boxes formulation</td>\n",
       "      <td>comes multinomial</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15531</th>\n",
       "      <td>338749</td>\n",
       "      <td>6479</td>\n",
       "      <td>prevalence</td>\n",
       "      <td>alternatives correct</td>\n",
       "      <td>binomial exact</td>\n",
       "      <td>cited exact</td>\n",
       "      <td>crossvalidated bunch</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16628</th>\n",
       "      <td>340187</td>\n",
       "      <td>154156</td>\n",
       "      <td>cutoff</td>\n",
       "      <td>di</td>\n",
       "      <td>cut</td>\n",
       "      <td>parameters sample</td>\n",
       "      <td>specifically</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16638</th>\n",
       "      <td>340198</td>\n",
       "      <td>158101</td>\n",
       "      <td>sensitivity</td>\n",
       "      <td>loan</td>\n",
       "      <td>risky</td>\n",
       "      <td>safe</td>\n",
       "      <td>analyze lending</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16659</th>\n",
       "      <td>340225</td>\n",
       "      <td>200909</td>\n",
       "      <td>going malaria</td>\n",
       "      <td>malaria foundation</td>\n",
       "      <td>donations</td>\n",
       "      <td>malaria</td>\n",
       "      <td>foundation</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17685</th>\n",
       "      <td>341581</td>\n",
       "      <td>189458</td>\n",
       "      <td>cutoff</td>\n",
       "      <td>biomarker</td>\n",
       "      <td>level biomarker</td>\n",
       "      <td>cutoff values</td>\n",
       "      <td>males females</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17963</th>\n",
       "      <td>341936</td>\n",
       "      <td>83694</td>\n",
       "      <td>specificity</td>\n",
       "      <td>appears sorts</td>\n",
       "      <td>attempting construct</td>\n",
       "      <td>average depending</td>\n",
       "      <td>class generalizes</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18346</th>\n",
       "      <td>342457</td>\n",
       "      <td>98627</td>\n",
       "      <td>npo</td>\n",
       "      <td>donations</td>\n",
       "      <td>grants</td>\n",
       "      <td>appoint</td>\n",
       "      <td>appoint auditor</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18496</th>\n",
       "      <td>342670</td>\n",
       "      <td>203934</td>\n",
       "      <td>accuracy sensitivity</td>\n",
       "      <td>sensitivity specificity</td>\n",
       "      <td>specificity</td>\n",
       "      <td>sensitivity</td>\n",
       "      <td>applied contingency</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19158</th>\n",
       "      <td>343491</td>\n",
       "      <td>16939</td>\n",
       "      <td>roll</td>\n",
       "      <td>probability roll</td>\n",
       "      <td>die times</td>\n",
       "      <td>roll sided</td>\n",
       "      <td>sided die</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19361</th>\n",
       "      <td>343760</td>\n",
       "      <td>195935</td>\n",
       "      <td>sensitivity specificity</td>\n",
       "      <td>specificity</td>\n",
       "      <td>sensitivity</td>\n",
       "      <td>actions undertaken</td>\n",
       "      <td>associated actions</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id  user_id                 TopWord1                 TopWord2  \\\n",
       "363     318158   175125                    risky                 labelled   \n",
       "528     318379   164061                     dice                     roll   \n",
       "1050    319047     8013                      cut              sensitivity   \n",
       "1577    319730   121522           existing model                 existing   \n",
       "2546    320994   142829                     wire         probability wire   \n",
       "2696    321181   190212          existing cohort                   cohort   \n",
       "4142    323039    23732                      vis             second order   \n",
       "4883    323970    60065                     corr                corr corr   \n",
       "5035    324172   115061                 hand cut                cut point   \n",
       "6764    326316    28740                      tpr                      fpr   \n",
       "8528    328550   133901               prevalence              sensitivity   \n",
       "8984    329150    78964              cut essence           essence matter   \n",
       "9322    329605     1352                  baskets                   cutoff   \n",
       "9829    330278   130206  sensitivity specificity                  mausner   \n",
       "9922    330409     7290                      alt              sensitivity   \n",
       "9993    330508   189008                     dice               dice faces   \n",
       "10071   330619   196757                  essence          bonus implement   \n",
       "10103   330666   114446              specificity              sensitivity   \n",
       "11271   332149   162303               sanitation                  malaria   \n",
       "11764   332806   198339                     dice               value dice   \n",
       "11789   332834   164061                     dice                    dices   \n",
       "12084   333227   198641                  bracket                     safe   \n",
       "12178   333353   198724                      die                     roll   \n",
       "12876   335260   200333            unstandarized      unstandarized value   \n",
       "13578   336178   201105      existing predictors                 existing   \n",
       "13911   336599    11887                     dice                    times   \n",
       "15531   338749     6479               prevalence     alternatives correct   \n",
       "16628   340187   154156                   cutoff                       di   \n",
       "16638   340198   158101              sensitivity                     loan   \n",
       "16659   340225   200909            going malaria       malaria foundation   \n",
       "17685   341581   189458                   cutoff                biomarker   \n",
       "17963   341936    83694              specificity            appears sorts   \n",
       "18346   342457    98627                      npo                donations   \n",
       "18496   342670   203934     accuracy sensitivity  sensitivity specificity   \n",
       "19158   343491    16939                     roll         probability roll   \n",
       "19361   343760   195935  sensitivity specificity              specificity   \n",
       "\n",
       "                      TopWord3              TopWord4                 TopWord5  \\\n",
       "363               deemed risky           risky risky            labelled data   \n",
       "528            limit summation            rolls dice                roll dice   \n",
       "1050                     harms           specificity  sensitivity specificity   \n",
       "1577                     risky           deems risky                    deems   \n",
       "2546                  wire cut           wire length                      cut   \n",
       "2696               power study           prospective                 existing   \n",
       "4142                    second           better fits            check complex   \n",
       "4883                    cutoff              add edge                add edges   \n",
       "5035                       cut        age understand        coefficient lower   \n",
       "6764   assessing probabilistic           specificity              sensitivity   \n",
       "8528               specificity       properties test              outpatients   \n",
       "8984                  like cut               essence                      cut   \n",
       "9322           baskets contain        subset baskets           contain cutoff   \n",
       "9829               specificity               ppv npv              sensitivity   \n",
       "9922                     liver           specificity               levels alt   \n",
       "9993             faces numbers              like clt             numbers dice   \n",
       "10071          capture essence       classical error    endogeneity classical   \n",
       "10103  sensitivity specificity            prevalence                       dc   \n",
       "11271               prevalence  community sanitation                      dhs   \n",
       "11764              dice having      dice probability             having value   \n",
       "11789                dice roll                  roll        probability sided   \n",
       "12084                    risky                  roll                     head   \n",
       "12178               second die   dice simultaneously                   second   \n",
       "12876               value rate                  loan                 research   \n",
       "13578       additivity instead      changes existing    coefficients existing   \n",
       "13911      answered completely     boxes formulation        comes multinomial   \n",
       "15531           binomial exact           cited exact     crossvalidated bunch   \n",
       "16628                      cut     parameters sample             specifically   \n",
       "16638                    risky                  safe          analyze lending   \n",
       "16659                donations               malaria               foundation   \n",
       "17685          level biomarker         cutoff values            males females   \n",
       "17963     attempting construct     average depending        class generalizes   \n",
       "18346                   grants               appoint          appoint auditor   \n",
       "18496              specificity           sensitivity      applied contingency   \n",
       "19158                die times            roll sided                sided die   \n",
       "19361              sensitivity    actions undertaken       associated actions   \n",
       "\n",
       "       Community  \n",
       "363           13  \n",
       "528           13  \n",
       "1050          13  \n",
       "1577          13  \n",
       "2546          13  \n",
       "2696          13  \n",
       "4142          13  \n",
       "4883          13  \n",
       "5035          13  \n",
       "6764          13  \n",
       "8528          13  \n",
       "8984          13  \n",
       "9322          13  \n",
       "9829          13  \n",
       "9922          13  \n",
       "9993          13  \n",
       "10071         13  \n",
       "10103         13  \n",
       "11271         13  \n",
       "11764         13  \n",
       "11789         13  \n",
       "12084         13  \n",
       "12178         13  \n",
       "12876         13  \n",
       "13578         13  \n",
       "13911         13  \n",
       "15531         13  \n",
       "16628         13  \n",
       "16638         13  \n",
       "16659         13  \n",
       "17685         13  \n",
       "17963         13  \n",
       "18346         13  \n",
       "18496         13  \n",
       "19158         13  \n",
       "19361         13  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df[post_df_w_comm[\"Community\"] == 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>TopWord1</th>\n",
       "      <th>TopWord2</th>\n",
       "      <th>TopWord3</th>\n",
       "      <th>TopWord4</th>\n",
       "      <th>TopWord5</th>\n",
       "      <th>Community</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317697</td>\n",
       "      <td>134975</td>\n",
       "      <td>variables statistically</td>\n",
       "      <td>mixed effect</td>\n",
       "      <td>mixed</td>\n",
       "      <td>variables</td>\n",
       "      <td>statistically significant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>317699</td>\n",
       "      <td>45374</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>validation accuracy</td>\n",
       "      <td>training accuracy</td>\n",
       "      <td>shallow</td>\n",
       "      <td>validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>317701</td>\n",
       "      <td>171235</td>\n",
       "      <td>freedom</td>\n",
       "      <td>mean</td>\n",
       "      <td>degrees freedom</td>\n",
       "      <td>degrees</td>\n",
       "      <td>data degree</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>317703</td>\n",
       "      <td>187797</td>\n",
       "      <td>patients</td>\n",
       "      <td>treating</td>\n",
       "      <td>excel</td>\n",
       "      <td>treat</td>\n",
       "      <td>bias treating</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317704</td>\n",
       "      <td>61496</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>error ways</td>\n",
       "      <td>estimate realistic</td>\n",
       "      <td>forecast method</td>\n",
       "      <td>predicts time</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>317705</td>\n",
       "      <td>134369</td>\n",
       "      <td>bonferroni correction</td>\n",
       "      <td>bonferroni</td>\n",
       "      <td>correction</td>\n",
       "      <td>outliers</td>\n",
       "      <td>don understand</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>317707</td>\n",
       "      <td>187792</td>\n",
       "      <td>environmental variables</td>\n",
       "      <td>traits</td>\n",
       "      <td>environmental</td>\n",
       "      <td>plant traits</td>\n",
       "      <td>plant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>317708</td>\n",
       "      <td>82816</td>\n",
       "      <td>solve differential</td>\n",
       "      <td>equation distribution</td>\n",
       "      <td>differential equation</td>\n",
       "      <td>need solve</td>\n",
       "      <td>distribution pdf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>317710</td>\n",
       "      <td>61092</td>\n",
       "      <td>monotonically increasing</td>\n",
       "      <td>monotonically</td>\n",
       "      <td>differentiable</td>\n",
       "      <td>increasing</td>\n",
       "      <td>assuming differentiable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>317711</td>\n",
       "      <td>179343</td>\n",
       "      <td>affect specified</td>\n",
       "      <td>shock affect</td>\n",
       "      <td>understand shock</td>\n",
       "      <td>prof</td>\n",
       "      <td>shock</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>317712</td>\n",
       "      <td>67137</td>\n",
       "      <td>simr</td>\n",
       "      <td>able edit</td>\n",
       "      <td>deciding sample</td>\n",
       "      <td>edit turns</td>\n",
       "      <td>effect variety</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>317713</td>\n",
       "      <td>67137</td>\n",
       "      <td>measures construct</td>\n",
       "      <td>power</td>\n",
       "      <td>measures</td>\n",
       "      <td>outcome measures</td>\n",
       "      <td>multivariate regression</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>317716</td>\n",
       "      <td>187803</td>\n",
       "      <td>terms applications</td>\n",
       "      <td>difference poisson</td>\n",
       "      <td>process terms</td>\n",
       "      <td>process markov</td>\n",
       "      <td>markov process</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>317717</td>\n",
       "      <td>113090</td>\n",
       "      <td>graphical</td>\n",
       "      <td>graphical models</td>\n",
       "      <td>log linear</td>\n",
       "      <td>models</td>\n",
       "      <td>log</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>317718</td>\n",
       "      <td>80922</td>\n",
       "      <td>recurrent</td>\n",
       "      <td>deep recurrent</td>\n",
       "      <td>neural network</td>\n",
       "      <td>recurrent neural</td>\n",
       "      <td>neural</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>317719</td>\n",
       "      <td>187804</td>\n",
       "      <td>person</td>\n",
       "      <td>young old</td>\n",
       "      <td>young</td>\n",
       "      <td>old</td>\n",
       "      <td>age indicator</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>317721</td>\n",
       "      <td>177228</td>\n",
       "      <td>intervention</td>\n",
       "      <td>male respondents</td>\n",
       "      <td>intervention time</td>\n",
       "      <td>respondents</td>\n",
       "      <td>male</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>317722</td>\n",
       "      <td>187806</td>\n",
       "      <td>fans</td>\n",
       "      <td>band</td>\n",
       "      <td>universe</td>\n",
       "      <td>overall universe</td>\n",
       "      <td>cities</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>317723</td>\n",
       "      <td>8123</td>\n",
       "      <td>distinguishes legged</td>\n",
       "      <td>legged</td>\n",
       "      <td>old style</td>\n",
       "      <td>seek opinions</td>\n",
       "      <td>recognition</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>317724</td>\n",
       "      <td>117168</td>\n",
       "      <td>block</td>\n",
       "      <td>resampling</td>\n",
       "      <td>series</td>\n",
       "      <td>blocks</td>\n",
       "      <td>resample</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>317725</td>\n",
       "      <td>185581</td>\n",
       "      <td>rows small</td>\n",
       "      <td>corresponding rows</td>\n",
       "      <td>constraint</td>\n",
       "      <td>matrix shape</td>\n",
       "      <td>row</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>317727</td>\n",
       "      <td>117168</td>\n",
       "      <td>block</td>\n",
       "      <td>blocks</td>\n",
       "      <td>block observation</td>\n",
       "      <td>resample</td>\n",
       "      <td>fourth block</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>317728</td>\n",
       "      <td>187809</td>\n",
       "      <td>subscribe</td>\n",
       "      <td>mining</td>\n",
       "      <td>brief ideally</td>\n",
       "      <td>customer subscribe</td>\n",
       "      <td>incorporate mining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>317729</td>\n",
       "      <td>31484</td>\n",
       "      <td>beta distribution</td>\n",
       "      <td>beta</td>\n",
       "      <td>distribution prove</td>\n",
       "      <td>look beta</td>\n",
       "      <td>prove beta</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>317730</td>\n",
       "      <td>111479</td>\n",
       "      <td>eigenvector</td>\n",
       "      <td>grey</td>\n",
       "      <td>components</td>\n",
       "      <td>eigenvalue</td>\n",
       "      <td>variables</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>317731</td>\n",
       "      <td>805</td>\n",
       "      <td>beta</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>symmetric beta</td>\n",
       "      <td>symmetric</td>\n",
       "      <td>beta gaussian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>317732</td>\n",
       "      <td>88727</td>\n",
       "      <td>signal</td>\n",
       "      <td>bound</td>\n",
       "      <td>bound saturated</td>\n",
       "      <td>channel variance</td>\n",
       "      <td>deriving relationships</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>317733</td>\n",
       "      <td>131977</td>\n",
       "      <td>don understand</td>\n",
       "      <td>architecture explaining</td>\n",
       "      <td>concatenate inputs</td>\n",
       "      <td>deep mind</td>\n",
       "      <td>explaining works</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>317734</td>\n",
       "      <td>187810</td>\n",
       "      <td>intervention</td>\n",
       "      <td>signal</td>\n",
       "      <td>events</td>\n",
       "      <td>external events</td>\n",
       "      <td>external</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>317735</td>\n",
       "      <td>8402</td>\n",
       "      <td>denote noncentral</td>\n",
       "      <td>derived theory</td>\n",
       "      <td>don proof</td>\n",
       "      <td>ingersoll ros</td>\n",
       "      <td>proof prove</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>344573</td>\n",
       "      <td>94586</td>\n",
       "      <td>tree species</td>\n",
       "      <td>species</td>\n",
       "      <td>tree</td>\n",
       "      <td>crown</td>\n",
       "      <td>lidar</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>344574</td>\n",
       "      <td>105149</td>\n",
       "      <td>college</td>\n",
       "      <td>college highschool</td>\n",
       "      <td>highschool</td>\n",
       "      <td>student answer</td>\n",
       "      <td>correctly question</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>344575</td>\n",
       "      <td>136275</td>\n",
       "      <td>mean</td>\n",
       "      <td>use mean</td>\n",
       "      <td>test set</td>\n",
       "      <td>test</td>\n",
       "      <td>mean test</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>344578</td>\n",
       "      <td>163648</td>\n",
       "      <td>number correct</td>\n",
       "      <td>correct answers</td>\n",
       "      <td>let number</td>\n",
       "      <td>high school</td>\n",
       "      <td>let let</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>344579</td>\n",
       "      <td>207112</td>\n",
       "      <td>equal equal</td>\n",
       "      <td>distribution variable</td>\n",
       "      <td>function parameters</td>\n",
       "      <td>interpretation</td>\n",
       "      <td>parameters</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>344580</td>\n",
       "      <td>195935</td>\n",
       "      <td>model</td>\n",
       "      <td>mean</td>\n",
       "      <td>application model</td>\n",
       "      <td>baseline renders</td>\n",
       "      <td>building important</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>344581</td>\n",
       "      <td>195935</td>\n",
       "      <td>pairwise comparisons</td>\n",
       "      <td>pairwise</td>\n",
       "      <td>comparisons</td>\n",
       "      <td>ability explicitly</td>\n",
       "      <td>addition pairwise</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>344583</td>\n",
       "      <td>195935</td>\n",
       "      <td>assume continous</td>\n",
       "      <td>binary models</td>\n",
       "      <td>combining binary</td>\n",
       "      <td>continous distribution</td>\n",
       "      <td>decisions assume</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>344585</td>\n",
       "      <td>207115</td>\n",
       "      <td>instrumental variables</td>\n",
       "      <td>instrumental</td>\n",
       "      <td>accounting political</td>\n",
       "      <td>author help</td>\n",
       "      <td>economics health</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>344586</td>\n",
       "      <td>206620</td>\n",
       "      <td>fonction</td>\n",
       "      <td>anova fonction</td>\n",
       "      <td>lm fonction</td>\n",
       "      <td>lm</td>\n",
       "      <td>avoid overpramatizing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>344587</td>\n",
       "      <td>207096</td>\n",
       "      <td>probability</td>\n",
       "      <td>probability density</td>\n",
       "      <td>density function</td>\n",
       "      <td>density</td>\n",
       "      <td>function</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>344588</td>\n",
       "      <td>198848</td>\n",
       "      <td>sufficient estimator</td>\n",
       "      <td>bernoulli</td>\n",
       "      <td>binomial distributions</td>\n",
       "      <td>sufficient</td>\n",
       "      <td>textbook</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>344589</td>\n",
       "      <td>206947</td>\n",
       "      <td>spss</td>\n",
       "      <td>come instruction</td>\n",
       "      <td>downloaded plugin</td>\n",
       "      <td>fully motivated</td>\n",
       "      <td>instruction spss</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>344592</td>\n",
       "      <td>96600</td>\n",
       "      <td>area posterior</td>\n",
       "      <td>axis reasonable</td>\n",
       "      <td>direction observed</td>\n",
       "      <td>does unable</td>\n",
       "      <td>effect opposite</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>344593</td>\n",
       "      <td>206570</td>\n",
       "      <td>gain lift</td>\n",
       "      <td>lift used</td>\n",
       "      <td>metrics auc</td>\n",
       "      <td>score gain</td>\n",
       "      <td>different evaluation</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>344594</td>\n",
       "      <td>207118</td>\n",
       "      <td>neurons</td>\n",
       "      <td>basis</td>\n",
       "      <td>inputs</td>\n",
       "      <td>correlations</td>\n",
       "      <td>basing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>344595</td>\n",
       "      <td>230</td>\n",
       "      <td>example unsupervised</td>\n",
       "      <td>unsupervised learning</td>\n",
       "      <td>simple example</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>learning</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>344596</td>\n",
       "      <td>207021</td>\n",
       "      <td>plant</td>\n",
       "      <td>host</td>\n",
       "      <td>dodder</td>\n",
       "      <td>host plant</td>\n",
       "      <td>dodder plant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>344597</td>\n",
       "      <td>207096</td>\n",
       "      <td>estimating</td>\n",
       "      <td>clustering meta</td>\n",
       "      <td>clusters estimating</td>\n",
       "      <td>density estimating</td>\n",
       "      <td>estimating density</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>344598</td>\n",
       "      <td>207058</td>\n",
       "      <td>series</td>\n",
       "      <td>paper</td>\n",
       "      <td>time series</td>\n",
       "      <td>time</td>\n",
       "      <td>forecasting</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>344599</td>\n",
       "      <td>207096</td>\n",
       "      <td>spaces</td>\n",
       "      <td>probability measures</td>\n",
       "      <td>functions probability</td>\n",
       "      <td>finite dimensional</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>344600</td>\n",
       "      <td>163114</td>\n",
       "      <td>dickey fuller</td>\n",
       "      <td>dickey</td>\n",
       "      <td>fuller</td>\n",
       "      <td>fuller test</td>\n",
       "      <td>test stata</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>344601</td>\n",
       "      <td>207099</td>\n",
       "      <td>possible wilcox</td>\n",
       "      <td>check sample</td>\n",
       "      <td>different possible</td>\n",
       "      <td>wilcox test</td>\n",
       "      <td>sample normal</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>344602</td>\n",
       "      <td>121522</td>\n",
       "      <td>logistic</td>\n",
       "      <td>logistic tag</td>\n",
       "      <td>number relevant</td>\n",
       "      <td>relevant questions</td>\n",
       "      <td>tag number</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>344604</td>\n",
       "      <td>161278</td>\n",
       "      <td>effects limited</td>\n",
       "      <td>existing mean</td>\n",
       "      <td>matrix existing</td>\n",
       "      <td>permanent effect</td>\n",
       "      <td>matter variable</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>344605</td>\n",
       "      <td>207126</td>\n",
       "      <td>pasta</td>\n",
       "      <td>anova tukeys</td>\n",
       "      <td>apologies incorrectly</td>\n",
       "      <td>batches pasta</td>\n",
       "      <td>cooking anova</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>344607</td>\n",
       "      <td>121270</td>\n",
       "      <td>rbms</td>\n",
       "      <td>networks</td>\n",
       "      <td>hopfield nets</td>\n",
       "      <td>hopfield</td>\n",
       "      <td>hinton</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>344608</td>\n",
       "      <td>191681</td>\n",
       "      <td>hessian</td>\n",
       "      <td>approximate hessian</td>\n",
       "      <td>dealing hessian</td>\n",
       "      <td>trying approximate</td>\n",
       "      <td>bfgs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>344609</td>\n",
       "      <td>127538</td>\n",
       "      <td>spikes</td>\n",
       "      <td>tsclean</td>\n",
       "      <td>winsorization</td>\n",
       "      <td>dummy variables</td>\n",
       "      <td>explainable</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>344610</td>\n",
       "      <td>207128</td>\n",
       "      <td>normality</td>\n",
       "      <td>dismiss assumption</td>\n",
       "      <td>know violate</td>\n",
       "      <td>know wilcox</td>\n",
       "      <td>lt dismiss</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19725 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id  user_id                  TopWord1                 TopWord2  \\\n",
       "0       317697   134975   variables statistically             mixed effect   \n",
       "1       317699    45374                  accuracy      validation accuracy   \n",
       "2       317701   171235                   freedom                     mean   \n",
       "3       317703   187797                  patients                 treating   \n",
       "4       317704    61496               uncertainty               error ways   \n",
       "5       317705   134369     bonferroni correction               bonferroni   \n",
       "6       317707   187792   environmental variables                   traits   \n",
       "7       317708    82816        solve differential    equation distribution   \n",
       "8       317710    61092  monotonically increasing            monotonically   \n",
       "9       317711   179343          affect specified             shock affect   \n",
       "10      317712    67137                      simr                able edit   \n",
       "11      317713    67137        measures construct                    power   \n",
       "12      317716   187803        terms applications       difference poisson   \n",
       "13      317717   113090                 graphical         graphical models   \n",
       "14      317718    80922                 recurrent           deep recurrent   \n",
       "15      317719   187804                    person                young old   \n",
       "16      317721   177228              intervention         male respondents   \n",
       "17      317722   187806                      fans                     band   \n",
       "18      317723     8123      distinguishes legged                   legged   \n",
       "19      317724   117168                     block               resampling   \n",
       "20      317725   185581                rows small       corresponding rows   \n",
       "21      317727   117168                     block                   blocks   \n",
       "22      317728   187809                 subscribe                   mining   \n",
       "23      317729    31484         beta distribution                     beta   \n",
       "24      317730   111479               eigenvector                     grey   \n",
       "25      317731      805                      beta                 gaussian   \n",
       "26      317732    88727                    signal                    bound   \n",
       "27      317733   131977            don understand  architecture explaining   \n",
       "28      317734   187810              intervention                   signal   \n",
       "29      317735     8402         denote noncentral           derived theory   \n",
       "...        ...      ...                       ...                      ...   \n",
       "19970   344573    94586              tree species                  species   \n",
       "19971   344574   105149                   college       college highschool   \n",
       "19972   344575   136275                      mean                 use mean   \n",
       "19973   344578   163648            number correct          correct answers   \n",
       "19974   344579   207112               equal equal    distribution variable   \n",
       "19975   344580   195935                     model                     mean   \n",
       "19976   344581   195935      pairwise comparisons                 pairwise   \n",
       "19977   344583   195935          assume continous            binary models   \n",
       "19978   344585   207115    instrumental variables             instrumental   \n",
       "19979   344586   206620                  fonction           anova fonction   \n",
       "19980   344587   207096               probability      probability density   \n",
       "19981   344588   198848      sufficient estimator                bernoulli   \n",
       "19982   344589   206947                      spss         come instruction   \n",
       "19983   344592    96600            area posterior          axis reasonable   \n",
       "19984   344593   206570                 gain lift                lift used   \n",
       "19985   344594   207118                   neurons                    basis   \n",
       "19986   344595      230      example unsupervised    unsupervised learning   \n",
       "19987   344596   207021                     plant                     host   \n",
       "19988   344597   207096                estimating          clustering meta   \n",
       "19989   344598   207058                    series                    paper   \n",
       "19990   344599   207096                    spaces     probability measures   \n",
       "19991   344600   163114             dickey fuller                   dickey   \n",
       "19992   344601   207099           possible wilcox             check sample   \n",
       "19993   344602   121522                  logistic             logistic tag   \n",
       "19994   344604   161278           effects limited            existing mean   \n",
       "19995   344605   207126                     pasta             anova tukeys   \n",
       "19996   344607   121270                      rbms                 networks   \n",
       "19997   344608   191681                   hessian      approximate hessian   \n",
       "19998   344609   127538                    spikes                  tsclean   \n",
       "19999   344610   207128                 normality       dismiss assumption   \n",
       "\n",
       "                     TopWord3                TopWord4  \\\n",
       "0                       mixed               variables   \n",
       "1           training accuracy                 shallow   \n",
       "2             degrees freedom                 degrees   \n",
       "3                       excel                   treat   \n",
       "4          estimate realistic         forecast method   \n",
       "5                  correction                outliers   \n",
       "6               environmental            plant traits   \n",
       "7       differential equation              need solve   \n",
       "8              differentiable              increasing   \n",
       "9            understand shock                    prof   \n",
       "10            deciding sample              edit turns   \n",
       "11                   measures        outcome measures   \n",
       "12              process terms          process markov   \n",
       "13                 log linear                  models   \n",
       "14             neural network        recurrent neural   \n",
       "15                      young                     old   \n",
       "16          intervention time             respondents   \n",
       "17                   universe        overall universe   \n",
       "18                  old style           seek opinions   \n",
       "19                     series                  blocks   \n",
       "20                 constraint            matrix shape   \n",
       "21          block observation                resample   \n",
       "22              brief ideally      customer subscribe   \n",
       "23         distribution prove               look beta   \n",
       "24                 components              eigenvalue   \n",
       "25             symmetric beta               symmetric   \n",
       "26            bound saturated        channel variance   \n",
       "27         concatenate inputs               deep mind   \n",
       "28                     events         external events   \n",
       "29                  don proof           ingersoll ros   \n",
       "...                       ...                     ...   \n",
       "19970                    tree                   crown   \n",
       "19971              highschool          student answer   \n",
       "19972                test set                    test   \n",
       "19973              let number             high school   \n",
       "19974     function parameters          interpretation   \n",
       "19975       application model        baseline renders   \n",
       "19976             comparisons      ability explicitly   \n",
       "19977        combining binary  continous distribution   \n",
       "19978    accounting political             author help   \n",
       "19979             lm fonction                      lm   \n",
       "19980        density function                 density   \n",
       "19981  binomial distributions              sufficient   \n",
       "19982       downloaded plugin         fully motivated   \n",
       "19983      direction observed             does unable   \n",
       "19984             metrics auc              score gain   \n",
       "19985                  inputs            correlations   \n",
       "19986          simple example            unsupervised   \n",
       "19987                  dodder              host plant   \n",
       "19988     clusters estimating      density estimating   \n",
       "19989             time series                    time   \n",
       "19990   functions probability      finite dimensional   \n",
       "19991                  fuller             fuller test   \n",
       "19992      different possible             wilcox test   \n",
       "19993         number relevant      relevant questions   \n",
       "19994         matrix existing        permanent effect   \n",
       "19995   apologies incorrectly           batches pasta   \n",
       "19996           hopfield nets                hopfield   \n",
       "19997         dealing hessian      trying approximate   \n",
       "19998           winsorization         dummy variables   \n",
       "19999            know violate             know wilcox   \n",
       "\n",
       "                        TopWord5  Community  \n",
       "0      statistically significant          1  \n",
       "1                     validation          3  \n",
       "2                    data degree          0  \n",
       "3                  bias treating          4  \n",
       "4                  predicts time          8  \n",
       "5                 don understand          4  \n",
       "6                          plant          1  \n",
       "7               distribution pdf          0  \n",
       "8        assuming differentiable          0  \n",
       "9                          shock          4  \n",
       "10                effect variety         -1  \n",
       "11       multivariate regression          8  \n",
       "12                markov process          3  \n",
       "13                           log          0  \n",
       "14                        neural          3  \n",
       "15                 age indicator          4  \n",
       "16                          male          4  \n",
       "17                        cities          4  \n",
       "18                   recognition          3  \n",
       "19                      resample          4  \n",
       "20                           row          3  \n",
       "21                  fourth block          4  \n",
       "22            incorporate mining          3  \n",
       "23                    prove beta          0  \n",
       "24                     variables          1  \n",
       "25                 beta gaussian          0  \n",
       "26        deriving relationships          4  \n",
       "27              explaining works          0  \n",
       "28                      external          4  \n",
       "29                   proof prove         -1  \n",
       "...                          ...        ...  \n",
       "19970                      lidar          3  \n",
       "19971         correctly question          4  \n",
       "19972                  mean test          3  \n",
       "19973                    let let         25  \n",
       "19974                 parameters          0  \n",
       "19975         building important          0  \n",
       "19976          addition pairwise          4  \n",
       "19977           decisions assume         -1  \n",
       "19978           economics health          0  \n",
       "19979      avoid overpramatizing          1  \n",
       "19980                   function          0  \n",
       "19981                   textbook          0  \n",
       "19982           instruction spss          8  \n",
       "19983            effect opposite         -1  \n",
       "19984       different evaluation         -1  \n",
       "19985                     basing          3  \n",
       "19986                   learning          9  \n",
       "19987               dodder plant          1  \n",
       "19988         estimating density          3  \n",
       "19989                forecasting          4  \n",
       "19990                   discrete          1  \n",
       "19991                 test stata         26  \n",
       "19992              sample normal         27  \n",
       "19993                 tag number          1  \n",
       "19994            matter variable         -1  \n",
       "19995              cooking anova         -1  \n",
       "19996                     hinton          3  \n",
       "19997                       bfgs          0  \n",
       "19998                explainable          4  \n",
       "19999                 lt dismiss          4  \n",
       "\n",
       "[19725 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap.SaveEdgeList(post_graph, EDGELIST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency in communities\n",
    "word_counts = collections.defaultdict(dict)\n",
    "word_freqs = collections.defaultdict(dict)\n",
    "for comm in communities:\n",
    "    total_words = 0.0\n",
    "    word_counts[comm] = collections.defaultdict(int)\n",
    "    for word in post_df_w_comm[post_df_w_comm[\"Community\"] == comm]['TopWord1']:\n",
    "        word_counts[comm][word] += 1\n",
    "        total_words += 1\n",
    "    for word in post_df_w_comm[post_df_w_comm[\"Community\"] == comm]['TopWord2']:\n",
    "        word_counts[comm][word] += 1\n",
    "        total_words += 1\n",
    "    for word in post_df_w_comm[post_df_w_comm[\"Community\"] == comm]['TopWord3']:\n",
    "        word_counts[comm][word] += 1\n",
    "        total_words += 1\n",
    "    for word in post_df_w_comm[post_df_w_comm[\"Community\"] == comm]['TopWord4']:\n",
    "        word_counts[comm][word] += 1\n",
    "        total_words += 1\n",
    "    for word in post_df_w_comm[post_df_w_comm[\"Community\"] == comm]['TopWord5']:\n",
    "        word_counts[comm][word] += 1\n",
    "        total_words += 1\n",
    "    for word in word_counts[comm]:\n",
    "        word_freqs[comm][word] = word_counts[comm][word] / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community:   0  Size:  4030  Label: distribution|sample|probability\n",
      "Community:   1  Size:  2303  Label: matrix|model|variables\n",
      "Community:   2  Size:    24  Label: pattern|sklearn|truck\n",
      "Community:   3  Size:  4711  Label: test|training|validation\n",
      "Community:   4  Size:  5343  Label: series|time|group\n",
      "Community:   5  Size:    97  Label: outliers|bias|percentile\n",
      "Community:   6  Size:     4  Label: et|al|et al\n",
      "Community:   7  Size:     5  Label: nest|materials|common materials\n",
      "Community:   8  Size:   159  Label: size|power|uncertainty\n",
      "Community:   9  Size:   127  Label: learning|rate|minutes\n",
      "Community:  10  Size:     2  Label: rmsep|lowest rmsep|command validation\n",
      "Community:  11  Size:     4  Label: cars|amounts cars|meet conditions\n",
      "Community:  12  Size:     9  Label: count data|count|data time\n",
      "Community:  13  Size:    36  Label: sensitivity|specificity|cut\n",
      "Community:  14  Size:     4  Label: hypertension|calcium|assay\n",
      "Community:  15  Size:     9  Label: concentration|flow|water\n",
      "Community:  16  Size:     2  Label: map2|change map2|argument change\n",
      "Community:  17  Size:    46  Label: inequality|code|money\n",
      "Community:  18  Size:     3  Label: payment|booking|booking cancelled\n",
      "Community:  19  Size:     8  Label: populations|performing|phenotype\n",
      "Community:  20  Size:     2  Label: attention|attention based|using attention\n",
      "Community:  21  Size:     7  Label: result|good|flows\n",
      "Community:  22  Size:    55  Label: lt|gt|gt gt\n",
      "Community:  23  Size:     4  Label: metropolis|rcpp|loops\n",
      "Community:  24  Size:     4  Label: birds|work|distances\n",
      "Community:  25  Size:     3  Label: let let|correct answers|let number\n",
      "Community:  26  Size:     4  Label: dickey fuller|fuller|dickey\n",
      "Community:  27  Size:     5  Label: wilcox|wilcox test|sums version\n",
      "Community:  28  Size:    10  Label: near|plm|popularity\n",
      "Community:  29  Size:    17  Label: table|holt winters|winters\n",
      "Community:  30  Size:     4  Label: beta3|beta2|decaying\n",
      "Community:  31  Size:     8  Label: dynamic|time warping|estimate model\n",
      "Community:  32  Size:    12  Label: question|invertible|dgp\n",
      "Community:  33  Size:     2  Label: advice|functions image|variance group\n",
      "Community:  34  Size:     2  Label: structure coefficients|corville|additionally sure\n",
      "Community:  35  Size:     2  Label: geo|thrown getting|coin thrown\n",
      "Community:  36  Size:     4  Label: orange|vitamin|linux\n",
      "Community:  37  Size:     4  Label: containing|case txs|answers advance\n",
      "Community:  38  Size:     2  Label: donors|lapsed|frequency donations\n",
      "Community:  39  Size:     5  Label: https github|github com|com\n",
      "Community:  40  Size:     2  Label: distributed random|having exponential|convergeges\n",
      "Community:  41  Size:     2  Label: armax|actually end|armax model\n",
      "Community:  42  Size:     3  Label: names|applicants|adjusted icc\n",
      "Community:  43  Size:     2  Label: stuck|suppose hypotheses|able stuck\n",
      "Community:  44  Size:     2  Label: tso|lets nn1|tso function\n",
      "Community:  45  Size:     2  Label: covariance variable|equivalent variance|equals variance\n",
      "Community:  46  Size:     2  Label: independent measures|anova scores|violating\n",
      "Community:  47  Size:     2  Label: factor1|factor2|according factor2\n",
      "Community:  48  Size:     5  Label: fdr|produced|scale inference\n",
      "Community:  49  Size:     2  Label: explain does|estimation come|does maximum\n",
      "Community:  50  Size:     2  Label: tests sampling|assumptions large|assumptions amp\n",
      "Community:  51  Size:     2  Label: samples groups|error anova|groups similarities\n",
      "Community:  52  Size:     2  Label: consultation|care treatment|bothering consult\n",
      "Community:  53  Size:     2  Label: km curve|mean km|correct job\n",
      "Community:  54  Size:     2  Label: berry esseen|berry|esseen\n",
      "Community:  55  Size:     2  Label: convolution quite|quite nicely|nicely different\n",
      "Community:  56  Size:     2  Label: ed|distributions ed|budgetary\n",
      "Community:  57  Size:     2  Label: wondering value|know place|place firstly\n",
      "Community:  58  Size:     2  Label: using boruta|meaning interpret|correctly feature\n",
      "Community:  59  Size:     2  Label: random component|component actually|complex stochastic\n",
      "Community:  60  Size:     2  Label: group comparison|trt2|comparison procedure\n",
      "Community:  61  Size:     3  Label: quotes|query|sql server\n",
      "Community:  62  Size:     2  Label: coefficient simple|work difference|coefficients refer\n",
      "Community:  63  Size:     2  Label: grain|mineral|assume grain\n",
      "Community:  64  Size:     2  Label: filtering|filtering step|arrays\n",
      "Community:  65  Size:     2  Label: unclear vague|scope use|tags instead\n",
      "Community:  -1  Size:  2586  Label: formula relation|account everybody|leverage observations\n"
     ]
    }
   ],
   "source": [
    "sorted_word_freqs = collections.defaultdict(dict)\n",
    "for comm in word_freqs:\n",
    "    sorted_word_freqs[comm] = sorted(word_freqs[comm].items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "community_labels = collections.defaultdict(str) \n",
    "\n",
    "for ind, comm in enumerate(sorted_word_freqs):\n",
    "    community_labels[comm] = \"{}|{}|{}\".format(sorted_word_freqs[comm][0][0], sorted_word_freqs[comm][1][0], sorted_word_freqs[comm][2][0])\n",
    "\n",
    "for comm in communities:\n",
    "    print \"Community: {:3}  Size: {:5}  Label: {}\".format(comm, len(post_df[post_df_w_comm[\"Community\"] == comm]), community_labels[comm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>TopWord1</th>\n",
       "      <th>TopWord2</th>\n",
       "      <th>TopWord3</th>\n",
       "      <th>TopWord4</th>\n",
       "      <th>TopWord5</th>\n",
       "      <th>Community</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317697</td>\n",
       "      <td>134975</td>\n",
       "      <td>variables statistically</td>\n",
       "      <td>mixed effect</td>\n",
       "      <td>mixed</td>\n",
       "      <td>variables</td>\n",
       "      <td>statistically significant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>317699</td>\n",
       "      <td>45374</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>validation accuracy</td>\n",
       "      <td>training accuracy</td>\n",
       "      <td>shallow</td>\n",
       "      <td>validation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>317701</td>\n",
       "      <td>171235</td>\n",
       "      <td>freedom</td>\n",
       "      <td>mean</td>\n",
       "      <td>degrees freedom</td>\n",
       "      <td>degrees</td>\n",
       "      <td>data degree</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>317703</td>\n",
       "      <td>187797</td>\n",
       "      <td>patients</td>\n",
       "      <td>treating</td>\n",
       "      <td>excel</td>\n",
       "      <td>treat</td>\n",
       "      <td>bias treating</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317704</td>\n",
       "      <td>61496</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>error ways</td>\n",
       "      <td>estimate realistic</td>\n",
       "      <td>forecast method</td>\n",
       "      <td>predicts time</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>317705</td>\n",
       "      <td>134369</td>\n",
       "      <td>bonferroni correction</td>\n",
       "      <td>bonferroni</td>\n",
       "      <td>correction</td>\n",
       "      <td>outliers</td>\n",
       "      <td>don understand</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>317707</td>\n",
       "      <td>187792</td>\n",
       "      <td>environmental variables</td>\n",
       "      <td>traits</td>\n",
       "      <td>environmental</td>\n",
       "      <td>plant traits</td>\n",
       "      <td>plant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>317708</td>\n",
       "      <td>82816</td>\n",
       "      <td>solve differential</td>\n",
       "      <td>equation distribution</td>\n",
       "      <td>differential equation</td>\n",
       "      <td>need solve</td>\n",
       "      <td>distribution pdf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>317710</td>\n",
       "      <td>61092</td>\n",
       "      <td>monotonically increasing</td>\n",
       "      <td>monotonically</td>\n",
       "      <td>differentiable</td>\n",
       "      <td>increasing</td>\n",
       "      <td>assuming differentiable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>317711</td>\n",
       "      <td>179343</td>\n",
       "      <td>affect specified</td>\n",
       "      <td>shock affect</td>\n",
       "      <td>understand shock</td>\n",
       "      <td>prof</td>\n",
       "      <td>shock</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>317712</td>\n",
       "      <td>67137</td>\n",
       "      <td>simr</td>\n",
       "      <td>able edit</td>\n",
       "      <td>deciding sample</td>\n",
       "      <td>edit turns</td>\n",
       "      <td>effect variety</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>317713</td>\n",
       "      <td>67137</td>\n",
       "      <td>measures construct</td>\n",
       "      <td>power</td>\n",
       "      <td>measures</td>\n",
       "      <td>outcome measures</td>\n",
       "      <td>multivariate regression</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>317716</td>\n",
       "      <td>187803</td>\n",
       "      <td>terms applications</td>\n",
       "      <td>difference poisson</td>\n",
       "      <td>process terms</td>\n",
       "      <td>process markov</td>\n",
       "      <td>markov process</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>317717</td>\n",
       "      <td>113090</td>\n",
       "      <td>graphical</td>\n",
       "      <td>graphical models</td>\n",
       "      <td>log linear</td>\n",
       "      <td>models</td>\n",
       "      <td>log</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>317718</td>\n",
       "      <td>80922</td>\n",
       "      <td>recurrent</td>\n",
       "      <td>deep recurrent</td>\n",
       "      <td>neural network</td>\n",
       "      <td>recurrent neural</td>\n",
       "      <td>neural</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>317719</td>\n",
       "      <td>187804</td>\n",
       "      <td>person</td>\n",
       "      <td>young old</td>\n",
       "      <td>young</td>\n",
       "      <td>old</td>\n",
       "      <td>age indicator</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>317721</td>\n",
       "      <td>177228</td>\n",
       "      <td>intervention</td>\n",
       "      <td>male respondents</td>\n",
       "      <td>intervention time</td>\n",
       "      <td>respondents</td>\n",
       "      <td>male</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>317722</td>\n",
       "      <td>187806</td>\n",
       "      <td>fans</td>\n",
       "      <td>band</td>\n",
       "      <td>universe</td>\n",
       "      <td>overall universe</td>\n",
       "      <td>cities</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>317723</td>\n",
       "      <td>8123</td>\n",
       "      <td>distinguishes legged</td>\n",
       "      <td>legged</td>\n",
       "      <td>old style</td>\n",
       "      <td>seek opinions</td>\n",
       "      <td>recognition</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>317724</td>\n",
       "      <td>117168</td>\n",
       "      <td>block</td>\n",
       "      <td>resampling</td>\n",
       "      <td>series</td>\n",
       "      <td>blocks</td>\n",
       "      <td>resample</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>317725</td>\n",
       "      <td>185581</td>\n",
       "      <td>rows small</td>\n",
       "      <td>corresponding rows</td>\n",
       "      <td>constraint</td>\n",
       "      <td>matrix shape</td>\n",
       "      <td>row</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>317727</td>\n",
       "      <td>117168</td>\n",
       "      <td>block</td>\n",
       "      <td>blocks</td>\n",
       "      <td>block observation</td>\n",
       "      <td>resample</td>\n",
       "      <td>fourth block</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>317728</td>\n",
       "      <td>187809</td>\n",
       "      <td>subscribe</td>\n",
       "      <td>mining</td>\n",
       "      <td>brief ideally</td>\n",
       "      <td>customer subscribe</td>\n",
       "      <td>incorporate mining</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>317729</td>\n",
       "      <td>31484</td>\n",
       "      <td>beta distribution</td>\n",
       "      <td>beta</td>\n",
       "      <td>distribution prove</td>\n",
       "      <td>look beta</td>\n",
       "      <td>prove beta</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>317730</td>\n",
       "      <td>111479</td>\n",
       "      <td>eigenvector</td>\n",
       "      <td>grey</td>\n",
       "      <td>components</td>\n",
       "      <td>eigenvalue</td>\n",
       "      <td>variables</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>317731</td>\n",
       "      <td>805</td>\n",
       "      <td>beta</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>symmetric beta</td>\n",
       "      <td>symmetric</td>\n",
       "      <td>beta gaussian</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>317732</td>\n",
       "      <td>88727</td>\n",
       "      <td>signal</td>\n",
       "      <td>bound</td>\n",
       "      <td>bound saturated</td>\n",
       "      <td>channel variance</td>\n",
       "      <td>deriving relationships</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>317733</td>\n",
       "      <td>131977</td>\n",
       "      <td>don understand</td>\n",
       "      <td>architecture explaining</td>\n",
       "      <td>concatenate inputs</td>\n",
       "      <td>deep mind</td>\n",
       "      <td>explaining works</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>317734</td>\n",
       "      <td>187810</td>\n",
       "      <td>intervention</td>\n",
       "      <td>signal</td>\n",
       "      <td>events</td>\n",
       "      <td>external events</td>\n",
       "      <td>external</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>317735</td>\n",
       "      <td>8402</td>\n",
       "      <td>denote noncentral</td>\n",
       "      <td>derived theory</td>\n",
       "      <td>don proof</td>\n",
       "      <td>ingersoll ros</td>\n",
       "      <td>proof prove</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>344573</td>\n",
       "      <td>94586</td>\n",
       "      <td>tree species</td>\n",
       "      <td>species</td>\n",
       "      <td>tree</td>\n",
       "      <td>crown</td>\n",
       "      <td>lidar</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>344574</td>\n",
       "      <td>105149</td>\n",
       "      <td>college</td>\n",
       "      <td>college highschool</td>\n",
       "      <td>highschool</td>\n",
       "      <td>student answer</td>\n",
       "      <td>correctly question</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>344575</td>\n",
       "      <td>136275</td>\n",
       "      <td>mean</td>\n",
       "      <td>use mean</td>\n",
       "      <td>test set</td>\n",
       "      <td>test</td>\n",
       "      <td>mean test</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>344578</td>\n",
       "      <td>163648</td>\n",
       "      <td>number correct</td>\n",
       "      <td>correct answers</td>\n",
       "      <td>let number</td>\n",
       "      <td>high school</td>\n",
       "      <td>let let</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>344579</td>\n",
       "      <td>207112</td>\n",
       "      <td>equal equal</td>\n",
       "      <td>distribution variable</td>\n",
       "      <td>function parameters</td>\n",
       "      <td>interpretation</td>\n",
       "      <td>parameters</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>344580</td>\n",
       "      <td>195935</td>\n",
       "      <td>model</td>\n",
       "      <td>mean</td>\n",
       "      <td>application model</td>\n",
       "      <td>baseline renders</td>\n",
       "      <td>building important</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>344581</td>\n",
       "      <td>195935</td>\n",
       "      <td>pairwise comparisons</td>\n",
       "      <td>pairwise</td>\n",
       "      <td>comparisons</td>\n",
       "      <td>ability explicitly</td>\n",
       "      <td>addition pairwise</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>344583</td>\n",
       "      <td>195935</td>\n",
       "      <td>assume continous</td>\n",
       "      <td>binary models</td>\n",
       "      <td>combining binary</td>\n",
       "      <td>continous distribution</td>\n",
       "      <td>decisions assume</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>344585</td>\n",
       "      <td>207115</td>\n",
       "      <td>instrumental variables</td>\n",
       "      <td>instrumental</td>\n",
       "      <td>accounting political</td>\n",
       "      <td>author help</td>\n",
       "      <td>economics health</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>344586</td>\n",
       "      <td>206620</td>\n",
       "      <td>fonction</td>\n",
       "      <td>anova fonction</td>\n",
       "      <td>lm fonction</td>\n",
       "      <td>lm</td>\n",
       "      <td>avoid overpramatizing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>344587</td>\n",
       "      <td>207096</td>\n",
       "      <td>probability</td>\n",
       "      <td>probability density</td>\n",
       "      <td>density function</td>\n",
       "      <td>density</td>\n",
       "      <td>function</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>344588</td>\n",
       "      <td>198848</td>\n",
       "      <td>sufficient estimator</td>\n",
       "      <td>bernoulli</td>\n",
       "      <td>binomial distributions</td>\n",
       "      <td>sufficient</td>\n",
       "      <td>textbook</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>344589</td>\n",
       "      <td>206947</td>\n",
       "      <td>spss</td>\n",
       "      <td>come instruction</td>\n",
       "      <td>downloaded plugin</td>\n",
       "      <td>fully motivated</td>\n",
       "      <td>instruction spss</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>344592</td>\n",
       "      <td>96600</td>\n",
       "      <td>area posterior</td>\n",
       "      <td>axis reasonable</td>\n",
       "      <td>direction observed</td>\n",
       "      <td>does unable</td>\n",
       "      <td>effect opposite</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>344593</td>\n",
       "      <td>206570</td>\n",
       "      <td>gain lift</td>\n",
       "      <td>lift used</td>\n",
       "      <td>metrics auc</td>\n",
       "      <td>score gain</td>\n",
       "      <td>different evaluation</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>344594</td>\n",
       "      <td>207118</td>\n",
       "      <td>neurons</td>\n",
       "      <td>basis</td>\n",
       "      <td>inputs</td>\n",
       "      <td>correlations</td>\n",
       "      <td>basing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>344595</td>\n",
       "      <td>230</td>\n",
       "      <td>example unsupervised</td>\n",
       "      <td>unsupervised learning</td>\n",
       "      <td>simple example</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>learning</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>344596</td>\n",
       "      <td>207021</td>\n",
       "      <td>plant</td>\n",
       "      <td>host</td>\n",
       "      <td>dodder</td>\n",
       "      <td>host plant</td>\n",
       "      <td>dodder plant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>344597</td>\n",
       "      <td>207096</td>\n",
       "      <td>estimating</td>\n",
       "      <td>clustering meta</td>\n",
       "      <td>clusters estimating</td>\n",
       "      <td>density estimating</td>\n",
       "      <td>estimating density</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>344598</td>\n",
       "      <td>207058</td>\n",
       "      <td>series</td>\n",
       "      <td>paper</td>\n",
       "      <td>time series</td>\n",
       "      <td>time</td>\n",
       "      <td>forecasting</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>344599</td>\n",
       "      <td>207096</td>\n",
       "      <td>spaces</td>\n",
       "      <td>probability measures</td>\n",
       "      <td>functions probability</td>\n",
       "      <td>finite dimensional</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>344600</td>\n",
       "      <td>163114</td>\n",
       "      <td>dickey fuller</td>\n",
       "      <td>dickey</td>\n",
       "      <td>fuller</td>\n",
       "      <td>fuller test</td>\n",
       "      <td>test stata</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>344601</td>\n",
       "      <td>207099</td>\n",
       "      <td>possible wilcox</td>\n",
       "      <td>check sample</td>\n",
       "      <td>different possible</td>\n",
       "      <td>wilcox test</td>\n",
       "      <td>sample normal</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>344602</td>\n",
       "      <td>121522</td>\n",
       "      <td>logistic</td>\n",
       "      <td>logistic tag</td>\n",
       "      <td>number relevant</td>\n",
       "      <td>relevant questions</td>\n",
       "      <td>tag number</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>344604</td>\n",
       "      <td>161278</td>\n",
       "      <td>effects limited</td>\n",
       "      <td>existing mean</td>\n",
       "      <td>matrix existing</td>\n",
       "      <td>permanent effect</td>\n",
       "      <td>matter variable</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>344605</td>\n",
       "      <td>207126</td>\n",
       "      <td>pasta</td>\n",
       "      <td>anova tukeys</td>\n",
       "      <td>apologies incorrectly</td>\n",
       "      <td>batches pasta</td>\n",
       "      <td>cooking anova</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>344607</td>\n",
       "      <td>121270</td>\n",
       "      <td>rbms</td>\n",
       "      <td>networks</td>\n",
       "      <td>hopfield nets</td>\n",
       "      <td>hopfield</td>\n",
       "      <td>hinton</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>344608</td>\n",
       "      <td>191681</td>\n",
       "      <td>hessian</td>\n",
       "      <td>approximate hessian</td>\n",
       "      <td>dealing hessian</td>\n",
       "      <td>trying approximate</td>\n",
       "      <td>bfgs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>344609</td>\n",
       "      <td>127538</td>\n",
       "      <td>spikes</td>\n",
       "      <td>tsclean</td>\n",
       "      <td>winsorization</td>\n",
       "      <td>dummy variables</td>\n",
       "      <td>explainable</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>344610</td>\n",
       "      <td>207128</td>\n",
       "      <td>normality</td>\n",
       "      <td>dismiss assumption</td>\n",
       "      <td>know violate</td>\n",
       "      <td>know wilcox</td>\n",
       "      <td>lt dismiss</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19725 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id  user_id                  TopWord1                 TopWord2  \\\n",
       "0       317697   134975   variables statistically             mixed effect   \n",
       "1       317699    45374                  accuracy      validation accuracy   \n",
       "2       317701   171235                   freedom                     mean   \n",
       "3       317703   187797                  patients                 treating   \n",
       "4       317704    61496               uncertainty               error ways   \n",
       "5       317705   134369     bonferroni correction               bonferroni   \n",
       "6       317707   187792   environmental variables                   traits   \n",
       "7       317708    82816        solve differential    equation distribution   \n",
       "8       317710    61092  monotonically increasing            monotonically   \n",
       "9       317711   179343          affect specified             shock affect   \n",
       "10      317712    67137                      simr                able edit   \n",
       "11      317713    67137        measures construct                    power   \n",
       "12      317716   187803        terms applications       difference poisson   \n",
       "13      317717   113090                 graphical         graphical models   \n",
       "14      317718    80922                 recurrent           deep recurrent   \n",
       "15      317719   187804                    person                young old   \n",
       "16      317721   177228              intervention         male respondents   \n",
       "17      317722   187806                      fans                     band   \n",
       "18      317723     8123      distinguishes legged                   legged   \n",
       "19      317724   117168                     block               resampling   \n",
       "20      317725   185581                rows small       corresponding rows   \n",
       "21      317727   117168                     block                   blocks   \n",
       "22      317728   187809                 subscribe                   mining   \n",
       "23      317729    31484         beta distribution                     beta   \n",
       "24      317730   111479               eigenvector                     grey   \n",
       "25      317731      805                      beta                 gaussian   \n",
       "26      317732    88727                    signal                    bound   \n",
       "27      317733   131977            don understand  architecture explaining   \n",
       "28      317734   187810              intervention                   signal   \n",
       "29      317735     8402         denote noncentral           derived theory   \n",
       "...        ...      ...                       ...                      ...   \n",
       "19970   344573    94586              tree species                  species   \n",
       "19971   344574   105149                   college       college highschool   \n",
       "19972   344575   136275                      mean                 use mean   \n",
       "19973   344578   163648            number correct          correct answers   \n",
       "19974   344579   207112               equal equal    distribution variable   \n",
       "19975   344580   195935                     model                     mean   \n",
       "19976   344581   195935      pairwise comparisons                 pairwise   \n",
       "19977   344583   195935          assume continous            binary models   \n",
       "19978   344585   207115    instrumental variables             instrumental   \n",
       "19979   344586   206620                  fonction           anova fonction   \n",
       "19980   344587   207096               probability      probability density   \n",
       "19981   344588   198848      sufficient estimator                bernoulli   \n",
       "19982   344589   206947                      spss         come instruction   \n",
       "19983   344592    96600            area posterior          axis reasonable   \n",
       "19984   344593   206570                 gain lift                lift used   \n",
       "19985   344594   207118                   neurons                    basis   \n",
       "19986   344595      230      example unsupervised    unsupervised learning   \n",
       "19987   344596   207021                     plant                     host   \n",
       "19988   344597   207096                estimating          clustering meta   \n",
       "19989   344598   207058                    series                    paper   \n",
       "19990   344599   207096                    spaces     probability measures   \n",
       "19991   344600   163114             dickey fuller                   dickey   \n",
       "19992   344601   207099           possible wilcox             check sample   \n",
       "19993   344602   121522                  logistic             logistic tag   \n",
       "19994   344604   161278           effects limited            existing mean   \n",
       "19995   344605   207126                     pasta             anova tukeys   \n",
       "19996   344607   121270                      rbms                 networks   \n",
       "19997   344608   191681                   hessian      approximate hessian   \n",
       "19998   344609   127538                    spikes                  tsclean   \n",
       "19999   344610   207128                 normality       dismiss assumption   \n",
       "\n",
       "                     TopWord3                TopWord4  \\\n",
       "0                       mixed               variables   \n",
       "1           training accuracy                 shallow   \n",
       "2             degrees freedom                 degrees   \n",
       "3                       excel                   treat   \n",
       "4          estimate realistic         forecast method   \n",
       "5                  correction                outliers   \n",
       "6               environmental            plant traits   \n",
       "7       differential equation              need solve   \n",
       "8              differentiable              increasing   \n",
       "9            understand shock                    prof   \n",
       "10            deciding sample              edit turns   \n",
       "11                   measures        outcome measures   \n",
       "12              process terms          process markov   \n",
       "13                 log linear                  models   \n",
       "14             neural network        recurrent neural   \n",
       "15                      young                     old   \n",
       "16          intervention time             respondents   \n",
       "17                   universe        overall universe   \n",
       "18                  old style           seek opinions   \n",
       "19                     series                  blocks   \n",
       "20                 constraint            matrix shape   \n",
       "21          block observation                resample   \n",
       "22              brief ideally      customer subscribe   \n",
       "23         distribution prove               look beta   \n",
       "24                 components              eigenvalue   \n",
       "25             symmetric beta               symmetric   \n",
       "26            bound saturated        channel variance   \n",
       "27         concatenate inputs               deep mind   \n",
       "28                     events         external events   \n",
       "29                  don proof           ingersoll ros   \n",
       "...                       ...                     ...   \n",
       "19970                    tree                   crown   \n",
       "19971              highschool          student answer   \n",
       "19972                test set                    test   \n",
       "19973              let number             high school   \n",
       "19974     function parameters          interpretation   \n",
       "19975       application model        baseline renders   \n",
       "19976             comparisons      ability explicitly   \n",
       "19977        combining binary  continous distribution   \n",
       "19978    accounting political             author help   \n",
       "19979             lm fonction                      lm   \n",
       "19980        density function                 density   \n",
       "19981  binomial distributions              sufficient   \n",
       "19982       downloaded plugin         fully motivated   \n",
       "19983      direction observed             does unable   \n",
       "19984             metrics auc              score gain   \n",
       "19985                  inputs            correlations   \n",
       "19986          simple example            unsupervised   \n",
       "19987                  dodder              host plant   \n",
       "19988     clusters estimating      density estimating   \n",
       "19989             time series                    time   \n",
       "19990   functions probability      finite dimensional   \n",
       "19991                  fuller             fuller test   \n",
       "19992      different possible             wilcox test   \n",
       "19993         number relevant      relevant questions   \n",
       "19994         matrix existing        permanent effect   \n",
       "19995   apologies incorrectly           batches pasta   \n",
       "19996           hopfield nets                hopfield   \n",
       "19997         dealing hessian      trying approximate   \n",
       "19998           winsorization         dummy variables   \n",
       "19999            know violate             know wilcox   \n",
       "\n",
       "                        TopWord5  Community  \n",
       "0      statistically significant          1  \n",
       "1                     validation          3  \n",
       "2                    data degree          0  \n",
       "3                  bias treating          4  \n",
       "4                  predicts time          8  \n",
       "5                 don understand          4  \n",
       "6                          plant          1  \n",
       "7               distribution pdf          0  \n",
       "8        assuming differentiable          0  \n",
       "9                          shock          4  \n",
       "10                effect variety         -1  \n",
       "11       multivariate regression          8  \n",
       "12                markov process          3  \n",
       "13                           log          0  \n",
       "14                        neural          3  \n",
       "15                 age indicator          4  \n",
       "16                          male          4  \n",
       "17                        cities          4  \n",
       "18                   recognition          3  \n",
       "19                      resample          4  \n",
       "20                           row          3  \n",
       "21                  fourth block          4  \n",
       "22            incorporate mining          3  \n",
       "23                    prove beta          0  \n",
       "24                     variables          1  \n",
       "25                 beta gaussian          0  \n",
       "26        deriving relationships          4  \n",
       "27              explaining works          0  \n",
       "28                      external          4  \n",
       "29                   proof prove         -1  \n",
       "...                          ...        ...  \n",
       "19970                      lidar          3  \n",
       "19971         correctly question          4  \n",
       "19972                  mean test          3  \n",
       "19973                    let let         25  \n",
       "19974                 parameters          0  \n",
       "19975         building important          0  \n",
       "19976          addition pairwise          4  \n",
       "19977           decisions assume         -1  \n",
       "19978           economics health          0  \n",
       "19979      avoid overpramatizing          1  \n",
       "19980                   function          0  \n",
       "19981                   textbook          0  \n",
       "19982           instruction spss          8  \n",
       "19983            effect opposite         -1  \n",
       "19984       different evaluation         -1  \n",
       "19985                     basing          3  \n",
       "19986                   learning          9  \n",
       "19987               dodder plant          1  \n",
       "19988         estimating density          3  \n",
       "19989                forecasting          4  \n",
       "19990                   discrete          1  \n",
       "19991                 test stata         26  \n",
       "19992              sample normal         27  \n",
       "19993                 tag number          1  \n",
       "19994            matter variable         -1  \n",
       "19995              cooking anova         -1  \n",
       "19996                     hinton          3  \n",
       "19997                       bfgs          0  \n",
       "19998                explainable          4  \n",
       "19999                 lt dismiss          4  \n",
       "\n",
       "[19725 rows x 8 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df_w_comm.to_csv(COMMUNITIES_VIZ_PATH, encoding='utf-8', index=False, columns = ['post_id', 'Community'])\n",
    "post_df_w_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1,\n",
       " 1: 1,\n",
       " 2: 74,\n",
       " 3: 78,\n",
       " 4: 106,\n",
       " 5: 1,\n",
       " 6: 99,\n",
       " 7: 1,\n",
       " 8: 78,\n",
       " 9: 1,\n",
       " 10: 111,\n",
       " 11: 83,\n",
       " 12: 17,\n",
       " 13: 61,\n",
       " 14: 133,\n",
       " 15: 1,\n",
       " 16: 93,\n",
       " 17: 118,\n",
       " 18: 1,\n",
       " 19: 1,\n",
       " 20: 1,\n",
       " 21: 1,\n",
       " 22: 56,\n",
       " 23: 118,\n",
       " 24: 118,\n",
       " 25: 102,\n",
       " 26: 1,\n",
       " 27: 1,\n",
       " 28: 118,\n",
       " 29: 118,\n",
       " 30: 126,\n",
       " 31: 1,\n",
       " 32: 1,\n",
       " 33: 50,\n",
       " 34: 1,\n",
       " 35: 1,\n",
       " 36: 1,\n",
       " 37: 1,\n",
       " 38: 1,\n",
       " 39: 1,\n",
       " 40: 66,\n",
       " 41: 1,\n",
       " 42: 1,\n",
       " 43: 66,\n",
       " 44: 1,\n",
       " 45: 1,\n",
       " 46: 16,\n",
       " 47: 147,\n",
       " 48: 122,\n",
       " 49: 143,\n",
       " 50: 58,\n",
       " 51: 1,\n",
       " 52: 1,\n",
       " 53: 119,\n",
       " 54: 1,\n",
       " 55: 1,\n",
       " 56: 1,\n",
       " 57: 130,\n",
       " 58: 1,\n",
       " 59: 1,\n",
       " 60: 1,\n",
       " 61: 1,\n",
       " 62: 1,\n",
       " 63: 1,\n",
       " 64: 96,\n",
       " 65: 129,\n",
       " 66: 1,\n",
       " 67: 1,\n",
       " 68: 1,\n",
       " 69: 1,\n",
       " 70: 1,\n",
       " 71: 61,\n",
       " 72: 77,\n",
       " 73: 118,\n",
       " 74: 117,\n",
       " 75: 1,\n",
       " 76: 145,\n",
       " 77: 1,\n",
       " 78: 135,\n",
       " 79: 1,\n",
       " 80: 72,\n",
       " 81: 118,\n",
       " 82: 1,\n",
       " 83: 68,\n",
       " 84: 1,\n",
       " 85: 1,\n",
       " 86: 1,\n",
       " 87: 54,\n",
       " 88: 58,\n",
       " 89: 1,\n",
       " 90: 1,\n",
       " 91: 1,\n",
       " 92: 1,\n",
       " 93: 137,\n",
       " 94: 1,\n",
       " 95: 111,\n",
       " 96: 66,\n",
       " 97: 1,\n",
       " 98: 121,\n",
       " 99: 1,\n",
       " 100: 1,\n",
       " 101: 106,\n",
       " 102: 1,\n",
       " 103: 1,\n",
       " 104: 1,\n",
       " 105: 101,\n",
       " 106: 1,\n",
       " 107: 109,\n",
       " 108: 1,\n",
       " 109: 118,\n",
       " 110: 69,\n",
       " 111: 137,\n",
       " 112: 130,\n",
       " 113: 1,\n",
       " 114: 117,\n",
       " 115: 1,\n",
       " 116: 1,\n",
       " 117: 1,\n",
       " 118: 73,\n",
       " 119: 60,\n",
       " 120: 1,\n",
       " 121: 73,\n",
       " 122: 71,\n",
       " 123: 1,\n",
       " 124: 70,\n",
       " 125: 1,\n",
       " 126: 93,\n",
       " 127: 42,\n",
       " 128: 1,\n",
       " 129: 1,\n",
       " 130: 118,\n",
       " 131: 1,\n",
       " 132: 1,\n",
       " 133: 103,\n",
       " 134: 86,\n",
       " 135: 1,\n",
       " 136: 122,\n",
       " 137: 1,\n",
       " 138: 102,\n",
       " 139: 1,\n",
       " 140: 118,\n",
       " 141: 1,\n",
       " 142: 93,\n",
       " 143: 66,\n",
       " 144: 1,\n",
       " 145: 1,\n",
       " 146: 1,\n",
       " 147: 1,\n",
       " 148: 118,\n",
       " 149: 118,\n",
       " 150: 1,\n",
       " 151: 1,\n",
       " 152: 1,\n",
       " 153: 56,\n",
       " 154: 118,\n",
       " 155: 56,\n",
       " 156: 1,\n",
       " 157: 1,\n",
       " 158: 138,\n",
       " 159: 84,\n",
       " 160: 1,\n",
       " 161: 1,\n",
       " 162: 84,\n",
       " 163: 90,\n",
       " 164: 1,\n",
       " 165: 1,\n",
       " 166: 147,\n",
       " 167: 134,\n",
       " 168: 107,\n",
       " 169: 22,\n",
       " 170: 120,\n",
       " 171: 102,\n",
       " 172: 145,\n",
       " 173: 133,\n",
       " 174: 1,\n",
       " 175: 1,\n",
       " 176: 1,\n",
       " 177: 1,\n",
       " 178: 147,\n",
       " 179: 1,\n",
       " 180: 1,\n",
       " 181: 1,\n",
       " 182: 145,\n",
       " 183: 1,\n",
       " 184: 40,\n",
       " 185: 135,\n",
       " 186: 140,\n",
       " 187: 1,\n",
       " 188: 1,\n",
       " 189: 1,\n",
       " 190: 1,\n",
       " 191: 1,\n",
       " 192: 147,\n",
       " 193: 1,\n",
       " 194: 1,\n",
       " 195: 1,\n",
       " 196: 87,\n",
       " 197: 1,\n",
       " 198: 1,\n",
       " 199: 124,\n",
       " 200: 118,\n",
       " 201: 147,\n",
       " 202: 1,\n",
       " 203: 1,\n",
       " 204: 1,\n",
       " 205: 1,\n",
       " 206: 102,\n",
       " 207: 1,\n",
       " 208: 95,\n",
       " 209: 1,\n",
       " 210: 1,\n",
       " 211: 102,\n",
       " 212: 1,\n",
       " 213: 118,\n",
       " 214: 99,\n",
       " 215: 1,\n",
       " 216: 88,\n",
       " 217: 86,\n",
       " 218: 42,\n",
       " 219: 1,\n",
       " 220: 1,\n",
       " 221: 1,\n",
       " 222: 1,\n",
       " 223: 86,\n",
       " 224: 119,\n",
       " 225: 1,\n",
       " 226: 118,\n",
       " 227: 1,\n",
       " 228: 119,\n",
       " 229: 68,\n",
       " 230: 1,\n",
       " 231: 1,\n",
       " 232: 1,\n",
       " 233: 1,\n",
       " 234: 1,\n",
       " 235: 60,\n",
       " 236: 1,\n",
       " 237: 1,\n",
       " 238: 1,\n",
       " 239: 64,\n",
       " 240: 1,\n",
       " 241: 77,\n",
       " 242: 93,\n",
       " 243: 118,\n",
       " 244: 132,\n",
       " 245: 133,\n",
       " 246: 93,\n",
       " 247: 145,\n",
       " 248: 42,\n",
       " 249: 118,\n",
       " 250: 58,\n",
       " 251: 78,\n",
       " 252: 145,\n",
       " 253: 122,\n",
       " 254: 1,\n",
       " 255: 1,\n",
       " 256: 1,\n",
       " 257: 1,\n",
       " 258: 1,\n",
       " 259: 1,\n",
       " 260: 16,\n",
       " 261: 93,\n",
       " 262: 135,\n",
       " 263: 117,\n",
       " 264: 102,\n",
       " 265: 1,\n",
       " 266: 78,\n",
       " 267: 1,\n",
       " 268: 137,\n",
       " 269: 1,\n",
       " 270: 122,\n",
       " 271: 1,\n",
       " 272: 140,\n",
       " 273: 1,\n",
       " 274: 118,\n",
       " 275: 1,\n",
       " 276: 66,\n",
       " 277: 118,\n",
       " 278: 1,\n",
       " 279: 1,\n",
       " 280: 132,\n",
       " 281: 1,\n",
       " 282: 1,\n",
       " 283: 1,\n",
       " 284: 137,\n",
       " 285: 66,\n",
       " 286: 1,\n",
       " 287: 22,\n",
       " 288: 132,\n",
       " 289: 1,\n",
       " 290: 1,\n",
       " 291: 1,\n",
       " 292: 117,\n",
       " 293: 67,\n",
       " 294: 124,\n",
       " 295: 50,\n",
       " 296: 1,\n",
       " 297: 1,\n",
       " 298: 1,\n",
       " 299: 1,\n",
       " 300: 42,\n",
       " 301: 102,\n",
       " 302: 39,\n",
       " 303: 140,\n",
       " 304: 1,\n",
       " 305: 1,\n",
       " 306: 84,\n",
       " 307: 1,\n",
       " 308: 95,\n",
       " 309: 132,\n",
       " 310: 1,\n",
       " 311: 145,\n",
       " 312: 66,\n",
       " 313: 17,\n",
       " 314: 1,\n",
       " 315: 1,\n",
       " 316: 137,\n",
       " 317: 124,\n",
       " 318: 1,\n",
       " 319: 1,\n",
       " 320: 84,\n",
       " 321: 1,\n",
       " 322: 1,\n",
       " 323: 148,\n",
       " 324: 100,\n",
       " 325: 111,\n",
       " 326: 1,\n",
       " 327: 105,\n",
       " 328: 109,\n",
       " 329: 1,\n",
       " 330: 1,\n",
       " 331: 77,\n",
       " 332: 144,\n",
       " 333: 1,\n",
       " 334: 1,\n",
       " 335: 1,\n",
       " 336: 1,\n",
       " 337: 1,\n",
       " 338: 102,\n",
       " 339: 120,\n",
       " 340: 68,\n",
       " 341: 1,\n",
       " 342: 44,\n",
       " 343: 1,\n",
       " 344: 140,\n",
       " 345: 1,\n",
       " 346: 1,\n",
       " 347: 50,\n",
       " 348: 125,\n",
       " 349: 148,\n",
       " 350: 103,\n",
       " 351: 1,\n",
       " 352: 1,\n",
       " 353: 95,\n",
       " 354: 1,\n",
       " 355: 95,\n",
       " 356: 101,\n",
       " 357: 1,\n",
       " 358: 140,\n",
       " 359: 1,\n",
       " 360: 64,\n",
       " 361: 95,\n",
       " 362: 137,\n",
       " 363: 108,\n",
       " 364: 54,\n",
       " 365: 16,\n",
       " 366: 1,\n",
       " 367: 1,\n",
       " 368: 1,\n",
       " 369: 1,\n",
       " 370: 118,\n",
       " 371: 118,\n",
       " 372: 1,\n",
       " 373: 131,\n",
       " 374: 50,\n",
       " 375: 118,\n",
       " 376: 1,\n",
       " 377: 1,\n",
       " 378: 1,\n",
       " 379: 1,\n",
       " 380: 1,\n",
       " 381: 118,\n",
       " 382: 1,\n",
       " 383: 1,\n",
       " 384: 17,\n",
       " 385: 1,\n",
       " 386: 87,\n",
       " 387: 1,\n",
       " 388: 68,\n",
       " 389: 1,\n",
       " 390: 1,\n",
       " 391: 1,\n",
       " 392: 96,\n",
       " 393: 1,\n",
       " 394: 1,\n",
       " 395: 1,\n",
       " 396: 1,\n",
       " 397: 1,\n",
       " 398: 118,\n",
       " 399: 1,\n",
       " 400: 119,\n",
       " 401: 83,\n",
       " 402: 145,\n",
       " 403: 1,\n",
       " 404: 66,\n",
       " 405: 66,\n",
       " 406: 68,\n",
       " 407: 59,\n",
       " 408: 1,\n",
       " 409: 101,\n",
       " 410: 1,\n",
       " 411: 107,\n",
       " 412: 89,\n",
       " 413: 1,\n",
       " 414: 1,\n",
       " 415: 1,\n",
       " 416: 145,\n",
       " 417: 1,\n",
       " 418: 1,\n",
       " 419: 1,\n",
       " 420: 88,\n",
       " 421: 1,\n",
       " 422: 147,\n",
       " 423: 1,\n",
       " 424: 1,\n",
       " 425: 118,\n",
       " 426: 1,\n",
       " 427: 1,\n",
       " 428: 140,\n",
       " 429: 54,\n",
       " 430: 1,\n",
       " 431: 16,\n",
       " 432: 130,\n",
       " 433: 72,\n",
       " 434: 1,\n",
       " 435: 1,\n",
       " 436: 58,\n",
       " 437: 58,\n",
       " 438: 1,\n",
       " 439: 1,\n",
       " 440: 130,\n",
       " 441: 54,\n",
       " 442: 1,\n",
       " 443: 147,\n",
       " 444: 1,\n",
       " 445: 110,\n",
       " 446: 1,\n",
       " 447: 58,\n",
       " 448: 118,\n",
       " 449: 59,\n",
       " 450: 1,\n",
       " 451: 1,\n",
       " 452: 1,\n",
       " 453: 130,\n",
       " 454: 118,\n",
       " 455: 100,\n",
       " 456: 1,\n",
       " 457: 1,\n",
       " 458: 1,\n",
       " 459: 54,\n",
       " 460: 1,\n",
       " 461: 1,\n",
       " 462: 1,\n",
       " 463: 95,\n",
       " 464: 64,\n",
       " 465: 123,\n",
       " 466: 1,\n",
       " 467: 115,\n",
       " 468: 42,\n",
       " 469: 54,\n",
       " 470: 118,\n",
       " 471: 1,\n",
       " 472: 1,\n",
       " 473: 89,\n",
       " 474: 16,\n",
       " 475: 1,\n",
       " 476: 102,\n",
       " 477: 1,\n",
       " 478: 131,\n",
       " 479: 66,\n",
       " 480: 118,\n",
       " 481: 118,\n",
       " 482: 118,\n",
       " 483: 107,\n",
       " 484: 117,\n",
       " 485: 1,\n",
       " 486: 1,\n",
       " 487: 1,\n",
       " 488: 66,\n",
       " 489: 88,\n",
       " 490: 107,\n",
       " 491: 17,\n",
       " 492: 143,\n",
       " 493: 1,\n",
       " 494: 54,\n",
       " 495: 137,\n",
       " 496: 1,\n",
       " 497: 118,\n",
       " 498: 134,\n",
       " 499: 50,\n",
       " 500: 73,\n",
       " 501: 116,\n",
       " 502: 145,\n",
       " 503: 145,\n",
       " 504: 77,\n",
       " 505: 66,\n",
       " 506: 84,\n",
       " 507: 1,\n",
       " 508: 1,\n",
       " 509: 1,\n",
       " 510: 1,\n",
       " 511: 58,\n",
       " 512: 84,\n",
       " 513: 1,\n",
       " 514: 1,\n",
       " 515: 131,\n",
       " 516: 1,\n",
       " 517: 1,\n",
       " 518: 89,\n",
       " 519: 101,\n",
       " 520: 1,\n",
       " 521: 1,\n",
       " 522: 147,\n",
       " 523: 1,\n",
       " 524: 118,\n",
       " 525: 1,\n",
       " 526: 118,\n",
       " 527: 1,\n",
       " 528: 105,\n",
       " 529: 1,\n",
       " 530: 16,\n",
       " 531: 1,\n",
       " 532: 131,\n",
       " 533: 16,\n",
       " 534: 1,\n",
       " 535: 58,\n",
       " 536: 84,\n",
       " 537: 1,\n",
       " 538: 1,\n",
       " 539: 108,\n",
       " 540: 1,\n",
       " 541: 1,\n",
       " 542: 137,\n",
       " 543: 77,\n",
       " 544: 82,\n",
       " 545: 131,\n",
       " 546: 76,\n",
       " 547: 1,\n",
       " 548: 1,\n",
       " 549: 1,\n",
       " 550: 1,\n",
       " 551: 116,\n",
       " 552: 76,\n",
       " 553: 118,\n",
       " 554: 117,\n",
       " 555: 1,\n",
       " 556: 1,\n",
       " 557: 125,\n",
       " 558: 67,\n",
       " 559: 134,\n",
       " 560: 66,\n",
       " 561: 126,\n",
       " 562: 102,\n",
       " 563: 78,\n",
       " 564: 1,\n",
       " 565: 1,\n",
       " 566: 1,\n",
       " 567: 1,\n",
       " 568: 77,\n",
       " 569: 1,\n",
       " 570: 72,\n",
       " 571: 76,\n",
       " 572: 146,\n",
       " 573: 137,\n",
       " 574: 140,\n",
       " 575: 137,\n",
       " 576: 1,\n",
       " 577: 119,\n",
       " 578: 1,\n",
       " 579: 1,\n",
       " 580: 138,\n",
       " 581: 1,\n",
       " 582: 1,\n",
       " 583: 137,\n",
       " 584: 63,\n",
       " 585: 68,\n",
       " 586: 83,\n",
       " 587: 118,\n",
       " 588: 95,\n",
       " 589: 140,\n",
       " 590: 67,\n",
       " 591: 1,\n",
       " 592: 110,\n",
       " 593: 1,\n",
       " 594: 118,\n",
       " 595: 130,\n",
       " 596: 118,\n",
       " 597: 118,\n",
       " 598: 1,\n",
       " 599: 1,\n",
       " 600: 93,\n",
       " 601: 102,\n",
       " 602: 118,\n",
       " 603: 1,\n",
       " 604: 126,\n",
       " 605: 1,\n",
       " 606: 65,\n",
       " 607: 107,\n",
       " 608: 58,\n",
       " 609: 1,\n",
       " 610: 1,\n",
       " 611: 63,\n",
       " 612: 144,\n",
       " 613: 86,\n",
       " 614: 118,\n",
       " 615: 1,\n",
       " 616: 102,\n",
       " 617: 118,\n",
       " 618: 1,\n",
       " 619: 1,\n",
       " 620: 125,\n",
       " 621: 1,\n",
       " 622: 67,\n",
       " 623: 67,\n",
       " 624: 82,\n",
       " 625: 1,\n",
       " 626: 1,\n",
       " 627: 1,\n",
       " 628: 67,\n",
       " 629: 67,\n",
       " 630: 1,\n",
       " 631: 1,\n",
       " 632: 1,\n",
       " 633: 1,\n",
       " 634: 1,\n",
       " 635: 1,\n",
       " 636: 1,\n",
       " 637: 1,\n",
       " 638: 105,\n",
       " 639: 1,\n",
       " 640: 123,\n",
       " 641: 135,\n",
       " 642: 1,\n",
       " 643: 58,\n",
       " 644: 125,\n",
       " 645: 1,\n",
       " 646: 1,\n",
       " 647: 1,\n",
       " 648: 122,\n",
       " 649: 28,\n",
       " 650: 1,\n",
       " 651: 50,\n",
       " 652: 1,\n",
       " 653: 1,\n",
       " 654: 117,\n",
       " 655: 1,\n",
       " 656: 145,\n",
       " 657: 135,\n",
       " 658: 1,\n",
       " 659: 91,\n",
       " 660: 50,\n",
       " 661: 89,\n",
       " 662: 108,\n",
       " 663: 1,\n",
       " 664: 1,\n",
       " 665: 1,\n",
       " 666: 97,\n",
       " 667: 118,\n",
       " 668: 97,\n",
       " 669: 97,\n",
       " 670: 1,\n",
       " 671: 1,\n",
       " 672: 1,\n",
       " 673: 1,\n",
       " 674: 1,\n",
       " 675: 1,\n",
       " 676: 91,\n",
       " 677: 1,\n",
       " 678: 1,\n",
       " 679: 103,\n",
       " 680: 107,\n",
       " 681: 1,\n",
       " 682: 1,\n",
       " 683: 1,\n",
       " 684: 1,\n",
       " 685: 1,\n",
       " 686: 97,\n",
       " 687: 1,\n",
       " 688: 1,\n",
       " 689: 145,\n",
       " 690: 130,\n",
       " 691: 64,\n",
       " 692: 1,\n",
       " 693: 118,\n",
       " 694: 103,\n",
       " 695: 106,\n",
       " 696: 1,\n",
       " 697: 1,\n",
       " 698: 1,\n",
       " 699: 1,\n",
       " 700: 96,\n",
       " 701: 1,\n",
       " 702: 1,\n",
       " 703: 1,\n",
       " 704: 147,\n",
       " 705: 1,\n",
       " 706: 1,\n",
       " 707: 118,\n",
       " 708: 50,\n",
       " 709: 77,\n",
       " 710: 1,\n",
       " 711: 1,\n",
       " 712: 1,\n",
       " 713: 1,\n",
       " 714: 1,\n",
       " 715: 78,\n",
       " 716: 1,\n",
       " 717: 1,\n",
       " 718: 1,\n",
       " 719: 1,\n",
       " 720: 147,\n",
       " 721: 1,\n",
       " 722: 1,\n",
       " 723: 67,\n",
       " 724: 1,\n",
       " 725: 1,\n",
       " 726: 1,\n",
       " 727: 1,\n",
       " 728: 1,\n",
       " 729: 0,\n",
       " 730: 1,\n",
       " 731: 1,\n",
       " 732: 134,\n",
       " 733: 123,\n",
       " 734: 109,\n",
       " 735: 119,\n",
       " 736: 1,\n",
       " 737: 1,\n",
       " 738: 1,\n",
       " 739: 118,\n",
       " 740: 50,\n",
       " 741: 123,\n",
       " 742: 117,\n",
       " 743: 1,\n",
       " 744: 102,\n",
       " 745: 101,\n",
       " 746: 130,\n",
       " 747: 1,\n",
       " 748: 1,\n",
       " 749: 1,\n",
       " 750: 1,\n",
       " 751: 145,\n",
       " 752: 130,\n",
       " 753: 1,\n",
       " 754: 126,\n",
       " 755: 114,\n",
       " 756: 1,\n",
       " 757: 141,\n",
       " 758: 1,\n",
       " 759: 50,\n",
       " 760: 101,\n",
       " 761: 50,\n",
       " 762: 1,\n",
       " 763: 1,\n",
       " 764: 1,\n",
       " 765: 1,\n",
       " 766: 91,\n",
       " 767: 134,\n",
       " 768: 64,\n",
       " 769: 1,\n",
       " 770: 50,\n",
       " 771: 82,\n",
       " 772: 1,\n",
       " 773: 1,\n",
       " 774: 95,\n",
       " 775: 103,\n",
       " 776: 117,\n",
       " 777: 118,\n",
       " 778: 1,\n",
       " 779: 91,\n",
       " 780: 1,\n",
       " 781: 1,\n",
       " 782: 1,\n",
       " 783: 18,\n",
       " 784: 1,\n",
       " 785: 117,\n",
       " 786: 1,\n",
       " 787: 78,\n",
       " 788: 118,\n",
       " 789: 1,\n",
       " 790: 1,\n",
       " 791: 118,\n",
       " 792: 1,\n",
       " 793: 1,\n",
       " 794: 117,\n",
       " 795: 118,\n",
       " 796: 1,\n",
       " 797: 116,\n",
       " 798: 140,\n",
       " 799: 54,\n",
       " 800: 1,\n",
       " 801: 103,\n",
       " 802: 135,\n",
       " 803: 1,\n",
       " 804: 54,\n",
       " 805: 1,\n",
       " 806: 54,\n",
       " 807: 17,\n",
       " 808: 68,\n",
       " 809: 67,\n",
       " 810: 1,\n",
       " 811: 1,\n",
       " 812: 118,\n",
       " 813: 137,\n",
       " 814: 105,\n",
       " 815: 1,\n",
       " 816: 58,\n",
       " 817: 1,\n",
       " 818: 1,\n",
       " 819: 1,\n",
       " 820: 1,\n",
       " 821: 1,\n",
       " 822: 1,\n",
       " 823: 1,\n",
       " 824: 1,\n",
       " 825: 118,\n",
       " 826: 118,\n",
       " 827: 1,\n",
       " 828: 1,\n",
       " 829: 118,\n",
       " 830: 145,\n",
       " 831: 1,\n",
       " 832: 84,\n",
       " 833: 16,\n",
       " 834: 132,\n",
       " 835: 132,\n",
       " 836: 1,\n",
       " 837: 137,\n",
       " 838: 1,\n",
       " 839: 135,\n",
       " 840: 60,\n",
       " 841: 1,\n",
       " 842: 1,\n",
       " 843: 1,\n",
       " 844: 137,\n",
       " 845: 16,\n",
       " 846: 56,\n",
       " 847: 1,\n",
       " 848: 145,\n",
       " 849: 56,\n",
       " 850: 1,\n",
       " 851: 118,\n",
       " 852: 1,\n",
       " 853: 1,\n",
       " 854: 105,\n",
       " 855: 147,\n",
       " 856: 123,\n",
       " 857: 96,\n",
       " 858: 51,\n",
       " 859: 1,\n",
       " 860: 117,\n",
       " 861: 105,\n",
       " 862: 84,\n",
       " 863: 118,\n",
       " 864: 115,\n",
       " 865: 1,\n",
       " 866: 1,\n",
       " 867: 1,\n",
       " 868: 145,\n",
       " 869: 1,\n",
       " 870: 118,\n",
       " 871: 1,\n",
       " 872: 42,\n",
       " 873: 1,\n",
       " 874: 1,\n",
       " 875: 1,\n",
       " 876: 78,\n",
       " 877: 1,\n",
       " 878: 54,\n",
       " 879: 137,\n",
       " 880: 1,\n",
       " 881: 120,\n",
       " 882: 1,\n",
       " 883: 127,\n",
       " 884: 16,\n",
       " 885: 123,\n",
       " 886: 1,\n",
       " 887: 1,\n",
       " 888: 147,\n",
       " 889: 108,\n",
       " 890: 1,\n",
       " 891: 147,\n",
       " 892: 1,\n",
       " 893: 1,\n",
       " 894: 118,\n",
       " 895: 87,\n",
       " 896: 1,\n",
       " 897: 144,\n",
       " 898: 1,\n",
       " 899: 16,\n",
       " 900: 1,\n",
       " 901: 1,\n",
       " 902: 67,\n",
       " 903: 1,\n",
       " 904: 1,\n",
       " 905: 102,\n",
       " 906: 100,\n",
       " 907: 147,\n",
       " 908: 66,\n",
       " 909: 1,\n",
       " 910: 118,\n",
       " 911: 119,\n",
       " 912: 30,\n",
       " 913: 93,\n",
       " 914: 118,\n",
       " 915: 131,\n",
       " 916: 1,\n",
       " 917: 78,\n",
       " 918: 78,\n",
       " 919: 1,\n",
       " 920: 1,\n",
       " 921: 107,\n",
       " 922: 1,\n",
       " 923: 114,\n",
       " 924: 105,\n",
       " 925: 1,\n",
       " 926: 1,\n",
       " 927: 1,\n",
       " 928: 146,\n",
       " 929: 1,\n",
       " 930: 1,\n",
       " 931: 1,\n",
       " 932: 118,\n",
       " 933: 67,\n",
       " 934: 84,\n",
       " 935: 118,\n",
       " 936: 118,\n",
       " 937: 147,\n",
       " 938: 147,\n",
       " 939: 1,\n",
       " 940: 96,\n",
       " 941: 1,\n",
       " 942: 0,\n",
       " 943: 119,\n",
       " 944: 123,\n",
       " 945: 66,\n",
       " 946: 68,\n",
       " 947: 118,\n",
       " 948: 84,\n",
       " 949: 1,\n",
       " 950: 42,\n",
       " 951: 147,\n",
       " 952: 1,\n",
       " 953: 1,\n",
       " 954: 54,\n",
       " 955: 129,\n",
       " 956: 148,\n",
       " 957: 1,\n",
       " 958: 118,\n",
       " 959: 1,\n",
       " 960: 118,\n",
       " 961: 1,\n",
       " 962: 140,\n",
       " 963: 50,\n",
       " 964: 1,\n",
       " 965: 140,\n",
       " 966: 107,\n",
       " 967: 1,\n",
       " 968: 1,\n",
       " 969: 1,\n",
       " 970: 1,\n",
       " 971: 1,\n",
       " 972: 1,\n",
       " 973: 113,\n",
       " 974: 54,\n",
       " 975: 113,\n",
       " 976: 1,\n",
       " 977: 54,\n",
       " 978: 1,\n",
       " 979: 58,\n",
       " 980: 1,\n",
       " 981: 54,\n",
       " 982: 1,\n",
       " 983: 102,\n",
       " 984: 118,\n",
       " 985: 89,\n",
       " 986: 42,\n",
       " 987: 122,\n",
       " 988: 1,\n",
       " 989: 1,\n",
       " 990: 1,\n",
       " 991: 70,\n",
       " 992: 1,\n",
       " 993: 76,\n",
       " 994: 145,\n",
       " 995: 102,\n",
       " 996: 1,\n",
       " 997: 1,\n",
       " 998: 134,\n",
       " 999: 1,\n",
       " ...}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spectral clustering\n",
    "pickle_f = open(SPEC_COMMUNITIES_PICKLE, 'rb')\n",
    "community_dict2 = pickle.load(pickle_f)\n",
    "\n",
    "community_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_postids(G):\n",
    "    \"\"\"\n",
    "    Returns a Graph containing all the remapped post_ids so that they go from 0 to n. \n",
    "    Also returns the dictionary that maps the ids to their new value.\n",
    "    \"\"\"\n",
    "    postid_map = dict()\n",
    "    new_G = snap.TUNGraph.New()\n",
    "    index = 0\n",
    "    \n",
    "    # Remap all nodes. Only keep ones with degree > 0.\n",
    "    for N in G.Nodes():\n",
    "        if (N.GetDeg() < 1): continue \n",
    "        postid_map[N.GetId()] = index\n",
    "        new_G.AddNode(index)\n",
    "        index += 1\n",
    "                \n",
    "    # Remap all edges.\n",
    "    for E in G.Edges(): # Edge traversal\n",
    "        new_G.AddEdge(postid_map[E.GetSrcNId()], postid_map[E.GetDstNId()])\n",
    "        \n",
    "    return new_G, postid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, postid_dict2 = remap_postids(post_graph)\n",
    "\n",
    "post_df2 = load_top_ngram_df(POST_TOP_NGRAM_PATH)\n",
    "post_df_w_comm2 = add_communities_post_df(post_df, community_dict2, postid_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>TopWord1</th>\n",
       "      <th>TopWord2</th>\n",
       "      <th>TopWord3</th>\n",
       "      <th>TopWord4</th>\n",
       "      <th>TopWord5</th>\n",
       "      <th>Community</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317697</td>\n",
       "      <td>134975</td>\n",
       "      <td>variables statistically</td>\n",
       "      <td>mixed effect</td>\n",
       "      <td>mixed</td>\n",
       "      <td>variables</td>\n",
       "      <td>statistically significant</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>317699</td>\n",
       "      <td>45374</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>validation accuracy</td>\n",
       "      <td>training accuracy</td>\n",
       "      <td>shallow</td>\n",
       "      <td>validation</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>317701</td>\n",
       "      <td>171235</td>\n",
       "      <td>freedom</td>\n",
       "      <td>mean</td>\n",
       "      <td>degrees freedom</td>\n",
       "      <td>degrees</td>\n",
       "      <td>data degree</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>317703</td>\n",
       "      <td>187797</td>\n",
       "      <td>patients</td>\n",
       "      <td>treating</td>\n",
       "      <td>excel</td>\n",
       "      <td>treat</td>\n",
       "      <td>bias treating</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317704</td>\n",
       "      <td>61496</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>error ways</td>\n",
       "      <td>estimate realistic</td>\n",
       "      <td>forecast method</td>\n",
       "      <td>predicts time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>317705</td>\n",
       "      <td>134369</td>\n",
       "      <td>bonferroni correction</td>\n",
       "      <td>bonferroni</td>\n",
       "      <td>correction</td>\n",
       "      <td>outliers</td>\n",
       "      <td>don understand</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>317707</td>\n",
       "      <td>187792</td>\n",
       "      <td>environmental variables</td>\n",
       "      <td>traits</td>\n",
       "      <td>environmental</td>\n",
       "      <td>plant traits</td>\n",
       "      <td>plant</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>317708</td>\n",
       "      <td>82816</td>\n",
       "      <td>solve differential</td>\n",
       "      <td>equation distribution</td>\n",
       "      <td>differential equation</td>\n",
       "      <td>need solve</td>\n",
       "      <td>distribution pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>317710</td>\n",
       "      <td>61092</td>\n",
       "      <td>monotonically increasing</td>\n",
       "      <td>monotonically</td>\n",
       "      <td>differentiable</td>\n",
       "      <td>increasing</td>\n",
       "      <td>assuming differentiable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>317711</td>\n",
       "      <td>179343</td>\n",
       "      <td>affect specified</td>\n",
       "      <td>shock affect</td>\n",
       "      <td>understand shock</td>\n",
       "      <td>prof</td>\n",
       "      <td>shock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>317712</td>\n",
       "      <td>67137</td>\n",
       "      <td>simr</td>\n",
       "      <td>able edit</td>\n",
       "      <td>deciding sample</td>\n",
       "      <td>edit turns</td>\n",
       "      <td>effect variety</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>317713</td>\n",
       "      <td>67137</td>\n",
       "      <td>measures construct</td>\n",
       "      <td>power</td>\n",
       "      <td>measures</td>\n",
       "      <td>outcome measures</td>\n",
       "      <td>multivariate regression</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>317716</td>\n",
       "      <td>187803</td>\n",
       "      <td>terms applications</td>\n",
       "      <td>difference poisson</td>\n",
       "      <td>process terms</td>\n",
       "      <td>process markov</td>\n",
       "      <td>markov process</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>317717</td>\n",
       "      <td>113090</td>\n",
       "      <td>graphical</td>\n",
       "      <td>graphical models</td>\n",
       "      <td>log linear</td>\n",
       "      <td>models</td>\n",
       "      <td>log</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>317718</td>\n",
       "      <td>80922</td>\n",
       "      <td>recurrent</td>\n",
       "      <td>deep recurrent</td>\n",
       "      <td>neural network</td>\n",
       "      <td>recurrent neural</td>\n",
       "      <td>neural</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>317719</td>\n",
       "      <td>187804</td>\n",
       "      <td>person</td>\n",
       "      <td>young old</td>\n",
       "      <td>young</td>\n",
       "      <td>old</td>\n",
       "      <td>age indicator</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>317721</td>\n",
       "      <td>177228</td>\n",
       "      <td>intervention</td>\n",
       "      <td>male respondents</td>\n",
       "      <td>intervention time</td>\n",
       "      <td>respondents</td>\n",
       "      <td>male</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>317722</td>\n",
       "      <td>187806</td>\n",
       "      <td>fans</td>\n",
       "      <td>band</td>\n",
       "      <td>universe</td>\n",
       "      <td>overall universe</td>\n",
       "      <td>cities</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>317723</td>\n",
       "      <td>8123</td>\n",
       "      <td>distinguishes legged</td>\n",
       "      <td>legged</td>\n",
       "      <td>old style</td>\n",
       "      <td>seek opinions</td>\n",
       "      <td>recognition</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>317724</td>\n",
       "      <td>117168</td>\n",
       "      <td>block</td>\n",
       "      <td>resampling</td>\n",
       "      <td>series</td>\n",
       "      <td>blocks</td>\n",
       "      <td>resample</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>317725</td>\n",
       "      <td>185581</td>\n",
       "      <td>rows small</td>\n",
       "      <td>corresponding rows</td>\n",
       "      <td>constraint</td>\n",
       "      <td>matrix shape</td>\n",
       "      <td>row</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>317727</td>\n",
       "      <td>117168</td>\n",
       "      <td>block</td>\n",
       "      <td>blocks</td>\n",
       "      <td>block observation</td>\n",
       "      <td>resample</td>\n",
       "      <td>fourth block</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>317728</td>\n",
       "      <td>187809</td>\n",
       "      <td>subscribe</td>\n",
       "      <td>mining</td>\n",
       "      <td>brief ideally</td>\n",
       "      <td>customer subscribe</td>\n",
       "      <td>incorporate mining</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>317729</td>\n",
       "      <td>31484</td>\n",
       "      <td>beta distribution</td>\n",
       "      <td>beta</td>\n",
       "      <td>distribution prove</td>\n",
       "      <td>look beta</td>\n",
       "      <td>prove beta</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>317730</td>\n",
       "      <td>111479</td>\n",
       "      <td>eigenvector</td>\n",
       "      <td>grey</td>\n",
       "      <td>components</td>\n",
       "      <td>eigenvalue</td>\n",
       "      <td>variables</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>317731</td>\n",
       "      <td>805</td>\n",
       "      <td>beta</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>symmetric beta</td>\n",
       "      <td>symmetric</td>\n",
       "      <td>beta gaussian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>317732</td>\n",
       "      <td>88727</td>\n",
       "      <td>signal</td>\n",
       "      <td>bound</td>\n",
       "      <td>bound saturated</td>\n",
       "      <td>channel variance</td>\n",
       "      <td>deriving relationships</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>317733</td>\n",
       "      <td>131977</td>\n",
       "      <td>don understand</td>\n",
       "      <td>architecture explaining</td>\n",
       "      <td>concatenate inputs</td>\n",
       "      <td>deep mind</td>\n",
       "      <td>explaining works</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>317734</td>\n",
       "      <td>187810</td>\n",
       "      <td>intervention</td>\n",
       "      <td>signal</td>\n",
       "      <td>events</td>\n",
       "      <td>external events</td>\n",
       "      <td>external</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>317735</td>\n",
       "      <td>8402</td>\n",
       "      <td>denote noncentral</td>\n",
       "      <td>derived theory</td>\n",
       "      <td>don proof</td>\n",
       "      <td>ingersoll ros</td>\n",
       "      <td>proof prove</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>344573</td>\n",
       "      <td>94586</td>\n",
       "      <td>tree species</td>\n",
       "      <td>species</td>\n",
       "      <td>tree</td>\n",
       "      <td>crown</td>\n",
       "      <td>lidar</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>344574</td>\n",
       "      <td>105149</td>\n",
       "      <td>college</td>\n",
       "      <td>college highschool</td>\n",
       "      <td>highschool</td>\n",
       "      <td>student answer</td>\n",
       "      <td>correctly question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>344575</td>\n",
       "      <td>136275</td>\n",
       "      <td>mean</td>\n",
       "      <td>use mean</td>\n",
       "      <td>test set</td>\n",
       "      <td>test</td>\n",
       "      <td>mean test</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>344578</td>\n",
       "      <td>163648</td>\n",
       "      <td>number correct</td>\n",
       "      <td>correct answers</td>\n",
       "      <td>let number</td>\n",
       "      <td>high school</td>\n",
       "      <td>let let</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>344579</td>\n",
       "      <td>207112</td>\n",
       "      <td>equal equal</td>\n",
       "      <td>distribution variable</td>\n",
       "      <td>function parameters</td>\n",
       "      <td>interpretation</td>\n",
       "      <td>parameters</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>344580</td>\n",
       "      <td>195935</td>\n",
       "      <td>model</td>\n",
       "      <td>mean</td>\n",
       "      <td>application model</td>\n",
       "      <td>baseline renders</td>\n",
       "      <td>building important</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>344581</td>\n",
       "      <td>195935</td>\n",
       "      <td>pairwise comparisons</td>\n",
       "      <td>pairwise</td>\n",
       "      <td>comparisons</td>\n",
       "      <td>ability explicitly</td>\n",
       "      <td>addition pairwise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>344583</td>\n",
       "      <td>195935</td>\n",
       "      <td>assume continous</td>\n",
       "      <td>binary models</td>\n",
       "      <td>combining binary</td>\n",
       "      <td>continous distribution</td>\n",
       "      <td>decisions assume</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>344585</td>\n",
       "      <td>207115</td>\n",
       "      <td>instrumental variables</td>\n",
       "      <td>instrumental</td>\n",
       "      <td>accounting political</td>\n",
       "      <td>author help</td>\n",
       "      <td>economics health</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>344586</td>\n",
       "      <td>206620</td>\n",
       "      <td>fonction</td>\n",
       "      <td>anova fonction</td>\n",
       "      <td>lm fonction</td>\n",
       "      <td>lm</td>\n",
       "      <td>avoid overpramatizing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>344587</td>\n",
       "      <td>207096</td>\n",
       "      <td>probability</td>\n",
       "      <td>probability density</td>\n",
       "      <td>density function</td>\n",
       "      <td>density</td>\n",
       "      <td>function</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>344588</td>\n",
       "      <td>198848</td>\n",
       "      <td>sufficient estimator</td>\n",
       "      <td>bernoulli</td>\n",
       "      <td>binomial distributions</td>\n",
       "      <td>sufficient</td>\n",
       "      <td>textbook</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>344589</td>\n",
       "      <td>206947</td>\n",
       "      <td>spss</td>\n",
       "      <td>come instruction</td>\n",
       "      <td>downloaded plugin</td>\n",
       "      <td>fully motivated</td>\n",
       "      <td>instruction spss</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>344592</td>\n",
       "      <td>96600</td>\n",
       "      <td>area posterior</td>\n",
       "      <td>axis reasonable</td>\n",
       "      <td>direction observed</td>\n",
       "      <td>does unable</td>\n",
       "      <td>effect opposite</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>344593</td>\n",
       "      <td>206570</td>\n",
       "      <td>gain lift</td>\n",
       "      <td>lift used</td>\n",
       "      <td>metrics auc</td>\n",
       "      <td>score gain</td>\n",
       "      <td>different evaluation</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>344594</td>\n",
       "      <td>207118</td>\n",
       "      <td>neurons</td>\n",
       "      <td>basis</td>\n",
       "      <td>inputs</td>\n",
       "      <td>correlations</td>\n",
       "      <td>basing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>344595</td>\n",
       "      <td>230</td>\n",
       "      <td>example unsupervised</td>\n",
       "      <td>unsupervised learning</td>\n",
       "      <td>simple example</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>learning</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>344596</td>\n",
       "      <td>207021</td>\n",
       "      <td>plant</td>\n",
       "      <td>host</td>\n",
       "      <td>dodder</td>\n",
       "      <td>host plant</td>\n",
       "      <td>dodder plant</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>344597</td>\n",
       "      <td>207096</td>\n",
       "      <td>estimating</td>\n",
       "      <td>clustering meta</td>\n",
       "      <td>clusters estimating</td>\n",
       "      <td>density estimating</td>\n",
       "      <td>estimating density</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>344598</td>\n",
       "      <td>207058</td>\n",
       "      <td>series</td>\n",
       "      <td>paper</td>\n",
       "      <td>time series</td>\n",
       "      <td>time</td>\n",
       "      <td>forecasting</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>344599</td>\n",
       "      <td>207096</td>\n",
       "      <td>spaces</td>\n",
       "      <td>probability measures</td>\n",
       "      <td>functions probability</td>\n",
       "      <td>finite dimensional</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>344600</td>\n",
       "      <td>163114</td>\n",
       "      <td>dickey fuller</td>\n",
       "      <td>dickey</td>\n",
       "      <td>fuller</td>\n",
       "      <td>fuller test</td>\n",
       "      <td>test stata</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>344601</td>\n",
       "      <td>207099</td>\n",
       "      <td>possible wilcox</td>\n",
       "      <td>check sample</td>\n",
       "      <td>different possible</td>\n",
       "      <td>wilcox test</td>\n",
       "      <td>sample normal</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>344602</td>\n",
       "      <td>121522</td>\n",
       "      <td>logistic</td>\n",
       "      <td>logistic tag</td>\n",
       "      <td>number relevant</td>\n",
       "      <td>relevant questions</td>\n",
       "      <td>tag number</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>344604</td>\n",
       "      <td>161278</td>\n",
       "      <td>effects limited</td>\n",
       "      <td>existing mean</td>\n",
       "      <td>matrix existing</td>\n",
       "      <td>permanent effect</td>\n",
       "      <td>matter variable</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>344605</td>\n",
       "      <td>207126</td>\n",
       "      <td>pasta</td>\n",
       "      <td>anova tukeys</td>\n",
       "      <td>apologies incorrectly</td>\n",
       "      <td>batches pasta</td>\n",
       "      <td>cooking anova</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>344607</td>\n",
       "      <td>121270</td>\n",
       "      <td>rbms</td>\n",
       "      <td>networks</td>\n",
       "      <td>hopfield nets</td>\n",
       "      <td>hopfield</td>\n",
       "      <td>hinton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>344608</td>\n",
       "      <td>191681</td>\n",
       "      <td>hessian</td>\n",
       "      <td>approximate hessian</td>\n",
       "      <td>dealing hessian</td>\n",
       "      <td>trying approximate</td>\n",
       "      <td>bfgs</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>344609</td>\n",
       "      <td>127538</td>\n",
       "      <td>spikes</td>\n",
       "      <td>tsclean</td>\n",
       "      <td>winsorization</td>\n",
       "      <td>dummy variables</td>\n",
       "      <td>explainable</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>344610</td>\n",
       "      <td>207128</td>\n",
       "      <td>normality</td>\n",
       "      <td>dismiss assumption</td>\n",
       "      <td>know violate</td>\n",
       "      <td>know wilcox</td>\n",
       "      <td>lt dismiss</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19725 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id  user_id                  TopWord1                 TopWord2  \\\n",
       "0       317697   134975   variables statistically             mixed effect   \n",
       "1       317699    45374                  accuracy      validation accuracy   \n",
       "2       317701   171235                   freedom                     mean   \n",
       "3       317703   187797                  patients                 treating   \n",
       "4       317704    61496               uncertainty               error ways   \n",
       "5       317705   134369     bonferroni correction               bonferroni   \n",
       "6       317707   187792   environmental variables                   traits   \n",
       "7       317708    82816        solve differential    equation distribution   \n",
       "8       317710    61092  monotonically increasing            monotonically   \n",
       "9       317711   179343          affect specified             shock affect   \n",
       "10      317712    67137                      simr                able edit   \n",
       "11      317713    67137        measures construct                    power   \n",
       "12      317716   187803        terms applications       difference poisson   \n",
       "13      317717   113090                 graphical         graphical models   \n",
       "14      317718    80922                 recurrent           deep recurrent   \n",
       "15      317719   187804                    person                young old   \n",
       "16      317721   177228              intervention         male respondents   \n",
       "17      317722   187806                      fans                     band   \n",
       "18      317723     8123      distinguishes legged                   legged   \n",
       "19      317724   117168                     block               resampling   \n",
       "20      317725   185581                rows small       corresponding rows   \n",
       "21      317727   117168                     block                   blocks   \n",
       "22      317728   187809                 subscribe                   mining   \n",
       "23      317729    31484         beta distribution                     beta   \n",
       "24      317730   111479               eigenvector                     grey   \n",
       "25      317731      805                      beta                 gaussian   \n",
       "26      317732    88727                    signal                    bound   \n",
       "27      317733   131977            don understand  architecture explaining   \n",
       "28      317734   187810              intervention                   signal   \n",
       "29      317735     8402         denote noncentral           derived theory   \n",
       "...        ...      ...                       ...                      ...   \n",
       "19970   344573    94586              tree species                  species   \n",
       "19971   344574   105149                   college       college highschool   \n",
       "19972   344575   136275                      mean                 use mean   \n",
       "19973   344578   163648            number correct          correct answers   \n",
       "19974   344579   207112               equal equal    distribution variable   \n",
       "19975   344580   195935                     model                     mean   \n",
       "19976   344581   195935      pairwise comparisons                 pairwise   \n",
       "19977   344583   195935          assume continous            binary models   \n",
       "19978   344585   207115    instrumental variables             instrumental   \n",
       "19979   344586   206620                  fonction           anova fonction   \n",
       "19980   344587   207096               probability      probability density   \n",
       "19981   344588   198848      sufficient estimator                bernoulli   \n",
       "19982   344589   206947                      spss         come instruction   \n",
       "19983   344592    96600            area posterior          axis reasonable   \n",
       "19984   344593   206570                 gain lift                lift used   \n",
       "19985   344594   207118                   neurons                    basis   \n",
       "19986   344595      230      example unsupervised    unsupervised learning   \n",
       "19987   344596   207021                     plant                     host   \n",
       "19988   344597   207096                estimating          clustering meta   \n",
       "19989   344598   207058                    series                    paper   \n",
       "19990   344599   207096                    spaces     probability measures   \n",
       "19991   344600   163114             dickey fuller                   dickey   \n",
       "19992   344601   207099           possible wilcox             check sample   \n",
       "19993   344602   121522                  logistic             logistic tag   \n",
       "19994   344604   161278           effects limited            existing mean   \n",
       "19995   344605   207126                     pasta             anova tukeys   \n",
       "19996   344607   121270                      rbms                 networks   \n",
       "19997   344608   191681                   hessian      approximate hessian   \n",
       "19998   344609   127538                    spikes                  tsclean   \n",
       "19999   344610   207128                 normality       dismiss assumption   \n",
       "\n",
       "                     TopWord3                TopWord4  \\\n",
       "0                       mixed               variables   \n",
       "1           training accuracy                 shallow   \n",
       "2             degrees freedom                 degrees   \n",
       "3                       excel                   treat   \n",
       "4          estimate realistic         forecast method   \n",
       "5                  correction                outliers   \n",
       "6               environmental            plant traits   \n",
       "7       differential equation              need solve   \n",
       "8              differentiable              increasing   \n",
       "9            understand shock                    prof   \n",
       "10            deciding sample              edit turns   \n",
       "11                   measures        outcome measures   \n",
       "12              process terms          process markov   \n",
       "13                 log linear                  models   \n",
       "14             neural network        recurrent neural   \n",
       "15                      young                     old   \n",
       "16          intervention time             respondents   \n",
       "17                   universe        overall universe   \n",
       "18                  old style           seek opinions   \n",
       "19                     series                  blocks   \n",
       "20                 constraint            matrix shape   \n",
       "21          block observation                resample   \n",
       "22              brief ideally      customer subscribe   \n",
       "23         distribution prove               look beta   \n",
       "24                 components              eigenvalue   \n",
       "25             symmetric beta               symmetric   \n",
       "26            bound saturated        channel variance   \n",
       "27         concatenate inputs               deep mind   \n",
       "28                     events         external events   \n",
       "29                  don proof           ingersoll ros   \n",
       "...                       ...                     ...   \n",
       "19970                    tree                   crown   \n",
       "19971              highschool          student answer   \n",
       "19972                test set                    test   \n",
       "19973              let number             high school   \n",
       "19974     function parameters          interpretation   \n",
       "19975       application model        baseline renders   \n",
       "19976             comparisons      ability explicitly   \n",
       "19977        combining binary  continous distribution   \n",
       "19978    accounting political             author help   \n",
       "19979             lm fonction                      lm   \n",
       "19980        density function                 density   \n",
       "19981  binomial distributions              sufficient   \n",
       "19982       downloaded plugin         fully motivated   \n",
       "19983      direction observed             does unable   \n",
       "19984             metrics auc              score gain   \n",
       "19985                  inputs            correlations   \n",
       "19986          simple example            unsupervised   \n",
       "19987                  dodder              host plant   \n",
       "19988     clusters estimating      density estimating   \n",
       "19989             time series                    time   \n",
       "19990   functions probability      finite dimensional   \n",
       "19991                  fuller             fuller test   \n",
       "19992      different possible             wilcox test   \n",
       "19993         number relevant      relevant questions   \n",
       "19994         matrix existing        permanent effect   \n",
       "19995   apologies incorrectly           batches pasta   \n",
       "19996           hopfield nets                hopfield   \n",
       "19997         dealing hessian      trying approximate   \n",
       "19998           winsorization         dummy variables   \n",
       "19999            know violate             know wilcox   \n",
       "\n",
       "                        TopWord5  Community  \n",
       "0      statistically significant         42  \n",
       "1                     validation        145  \n",
       "2                    data degree        140  \n",
       "3                  bias treating          1  \n",
       "4                  predicts time          1  \n",
       "5                 don understand        120  \n",
       "6                          plant         92  \n",
       "7               distribution pdf          1  \n",
       "8        assuming differentiable          1  \n",
       "9                          shock          1  \n",
       "10                effect variety         -1  \n",
       "11       multivariate regression        136  \n",
       "12                markov process          1  \n",
       "13                           log        148  \n",
       "14                        neural        102  \n",
       "15                 age indicator        137  \n",
       "16                          male         70  \n",
       "17                        cities          1  \n",
       "18                   recognition          1  \n",
       "19                      resample         50  \n",
       "20                           row          1  \n",
       "21                  fourth block          1  \n",
       "22            incorporate mining          1  \n",
       "23                    prove beta          1  \n",
       "24                     variables         42  \n",
       "25                 beta gaussian          1  \n",
       "26        deriving relationships          1  \n",
       "27              explaining works          1  \n",
       "28                      external         70  \n",
       "29                   proof prove         -1  \n",
       "...                          ...        ...  \n",
       "19970                      lidar        119  \n",
       "19971         correctly question          1  \n",
       "19972                  mean test         17  \n",
       "19973                    let let          1  \n",
       "19974                 parameters        118  \n",
       "19975         building important         16  \n",
       "19976          addition pairwise          1  \n",
       "19977           decisions assume         -1  \n",
       "19978           economics health          1  \n",
       "19979      avoid overpramatizing          1  \n",
       "19980                   function         59  \n",
       "19981                   textbook        118  \n",
       "19982           instruction spss        135  \n",
       "19983            effect opposite         -1  \n",
       "19984       different evaluation         -1  \n",
       "19985                     basing          1  \n",
       "19986                   learning         64  \n",
       "19987               dodder plant        144  \n",
       "19988         estimating density          1  \n",
       "19989                forecasting         50  \n",
       "19990                   discrete          1  \n",
       "19991                 test stata         32  \n",
       "19992              sample normal         26  \n",
       "19993                 tag number        147  \n",
       "19994            matter variable         -1  \n",
       "19995              cooking anova         -1  \n",
       "19996                     hinton          1  \n",
       "19997                       bfgs        118  \n",
       "19998                explainable        133  \n",
       "19999                 lt dismiss        118  \n",
       "\n",
       "[19725 rows x 8 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df_w_comm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community: 0 Size: 87\n",
      "Community: 1 Size: 8343\n",
      "Community: 2 Size: 2\n",
      "Community: 3 Size: 2\n",
      "Community: 4 Size: 2\n",
      "Community: 5 Size: 2\n",
      "Community: 6 Size: 2\n",
      "Community: 7 Size: 2\n",
      "Community: 8 Size: 2\n",
      "Community: 9 Size: 2\n",
      "Community: 10 Size: 2\n",
      "Community: 11 Size: 2\n",
      "Community: 12 Size: 2\n",
      "Community: 13 Size: 2\n",
      "Community: 14 Size: 2\n",
      "Community: 15 Size: 3\n",
      "Community: 16 Size: 169\n",
      "Community: 17 Size: 200\n",
      "Community: 18 Size: 8\n",
      "Community: 19 Size: 7\n",
      "Community: 20 Size: 2\n",
      "Community: 21 Size: 2\n",
      "Community: 22 Size: 2\n",
      "Community: 23 Size: 4\n",
      "Community: 24 Size: 2\n",
      "Community: 25 Size: 2\n",
      "Community: 26 Size: 5\n",
      "Community: 27 Size: 2\n",
      "Community: 28 Size: 2\n",
      "Community: 29 Size: 2\n",
      "Community: 30 Size: 29\n",
      "Community: 31 Size: 2\n",
      "Community: 32 Size: 4\n",
      "Community: 33 Size: 5\n",
      "Community: 34 Size: 2\n",
      "Community: 35 Size: 3\n",
      "Community: 36 Size: 2\n",
      "Community: 37 Size: 2\n",
      "Community: 38 Size: 6\n",
      "Community: 39 Size: 2\n",
      "Community: 40 Size: 2\n",
      "Community: 41 Size: 2\n",
      "Community: 42 Size: 109\n",
      "Community: 43 Size: 65\n",
      "Community: 44 Size: 2\n",
      "Community: 45 Size: 2\n",
      "Community: 46 Size: 2\n",
      "Community: 47 Size: 2\n",
      "Community: 48 Size: 2\n",
      "Community: 49 Size: 2\n",
      "Community: 50 Size: 222\n",
      "Community: 51 Size: 6\n",
      "Community: 52 Size: 4\n",
      "Community: 53 Size: 2\n",
      "Community: 54 Size: 145\n",
      "Community: 55 Size: 6\n",
      "Community: 56 Size: 67\n",
      "Community: 57 Size: 2\n",
      "Community: 58 Size: 114\n",
      "Community: 59 Size: 89\n",
      "Community: 60 Size: 50\n",
      "Community: 61 Size: 26\n",
      "Community: 62 Size: 2\n",
      "Community: 63 Size: 3\n",
      "Community: 64 Size: 73\n",
      "Community: 65 Size: 18\n",
      "Community: 66 Size: 206\n",
      "Community: 67 Size: 114\n",
      "Community: 68 Size: 84\n",
      "Community: 69 Size: 49\n",
      "Community: 70 Size: 42\n",
      "Community: 71 Size: 3\n",
      "Community: 72 Size: 84\n",
      "Community: 73 Size: 87\n",
      "Community: 74 Size: 21\n",
      "Community: 75 Size: 5\n",
      "Community: 76 Size: 58\n",
      "Community: 77 Size: 102\n",
      "Community: 78 Size: 104\n",
      "Community: 79 Size: 3\n",
      "Community: 80 Size: 6\n",
      "Community: 81 Size: 3\n",
      "Community: 82 Size: 53\n",
      "Community: 83 Size: 38\n",
      "Community: 84 Size: 120\n",
      "Community: 85 Size: 2\n",
      "Community: 86 Size: 50\n",
      "Community: 87 Size: 91\n",
      "Community: 88 Size: 79\n",
      "Community: 89 Size: 72\n",
      "Community: 90 Size: 13\n",
      "Community: 91 Size: 55\n",
      "Community: 92 Size: 19\n",
      "Community: 93 Size: 83\n",
      "Community: 94 Size: 8\n",
      "Community: 95 Size: 110\n",
      "Community: 96 Size: 72\n",
      "Community: 97 Size: 60\n",
      "Community: 98 Size: 2\n",
      "Community: 99 Size: 45\n",
      "Community: 100 Size: 128\n",
      "Community: 101 Size: 70\n",
      "Community: 102 Size: 246\n",
      "Community: 103 Size: 68\n",
      "Community: 104 Size: 16\n",
      "Community: 105 Size: 116\n",
      "Community: 106 Size: 27\n",
      "Community: 107 Size: 121\n",
      "Community: 108 Size: 112\n",
      "Community: 109 Size: 132\n",
      "Community: 110 Size: 80\n",
      "Community: 111 Size: 92\n",
      "Community: 112 Size: 6\n",
      "Community: 113 Size: 26\n",
      "Community: 114 Size: 82\n",
      "Community: 115 Size: 27\n",
      "Community: 116 Size: 73\n",
      "Community: 117 Size: 130\n",
      "Community: 118 Size: 1037\n",
      "Community: 119 Size: 106\n",
      "Community: 120 Size: 35\n",
      "Community: 121 Size: 55\n",
      "Community: 122 Size: 130\n",
      "Community: 123 Size: 184\n",
      "Community: 124 Size: 90\n",
      "Community: 125 Size: 48\n",
      "Community: 126 Size: 42\n",
      "Community: 127 Size: 77\n",
      "Community: 128 Size: 25\n",
      "Community: 129 Size: 42\n",
      "Community: 130 Size: 90\n",
      "Community: 131 Size: 88\n",
      "Community: 132 Size: 35\n",
      "Community: 133 Size: 44\n",
      "Community: 134 Size: 127\n",
      "Community: 135 Size: 180\n",
      "Community: 136 Size: 27\n",
      "Community: 137 Size: 175\n",
      "Community: 138 Size: 48\n",
      "Community: 139 Size: 5\n",
      "Community: 140 Size: 210\n",
      "Community: 141 Size: 34\n",
      "Community: 142 Size: 2\n",
      "Community: 143 Size: 18\n",
      "Community: 144 Size: 81\n",
      "Community: 145 Size: 301\n",
      "Community: 146 Size: 122\n",
      "Community: 147 Size: 156\n",
      "Community: 148 Size: 48\n",
      "Community: 149 Size: 38\n",
      "Community: -1 Size: 2586\n"
     ]
    }
   ],
   "source": [
    "communities2 = set(post_df_w_comm2[\"Community\"])\n",
    "for comm in communities2:\n",
    "    print \"Community:\", comm, \"Size:\", len(post_df[post_df_w_comm2[\"Community\"] == comm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency in communities2\n",
    "word_counts2 = collections.defaultdict(dict)\n",
    "word_freqs2 = collections.defaultdict(dict)\n",
    "for comm in communities2:\n",
    "    total_words = 0.0\n",
    "    word_counts2[comm] = collections.defaultdict(int)\n",
    "    for word in post_df_w_comm2[post_df_w_comm2[\"Community\"] == comm]['TopWord1']:\n",
    "        word_counts2[comm][word] += 1\n",
    "        total_words += 1\n",
    "    for word in post_df_w_comm2[post_df_w_comm2[\"Community\"] == comm]['TopWord2']:\n",
    "        word_counts2[comm][word] += 1\n",
    "        total_words += 1\n",
    "    for word in post_df_w_comm2[post_df_w_comm2[\"Community\"] == comm]['TopWord3']:\n",
    "        word_counts2[comm][word] += 1\n",
    "        total_words += 1\n",
    "    for word in post_df_w_comm2[post_df_w_comm2[\"Community\"] == comm]['TopWord4']:\n",
    "        word_counts2[comm][word] += 1\n",
    "        total_words += 1\n",
    "    for word in post_df_w_comm2[post_df_w_comm2[\"Community\"] == comm]['TopWord5']:\n",
    "        word_counts2[comm][word] += 1\n",
    "        total_words += 1\n",
    "    for word in word_counts2[comm]:\n",
    "        word_freqs2[comm][word] = word_counts2[comm][word] / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community:   0  Size:    87  Label: likelihood|log likelihood|likelihood function\n",
      "Community:   1  Size:  8343  Label: feature|dataset|value\n",
      "Community:   2  Size:     2  Label: stuck|suppose hypotheses|able stuck\n",
      "Community:   3  Size:     2  Label: structure coefficients|corville|additionally sure\n",
      "Community:   4  Size:     2  Label: geo|thrown getting|coin thrown\n",
      "Community:   5  Size:     2  Label: advice|functions image|variance group\n",
      "Community:   6  Size:     2  Label: convolution quite|quite nicely|nicely different\n",
      "Community:   7  Size:     2  Label: nrow|partykit|matrix nrow\n",
      "Community:   8  Size:     2  Label: result general|general finally|methods ok\n",
      "Community:   9  Size:     2  Label: covariance variable|equivalent variance|equals variance\n",
      "Community:  10  Size:     2  Label: donors|lapsed|frequency donations\n",
      "Community:  11  Size:     2  Label: distributed random|having exponential|convergeges\n",
      "Community:  12  Size:     2  Label: grain|mineral|assume grain\n",
      "Community:  13  Size:     2  Label: tso|lets nn1|tso function\n",
      "Community:  14  Size:     2  Label: explain does|estimation come|does maximum\n",
      "Community:  15  Size:     3  Label: rcpp|loops|current value\n",
      "Community:  16  Size:   169  Label: model|fit|validation\n",
      "Community:  17  Size:   200  Label: test|test set|test statistic\n",
      "Community:  18  Size:     8  Label: let|mean bu|continued strictly\n",
      "Community:  19  Size:     7  Label: near|ratio near|convergence deeper\n",
      "Community:  20  Size:     2  Label: factor1|factor2|according factor2\n",
      "Community:  21  Size:     2  Label: key|variable placed|current key\n",
      "Community:  22  Size:     2  Label: rmsep|lowest rmsep|command validation\n",
      "Community:  23  Size:     4  Label: orange|vitamin|linux\n",
      "Community:  24  Size:     2  Label: tax|property tax|tax size\n",
      "Community:  25  Size:     2  Label: km curve|mean km|correct job\n",
      "Community:  26  Size:     5  Label: wilcox|wilcox test|sums version\n",
      "Community:  27  Size:     2  Label: tests sampling|assumptions large|assumptions amp\n",
      "Community:  28  Size:     2  Label: map2|change map2|argument change\n",
      "Community:  29  Size:     2  Label: ip|ip port|wq\n",
      "Community:  30  Size:    29  Label: inequality|chebyshev|chebyshev inequality\n",
      "Community:  31  Size:     2  Label: variable following|distribution suppose|property probability\n",
      "Community:  32  Size:     4  Label: dickey fuller|fuller|dickey\n",
      "Community:  33  Size:     5  Label: https github|github com|com\n",
      "Community:  34  Size:     2  Label: ed|distributions ed|budgetary\n",
      "Community:  35  Size:     3  Label: nest|common materials|lastalive\n",
      "Community:  36  Size:     2  Label: armax|actually end|armax model\n",
      "Community:  37  Size:     2  Label: independent measures|anova scores|violating\n",
      "Community:  38  Size:     6  Label: derive|know derive|intercept simple\n",
      "Community:  39  Size:     2  Label: case increase|given increasing|path says\n",
      "Community:  40  Size:     2  Label: realistic|realistic subjects|exponentiate result\n",
      "Community:  41  Size:     2  Label: consultation|care treatment|bothering consult\n",
      "Community:  42  Size:   109  Label: variables|categorical variables|categorical\n",
      "Community:  43  Size:    65  Label: arima|arima model|seasonal\n",
      "Community:  44  Size:     2  Label: effects logistic|idre|factor mixed\n",
      "Community:  45  Size:     2  Label: subscript|gamma just|operator parentheses\n",
      "Community:  46  Size:     2  Label: group comparison|trt2|comparison procedure\n",
      "Community:  47  Size:     2  Label: berry esseen|berry|esseen\n",
      "Community:  48  Size:     2  Label: wondering value|know place|place firstly\n",
      "Community:  49  Size:     2  Label: random component|component actually|complex stochastic\n",
      "Community:  50  Size:   222  Label: series|time|time series\n",
      "Community:  51  Size:     6  Label: dots|slid|following plot\n",
      "Community:  52  Size:     4  Label: electricity|electricity used|homes\n",
      "Community:  53  Size:     2  Label: coefficient simple|work difference|coefficients refer\n",
      "Community:  54  Size:   145  Label: matrix|covariance matrix|covariance\n",
      "Community:  55  Size:     6  Label: religion|islam|religiosity\n",
      "Community:  56  Size:    67  Label: kernel|kernels|density\n",
      "Community:  57  Size:     2  Label: samples groups|error anova|groups similarities\n",
      "Community:  58  Size:   114  Label: group|treatment|groups\n",
      "Community:  59  Size:    89  Label: probability|conditional probability|probability density\n",
      "Community:  60  Size:    50  Label: year|year year|years\n",
      "Community:  61  Size:    26  Label: income|person years|dollar\n",
      "Community:  62  Size:     2  Label: condition area|anova condition|concentration subject\n",
      "Community:  63  Size:     3  Label: hypertension|history family|having headache\n",
      "Community:  64  Size:    73  Label: learning|deep learning|rate\n",
      "Community:  65  Size:    18  Label: statement|htm|vertical profile\n",
      "Community:  66  Size:   206  Label: distribution|normal distribution|probability\n",
      "Community:  67  Size:   114  Label: correlation|pearson|pearson correlation\n",
      "Community:  68  Size:    84  Label: items|item|user\n",
      "Community:  69  Size:    49  Label: entropy|cross entropy|joint\n",
      "Community:  70  Size:    42  Label: intervention|standard deviation|covariates\n",
      "Community:  71  Size:     3  Label: intuitive|grasph like|intuitive explanation\n",
      "Community:  72  Size:    84  Label: variable|random|random variable\n",
      "Community:  73  Size:    87  Label: variance|sample variance|sample\n",
      "Community:  74  Size:    21  Label: pattern|agent|falling\n",
      "Community:  75  Size:     5  Label: aggregated|attacks|mcfadden\n",
      "Community:  76  Size:    58  Label: age|mother|gender\n",
      "Community:  77  Size:   102  Label: confidence|interval|confidence interval\n",
      "Community:  78  Size:   104  Label: class|classes|class class\n",
      "Community:  79  Size:     3  Label: beta3|decaying|implicit conditions\n",
      "Community:  80  Size:     6  Label: chapter|chapter gmm|containing chapter\n",
      "Community:  81  Size:     3  Label: payment|booking|booking cancelled\n",
      "Community:  82  Size:    53  Label: survival|hazard|survival rate\n",
      "Community:  83  Size:    38  Label: integral|density|substitution\n",
      "Community:  84  Size:   120  Label: layer|layers|hidden\n",
      "Community:  85  Size:     2  Label: attention|attention based|using attention\n",
      "Community:  86  Size:    50  Label: factor|factors|factor analysis\n",
      "Community:  87  Size:    91  Label: amp|amp amp|frac\n",
      "Community:  88  Size:    79  Label: cell|convolution|memory\n",
      "Community:  89  Size:    72  Label: prior|posterior|priors\n",
      "Community:  90  Size:    13  Label: minutes|average mpg|people\n",
      "Community:  91  Size:    55  Label: pca|robust pca|robust\n",
      "Community:  92  Size:    19  Label: course|environmental|traits\n",
      "Community:  93  Size:    83  Label: estimator|unbiased|ols\n",
      "Community:  94  Size:     8  Label: tutorial|hope helps|definite model\n",
      "Community:  95  Size:   110  Label: features|feature|feature selection\n",
      "Community:  96  Size:    72  Label: roc|auc|curve\n",
      "Community:  97  Size:    60  Label: lasso|ridge|ridge regression\n",
      "Community:  98  Size:     2  Label: using boruta|meaning interpret|correctly feature\n",
      "Community:  99  Size:    45  Label: red|balls|ball\n",
      "Community: 100  Size:   128  Label: clusters|cluster|clustering\n",
      "Community: 101  Size:    70  Label: gradient|descent|gradient descent\n",
      "Community: 102  Size:   246  Label: network|cost|event\n",
      "Community: 103  Size:    68  Label: sequence|convergence|convergence probability\n",
      "Community: 104  Size:    16  Label: audio|recorded|sound\n",
      "Community: 105  Size:   116  Label: error|standard|standard error\n",
      "Community: 106  Size:    27  Label: logit|logit model|conditional logit\n",
      "Community: 107  Size:   121  Label: values|missing|imputation\n",
      "Community: 108  Size:   112  Label: treatment|treatments|control\n",
      "Community: 109  Size:   132  Label: state|policy|action\n",
      "Community: 110  Size:    80  Label: effect|effect size|size\n",
      "Community: 111  Size:    92  Label: image|images|cnn\n",
      "Community: 112  Size:     6  Label: understand|proceed|paper understand\n",
      "Community: 113  Size:    26  Label: monte|monte carlo|carlo\n",
      "Community: 114  Size:    82  Label: loss|loss function|reconstruction\n",
      "Community: 115  Size:    27  Label: customer|customers|ltv\n",
      "Community: 116  Size:    73  Label: odds|odds ratio|ratio\n",
      "Community: 117  Size:   130  Label: frac|lambda|hat\n",
      "Community: 118  Size:  1037  Label: density|normal|parameter\n",
      "Community: 119  Size:   106  Label: tree|forest|trees\n",
      "Community: 120  Size:    35  Label: correction|bonferroni|yates correction\n",
      "Community: 121  Size:    55  Label: sales|stores|store\n",
      "Community: 122  Size:   130  Label: word|words|embedding\n",
      "Community: 123  Size:   184  Label: stationary|process|acf\n",
      "Community: 124  Size:    90  Label: day|week|days\n",
      "Community: 125  Size:    48  Label: anova|table|iv2\n",
      "Community: 126  Size:    42  Label: bayes|naive bayes|naive\n",
      "Community: 127  Size:    77  Label: effects|random effects|fixed effects\n",
      "Community: 128  Size:    25  Label: lt|lt lt|ci\n",
      "Community: 129  Size:    42  Label: independent|independent variable|independent variables\n",
      "Community: 130  Size:    90  Label: measurements|measurement|stan\n",
      "Community: 131  Size:    88  Label: equation|begin equation|end equation\n",
      "Community: 132  Size:    35  Label: outliers|outlier|seasonal\n",
      "Community: 133  Size:    44  Label: dummy|dummy variables|dummies\n",
      "Community: 134  Size:   127  Label: score|game|team\n",
      "Community: 135  Size:   180  Label: distributions|aic|gamma\n",
      "Community: 136  Size:    27  Label: power|power analysis|effect size\n",
      "Community: 137  Size:   175  Label: weights|interaction|level\n",
      "Community: 138  Size:    48  Label: square|chi square|goodness fit\n",
      "Community: 139  Size:     5  Label: predator|prey|aquatic\n",
      "Community: 140  Size:   210  Label: sample|population|mean\n",
      "Community: 141  Size:    34  Label: meta|meta analysis|studies\n",
      "Community: 142  Size:     2  Label: unclear vague|scope use|tags instead\n",
      "Community: 143  Size:    18  Label: proof|ideas|subset\n",
      "Community: 144  Size:    81  Label: distance|species|abundance\n",
      "Community: 145  Size:   301  Label: training|validation|data\n",
      "Community: 146  Size:   122  Label: points|line|mse\n",
      "Community: 147  Size:   156  Label: regression|logistic|ordinal\n",
      "Community: 148  Size:    48  Label: log|log normal|log log\n",
      "Community: 149  Size:    38  Label: heads|coin|fair\n",
      "Community:  -1  Size:  2586  Label: formula relation|account everybody|leverage observations\n"
     ]
    }
   ],
   "source": [
    "sorted_word_freqs2 = collections.defaultdict(dict)\n",
    "for comm in word_freqs2:\n",
    "    sorted_word_freqs2[comm] = sorted(word_freqs2[comm].items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "community_labels2 = collections.defaultdict(str) \n",
    "\n",
    "for ind, comm in enumerate(sorted_word_freqs2):\n",
    "    community_labels2[comm] = \"{}|{}|{}\".format(sorted_word_freqs2[comm][0][0], sorted_word_freqs2[comm][1][0], sorted_word_freqs2[comm][2][0])\n",
    "\n",
    "for comm in communities2:\n",
    "    print \"Community: {:3}  Size: {:5}  Label: {}\".format(comm, len(post_df[post_df_w_comm2[\"Community\"] == comm]), community_labels2[comm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>TopWord1</th>\n",
       "      <th>TopWord2</th>\n",
       "      <th>TopWord3</th>\n",
       "      <th>TopWord4</th>\n",
       "      <th>TopWord5</th>\n",
       "      <th>Community</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>317697</td>\n",
       "      <td>134975</td>\n",
       "      <td>variables statistically</td>\n",
       "      <td>mixed effect</td>\n",
       "      <td>mixed</td>\n",
       "      <td>variables</td>\n",
       "      <td>statistically significant</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>317699</td>\n",
       "      <td>45374</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>validation accuracy</td>\n",
       "      <td>training accuracy</td>\n",
       "      <td>shallow</td>\n",
       "      <td>validation</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>317701</td>\n",
       "      <td>171235</td>\n",
       "      <td>freedom</td>\n",
       "      <td>mean</td>\n",
       "      <td>degrees freedom</td>\n",
       "      <td>degrees</td>\n",
       "      <td>data degree</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>317703</td>\n",
       "      <td>187797</td>\n",
       "      <td>patients</td>\n",
       "      <td>treating</td>\n",
       "      <td>excel</td>\n",
       "      <td>treat</td>\n",
       "      <td>bias treating</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317704</td>\n",
       "      <td>61496</td>\n",
       "      <td>uncertainty</td>\n",
       "      <td>error ways</td>\n",
       "      <td>estimate realistic</td>\n",
       "      <td>forecast method</td>\n",
       "      <td>predicts time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>317705</td>\n",
       "      <td>134369</td>\n",
       "      <td>bonferroni correction</td>\n",
       "      <td>bonferroni</td>\n",
       "      <td>correction</td>\n",
       "      <td>outliers</td>\n",
       "      <td>don understand</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>317707</td>\n",
       "      <td>187792</td>\n",
       "      <td>environmental variables</td>\n",
       "      <td>traits</td>\n",
       "      <td>environmental</td>\n",
       "      <td>plant traits</td>\n",
       "      <td>plant</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>317708</td>\n",
       "      <td>82816</td>\n",
       "      <td>solve differential</td>\n",
       "      <td>equation distribution</td>\n",
       "      <td>differential equation</td>\n",
       "      <td>need solve</td>\n",
       "      <td>distribution pdf</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>317710</td>\n",
       "      <td>61092</td>\n",
       "      <td>monotonically increasing</td>\n",
       "      <td>monotonically</td>\n",
       "      <td>differentiable</td>\n",
       "      <td>increasing</td>\n",
       "      <td>assuming differentiable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>317711</td>\n",
       "      <td>179343</td>\n",
       "      <td>affect specified</td>\n",
       "      <td>shock affect</td>\n",
       "      <td>understand shock</td>\n",
       "      <td>prof</td>\n",
       "      <td>shock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>317712</td>\n",
       "      <td>67137</td>\n",
       "      <td>simr</td>\n",
       "      <td>able edit</td>\n",
       "      <td>deciding sample</td>\n",
       "      <td>edit turns</td>\n",
       "      <td>effect variety</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>317713</td>\n",
       "      <td>67137</td>\n",
       "      <td>measures construct</td>\n",
       "      <td>power</td>\n",
       "      <td>measures</td>\n",
       "      <td>outcome measures</td>\n",
       "      <td>multivariate regression</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>317716</td>\n",
       "      <td>187803</td>\n",
       "      <td>terms applications</td>\n",
       "      <td>difference poisson</td>\n",
       "      <td>process terms</td>\n",
       "      <td>process markov</td>\n",
       "      <td>markov process</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>317717</td>\n",
       "      <td>113090</td>\n",
       "      <td>graphical</td>\n",
       "      <td>graphical models</td>\n",
       "      <td>log linear</td>\n",
       "      <td>models</td>\n",
       "      <td>log</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>317718</td>\n",
       "      <td>80922</td>\n",
       "      <td>recurrent</td>\n",
       "      <td>deep recurrent</td>\n",
       "      <td>neural network</td>\n",
       "      <td>recurrent neural</td>\n",
       "      <td>neural</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>317719</td>\n",
       "      <td>187804</td>\n",
       "      <td>person</td>\n",
       "      <td>young old</td>\n",
       "      <td>young</td>\n",
       "      <td>old</td>\n",
       "      <td>age indicator</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>317721</td>\n",
       "      <td>177228</td>\n",
       "      <td>intervention</td>\n",
       "      <td>male respondents</td>\n",
       "      <td>intervention time</td>\n",
       "      <td>respondents</td>\n",
       "      <td>male</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>317722</td>\n",
       "      <td>187806</td>\n",
       "      <td>fans</td>\n",
       "      <td>band</td>\n",
       "      <td>universe</td>\n",
       "      <td>overall universe</td>\n",
       "      <td>cities</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>317723</td>\n",
       "      <td>8123</td>\n",
       "      <td>distinguishes legged</td>\n",
       "      <td>legged</td>\n",
       "      <td>old style</td>\n",
       "      <td>seek opinions</td>\n",
       "      <td>recognition</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>317724</td>\n",
       "      <td>117168</td>\n",
       "      <td>block</td>\n",
       "      <td>resampling</td>\n",
       "      <td>series</td>\n",
       "      <td>blocks</td>\n",
       "      <td>resample</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>317725</td>\n",
       "      <td>185581</td>\n",
       "      <td>rows small</td>\n",
       "      <td>corresponding rows</td>\n",
       "      <td>constraint</td>\n",
       "      <td>matrix shape</td>\n",
       "      <td>row</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>317727</td>\n",
       "      <td>117168</td>\n",
       "      <td>block</td>\n",
       "      <td>blocks</td>\n",
       "      <td>block observation</td>\n",
       "      <td>resample</td>\n",
       "      <td>fourth block</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>317728</td>\n",
       "      <td>187809</td>\n",
       "      <td>subscribe</td>\n",
       "      <td>mining</td>\n",
       "      <td>brief ideally</td>\n",
       "      <td>customer subscribe</td>\n",
       "      <td>incorporate mining</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>317729</td>\n",
       "      <td>31484</td>\n",
       "      <td>beta distribution</td>\n",
       "      <td>beta</td>\n",
       "      <td>distribution prove</td>\n",
       "      <td>look beta</td>\n",
       "      <td>prove beta</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>317730</td>\n",
       "      <td>111479</td>\n",
       "      <td>eigenvector</td>\n",
       "      <td>grey</td>\n",
       "      <td>components</td>\n",
       "      <td>eigenvalue</td>\n",
       "      <td>variables</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>317731</td>\n",
       "      <td>805</td>\n",
       "      <td>beta</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>symmetric beta</td>\n",
       "      <td>symmetric</td>\n",
       "      <td>beta gaussian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>317732</td>\n",
       "      <td>88727</td>\n",
       "      <td>signal</td>\n",
       "      <td>bound</td>\n",
       "      <td>bound saturated</td>\n",
       "      <td>channel variance</td>\n",
       "      <td>deriving relationships</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>317733</td>\n",
       "      <td>131977</td>\n",
       "      <td>don understand</td>\n",
       "      <td>architecture explaining</td>\n",
       "      <td>concatenate inputs</td>\n",
       "      <td>deep mind</td>\n",
       "      <td>explaining works</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>317734</td>\n",
       "      <td>187810</td>\n",
       "      <td>intervention</td>\n",
       "      <td>signal</td>\n",
       "      <td>events</td>\n",
       "      <td>external events</td>\n",
       "      <td>external</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>317735</td>\n",
       "      <td>8402</td>\n",
       "      <td>denote noncentral</td>\n",
       "      <td>derived theory</td>\n",
       "      <td>don proof</td>\n",
       "      <td>ingersoll ros</td>\n",
       "      <td>proof prove</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>344573</td>\n",
       "      <td>94586</td>\n",
       "      <td>tree species</td>\n",
       "      <td>species</td>\n",
       "      <td>tree</td>\n",
       "      <td>crown</td>\n",
       "      <td>lidar</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>344574</td>\n",
       "      <td>105149</td>\n",
       "      <td>college</td>\n",
       "      <td>college highschool</td>\n",
       "      <td>highschool</td>\n",
       "      <td>student answer</td>\n",
       "      <td>correctly question</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>344575</td>\n",
       "      <td>136275</td>\n",
       "      <td>mean</td>\n",
       "      <td>use mean</td>\n",
       "      <td>test set</td>\n",
       "      <td>test</td>\n",
       "      <td>mean test</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>344578</td>\n",
       "      <td>163648</td>\n",
       "      <td>number correct</td>\n",
       "      <td>correct answers</td>\n",
       "      <td>let number</td>\n",
       "      <td>high school</td>\n",
       "      <td>let let</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>344579</td>\n",
       "      <td>207112</td>\n",
       "      <td>equal equal</td>\n",
       "      <td>distribution variable</td>\n",
       "      <td>function parameters</td>\n",
       "      <td>interpretation</td>\n",
       "      <td>parameters</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>344580</td>\n",
       "      <td>195935</td>\n",
       "      <td>model</td>\n",
       "      <td>mean</td>\n",
       "      <td>application model</td>\n",
       "      <td>baseline renders</td>\n",
       "      <td>building important</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>344581</td>\n",
       "      <td>195935</td>\n",
       "      <td>pairwise comparisons</td>\n",
       "      <td>pairwise</td>\n",
       "      <td>comparisons</td>\n",
       "      <td>ability explicitly</td>\n",
       "      <td>addition pairwise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>344583</td>\n",
       "      <td>195935</td>\n",
       "      <td>assume continous</td>\n",
       "      <td>binary models</td>\n",
       "      <td>combining binary</td>\n",
       "      <td>continous distribution</td>\n",
       "      <td>decisions assume</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>344585</td>\n",
       "      <td>207115</td>\n",
       "      <td>instrumental variables</td>\n",
       "      <td>instrumental</td>\n",
       "      <td>accounting political</td>\n",
       "      <td>author help</td>\n",
       "      <td>economics health</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>344586</td>\n",
       "      <td>206620</td>\n",
       "      <td>fonction</td>\n",
       "      <td>anova fonction</td>\n",
       "      <td>lm fonction</td>\n",
       "      <td>lm</td>\n",
       "      <td>avoid overpramatizing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>344587</td>\n",
       "      <td>207096</td>\n",
       "      <td>probability</td>\n",
       "      <td>probability density</td>\n",
       "      <td>density function</td>\n",
       "      <td>density</td>\n",
       "      <td>function</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>344588</td>\n",
       "      <td>198848</td>\n",
       "      <td>sufficient estimator</td>\n",
       "      <td>bernoulli</td>\n",
       "      <td>binomial distributions</td>\n",
       "      <td>sufficient</td>\n",
       "      <td>textbook</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>344589</td>\n",
       "      <td>206947</td>\n",
       "      <td>spss</td>\n",
       "      <td>come instruction</td>\n",
       "      <td>downloaded plugin</td>\n",
       "      <td>fully motivated</td>\n",
       "      <td>instruction spss</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>344592</td>\n",
       "      <td>96600</td>\n",
       "      <td>area posterior</td>\n",
       "      <td>axis reasonable</td>\n",
       "      <td>direction observed</td>\n",
       "      <td>does unable</td>\n",
       "      <td>effect opposite</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>344593</td>\n",
       "      <td>206570</td>\n",
       "      <td>gain lift</td>\n",
       "      <td>lift used</td>\n",
       "      <td>metrics auc</td>\n",
       "      <td>score gain</td>\n",
       "      <td>different evaluation</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>344594</td>\n",
       "      <td>207118</td>\n",
       "      <td>neurons</td>\n",
       "      <td>basis</td>\n",
       "      <td>inputs</td>\n",
       "      <td>correlations</td>\n",
       "      <td>basing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>344595</td>\n",
       "      <td>230</td>\n",
       "      <td>example unsupervised</td>\n",
       "      <td>unsupervised learning</td>\n",
       "      <td>simple example</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>learning</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>344596</td>\n",
       "      <td>207021</td>\n",
       "      <td>plant</td>\n",
       "      <td>host</td>\n",
       "      <td>dodder</td>\n",
       "      <td>host plant</td>\n",
       "      <td>dodder plant</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>344597</td>\n",
       "      <td>207096</td>\n",
       "      <td>estimating</td>\n",
       "      <td>clustering meta</td>\n",
       "      <td>clusters estimating</td>\n",
       "      <td>density estimating</td>\n",
       "      <td>estimating density</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>344598</td>\n",
       "      <td>207058</td>\n",
       "      <td>series</td>\n",
       "      <td>paper</td>\n",
       "      <td>time series</td>\n",
       "      <td>time</td>\n",
       "      <td>forecasting</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>344599</td>\n",
       "      <td>207096</td>\n",
       "      <td>spaces</td>\n",
       "      <td>probability measures</td>\n",
       "      <td>functions probability</td>\n",
       "      <td>finite dimensional</td>\n",
       "      <td>discrete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>344600</td>\n",
       "      <td>163114</td>\n",
       "      <td>dickey fuller</td>\n",
       "      <td>dickey</td>\n",
       "      <td>fuller</td>\n",
       "      <td>fuller test</td>\n",
       "      <td>test stata</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>344601</td>\n",
       "      <td>207099</td>\n",
       "      <td>possible wilcox</td>\n",
       "      <td>check sample</td>\n",
       "      <td>different possible</td>\n",
       "      <td>wilcox test</td>\n",
       "      <td>sample normal</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>344602</td>\n",
       "      <td>121522</td>\n",
       "      <td>logistic</td>\n",
       "      <td>logistic tag</td>\n",
       "      <td>number relevant</td>\n",
       "      <td>relevant questions</td>\n",
       "      <td>tag number</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>344604</td>\n",
       "      <td>161278</td>\n",
       "      <td>effects limited</td>\n",
       "      <td>existing mean</td>\n",
       "      <td>matrix existing</td>\n",
       "      <td>permanent effect</td>\n",
       "      <td>matter variable</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>344605</td>\n",
       "      <td>207126</td>\n",
       "      <td>pasta</td>\n",
       "      <td>anova tukeys</td>\n",
       "      <td>apologies incorrectly</td>\n",
       "      <td>batches pasta</td>\n",
       "      <td>cooking anova</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>344607</td>\n",
       "      <td>121270</td>\n",
       "      <td>rbms</td>\n",
       "      <td>networks</td>\n",
       "      <td>hopfield nets</td>\n",
       "      <td>hopfield</td>\n",
       "      <td>hinton</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>344608</td>\n",
       "      <td>191681</td>\n",
       "      <td>hessian</td>\n",
       "      <td>approximate hessian</td>\n",
       "      <td>dealing hessian</td>\n",
       "      <td>trying approximate</td>\n",
       "      <td>bfgs</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>344609</td>\n",
       "      <td>127538</td>\n",
       "      <td>spikes</td>\n",
       "      <td>tsclean</td>\n",
       "      <td>winsorization</td>\n",
       "      <td>dummy variables</td>\n",
       "      <td>explainable</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>344610</td>\n",
       "      <td>207128</td>\n",
       "      <td>normality</td>\n",
       "      <td>dismiss assumption</td>\n",
       "      <td>know violate</td>\n",
       "      <td>know wilcox</td>\n",
       "      <td>lt dismiss</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19725 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id  user_id                  TopWord1                 TopWord2  \\\n",
       "0       317697   134975   variables statistically             mixed effect   \n",
       "1       317699    45374                  accuracy      validation accuracy   \n",
       "2       317701   171235                   freedom                     mean   \n",
       "3       317703   187797                  patients                 treating   \n",
       "4       317704    61496               uncertainty               error ways   \n",
       "5       317705   134369     bonferroni correction               bonferroni   \n",
       "6       317707   187792   environmental variables                   traits   \n",
       "7       317708    82816        solve differential    equation distribution   \n",
       "8       317710    61092  monotonically increasing            monotonically   \n",
       "9       317711   179343          affect specified             shock affect   \n",
       "10      317712    67137                      simr                able edit   \n",
       "11      317713    67137        measures construct                    power   \n",
       "12      317716   187803        terms applications       difference poisson   \n",
       "13      317717   113090                 graphical         graphical models   \n",
       "14      317718    80922                 recurrent           deep recurrent   \n",
       "15      317719   187804                    person                young old   \n",
       "16      317721   177228              intervention         male respondents   \n",
       "17      317722   187806                      fans                     band   \n",
       "18      317723     8123      distinguishes legged                   legged   \n",
       "19      317724   117168                     block               resampling   \n",
       "20      317725   185581                rows small       corresponding rows   \n",
       "21      317727   117168                     block                   blocks   \n",
       "22      317728   187809                 subscribe                   mining   \n",
       "23      317729    31484         beta distribution                     beta   \n",
       "24      317730   111479               eigenvector                     grey   \n",
       "25      317731      805                      beta                 gaussian   \n",
       "26      317732    88727                    signal                    bound   \n",
       "27      317733   131977            don understand  architecture explaining   \n",
       "28      317734   187810              intervention                   signal   \n",
       "29      317735     8402         denote noncentral           derived theory   \n",
       "...        ...      ...                       ...                      ...   \n",
       "19970   344573    94586              tree species                  species   \n",
       "19971   344574   105149                   college       college highschool   \n",
       "19972   344575   136275                      mean                 use mean   \n",
       "19973   344578   163648            number correct          correct answers   \n",
       "19974   344579   207112               equal equal    distribution variable   \n",
       "19975   344580   195935                     model                     mean   \n",
       "19976   344581   195935      pairwise comparisons                 pairwise   \n",
       "19977   344583   195935          assume continous            binary models   \n",
       "19978   344585   207115    instrumental variables             instrumental   \n",
       "19979   344586   206620                  fonction           anova fonction   \n",
       "19980   344587   207096               probability      probability density   \n",
       "19981   344588   198848      sufficient estimator                bernoulli   \n",
       "19982   344589   206947                      spss         come instruction   \n",
       "19983   344592    96600            area posterior          axis reasonable   \n",
       "19984   344593   206570                 gain lift                lift used   \n",
       "19985   344594   207118                   neurons                    basis   \n",
       "19986   344595      230      example unsupervised    unsupervised learning   \n",
       "19987   344596   207021                     plant                     host   \n",
       "19988   344597   207096                estimating          clustering meta   \n",
       "19989   344598   207058                    series                    paper   \n",
       "19990   344599   207096                    spaces     probability measures   \n",
       "19991   344600   163114             dickey fuller                   dickey   \n",
       "19992   344601   207099           possible wilcox             check sample   \n",
       "19993   344602   121522                  logistic             logistic tag   \n",
       "19994   344604   161278           effects limited            existing mean   \n",
       "19995   344605   207126                     pasta             anova tukeys   \n",
       "19996   344607   121270                      rbms                 networks   \n",
       "19997   344608   191681                   hessian      approximate hessian   \n",
       "19998   344609   127538                    spikes                  tsclean   \n",
       "19999   344610   207128                 normality       dismiss assumption   \n",
       "\n",
       "                     TopWord3                TopWord4  \\\n",
       "0                       mixed               variables   \n",
       "1           training accuracy                 shallow   \n",
       "2             degrees freedom                 degrees   \n",
       "3                       excel                   treat   \n",
       "4          estimate realistic         forecast method   \n",
       "5                  correction                outliers   \n",
       "6               environmental            plant traits   \n",
       "7       differential equation              need solve   \n",
       "8              differentiable              increasing   \n",
       "9            understand shock                    prof   \n",
       "10            deciding sample              edit turns   \n",
       "11                   measures        outcome measures   \n",
       "12              process terms          process markov   \n",
       "13                 log linear                  models   \n",
       "14             neural network        recurrent neural   \n",
       "15                      young                     old   \n",
       "16          intervention time             respondents   \n",
       "17                   universe        overall universe   \n",
       "18                  old style           seek opinions   \n",
       "19                     series                  blocks   \n",
       "20                 constraint            matrix shape   \n",
       "21          block observation                resample   \n",
       "22              brief ideally      customer subscribe   \n",
       "23         distribution prove               look beta   \n",
       "24                 components              eigenvalue   \n",
       "25             symmetric beta               symmetric   \n",
       "26            bound saturated        channel variance   \n",
       "27         concatenate inputs               deep mind   \n",
       "28                     events         external events   \n",
       "29                  don proof           ingersoll ros   \n",
       "...                       ...                     ...   \n",
       "19970                    tree                   crown   \n",
       "19971              highschool          student answer   \n",
       "19972                test set                    test   \n",
       "19973              let number             high school   \n",
       "19974     function parameters          interpretation   \n",
       "19975       application model        baseline renders   \n",
       "19976             comparisons      ability explicitly   \n",
       "19977        combining binary  continous distribution   \n",
       "19978    accounting political             author help   \n",
       "19979             lm fonction                      lm   \n",
       "19980        density function                 density   \n",
       "19981  binomial distributions              sufficient   \n",
       "19982       downloaded plugin         fully motivated   \n",
       "19983      direction observed             does unable   \n",
       "19984             metrics auc              score gain   \n",
       "19985                  inputs            correlations   \n",
       "19986          simple example            unsupervised   \n",
       "19987                  dodder              host plant   \n",
       "19988     clusters estimating      density estimating   \n",
       "19989             time series                    time   \n",
       "19990   functions probability      finite dimensional   \n",
       "19991                  fuller             fuller test   \n",
       "19992      different possible             wilcox test   \n",
       "19993         number relevant      relevant questions   \n",
       "19994         matrix existing        permanent effect   \n",
       "19995   apologies incorrectly           batches pasta   \n",
       "19996           hopfield nets                hopfield   \n",
       "19997         dealing hessian      trying approximate   \n",
       "19998           winsorization         dummy variables   \n",
       "19999            know violate             know wilcox   \n",
       "\n",
       "                        TopWord5  Community  \n",
       "0      statistically significant         42  \n",
       "1                     validation        145  \n",
       "2                    data degree        140  \n",
       "3                  bias treating          1  \n",
       "4                  predicts time          1  \n",
       "5                 don understand        120  \n",
       "6                          plant         92  \n",
       "7               distribution pdf          1  \n",
       "8        assuming differentiable          1  \n",
       "9                          shock          1  \n",
       "10                effect variety         -1  \n",
       "11       multivariate regression        136  \n",
       "12                markov process          1  \n",
       "13                           log        148  \n",
       "14                        neural        102  \n",
       "15                 age indicator        137  \n",
       "16                          male         70  \n",
       "17                        cities          1  \n",
       "18                   recognition          1  \n",
       "19                      resample         50  \n",
       "20                           row          1  \n",
       "21                  fourth block          1  \n",
       "22            incorporate mining          1  \n",
       "23                    prove beta          1  \n",
       "24                     variables         42  \n",
       "25                 beta gaussian          1  \n",
       "26        deriving relationships          1  \n",
       "27              explaining works          1  \n",
       "28                      external         70  \n",
       "29                   proof prove         -1  \n",
       "...                          ...        ...  \n",
       "19970                      lidar        119  \n",
       "19971         correctly question          1  \n",
       "19972                  mean test         17  \n",
       "19973                    let let          1  \n",
       "19974                 parameters        118  \n",
       "19975         building important         16  \n",
       "19976          addition pairwise          1  \n",
       "19977           decisions assume         -1  \n",
       "19978           economics health          1  \n",
       "19979      avoid overpramatizing          1  \n",
       "19980                   function         59  \n",
       "19981                   textbook        118  \n",
       "19982           instruction spss        135  \n",
       "19983            effect opposite         -1  \n",
       "19984       different evaluation         -1  \n",
       "19985                     basing          1  \n",
       "19986                   learning         64  \n",
       "19987               dodder plant        144  \n",
       "19988         estimating density          1  \n",
       "19989                forecasting         50  \n",
       "19990                   discrete          1  \n",
       "19991                 test stata         32  \n",
       "19992              sample normal         26  \n",
       "19993                 tag number        147  \n",
       "19994            matter variable         -1  \n",
       "19995              cooking anova         -1  \n",
       "19996                     hinton          1  \n",
       "19997                       bfgs        118  \n",
       "19998                explainable        133  \n",
       "19999                 lt dismiss        118  \n",
       "\n",
       "[19725 rows x 8 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df_w_comm2.to_csv(COMMUNITIES_VIZ_PATH2, encoding='utf-8', index=False, columns = ['post_id', 'Community'])\n",
    "post_df_w_comm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs224w]",
   "language": "python",
   "name": "conda-env-cs224w-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
